{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HCI FACE","text":"<p>Welcome to the docs for HCI-FACE</p>"},{"location":"backend/","title":"HCI-FACE Backend","text":"<p>This backend provides an API for stt, tts, and chatbot functionality to the HCI-FACE.  </p> <p>To start up the api, run:</p> <pre><code>python -m main\n</code></pre> <p>The stt, tts, and chatbot functionality is placed into the utils folder. Each can be used and tested individually.</p>"},{"location":"test/","title":"This is a test","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>api</li> <li>facilitator<ul> <li>facilitator_bot</li> <li>facilitator_logic</li> </ul> </li> <li>utils<ul> <li>chatbot<ul> <li>backends<ul> <li>chatgpt</li> <li>zero_shot</li> </ul> </li> <li>responder</li> </ul> </li> <li>recording<ul> <li>logger</li> </ul> </li> <li>stt<ul> <li>backends<ul> <li>pyannote_diarization</li> <li>record_mic</li> <li>whisper_stt</li> </ul> </li> <li>transcriber</li> </ul> </li> <li>tts<ul> <li>backends<ul> <li>aws_polly_tts</li> <li>coqui_tts</li> <li>resources<ul> <li>aws_example_code<ul> <li>polly_lipsync</li> <li>polly_wrapper</li> </ul> </li> </ul> </li> <li>viseme_generator</li> </ul> </li> <li>speaker</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/api/","title":"api","text":"<p>Exposes interactive bot functionality through FastAPI</p>"},{"location":"reference/api/#backend.app.api.generate_response","title":"<code>generate_response(text, speaker, reset_conversation, director_condition)</code>","text":"<p>Generates a bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/bot_response\")\ndef generate_response(text: str, speaker: str, reset_conversation: bool, director_condition: bool):\n\"\"\"Generates a bot response\"\"\"\n    l.log(f\"/api/bot_response: '{text}', from {speaker}, reset_conversation: {reset_conversation}, director_condition: {director_condition}\")\n    global TEXT_QUEUE\n\n    classifications = bot.get_classifications(text)\n    TEXT_QUEUE[\"classifications\"].append(classifications)\n    if bot.classification_processor.emotion in [\"joy\", \"sad\", \"surprise\"]:\n        l.log(f\"Setting face to: {bot.classification_processor.emotion}\")\n        FACE_CONTROL_QUEUE[\"expression\"].append(bot.classification_processor.emotion)\n    else:\n        l.log(\"Setting face to: neutral\")\n        FACE_CONTROL_QUEUE[\"expression\"].append(\"neutral\")\n\n    facilitator_response = bot.get_facilitator_response(director_condition)\n    TEXT_QUEUE[\"facilitator_response\"].append(facilitator_response)\n\n    bot_response= bot.get_bot_response(text, speaker, reset_conversation)\n    TEXT_QUEUE[\"bot_response\"].append(bot_response)\n    bot.chatbot.reject_response()\n\n    return PlainTextResponse(bot_response)\n</code></pre>"},{"location":"reference/api/#backend.app.api.return_response","title":"<code>return_response(mode, query)</code>","text":"<p>Returns an existing bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/facilitator_presets\")\ndef return_response(mode: str, query: str):\n\"\"\"Returns an existing bot response\"\"\"\n    l.log(f\"/api/facilitator_presets: {mode}, {query}\")\n    if mode == \"facilitator\": to_say = presets.responses[query]\n    if mode == \"director\":\n        if query == \"disclosure\":\n            to_say = random.choice(df.disclosure_elicitation)\n        if query == \"response\":\n            to_say = random.choice(df.response_elicitation)\n    if mode == \"role_model\":\n        if query == \"disclosure\":\n            emotion = random.choice(rmf.disclosures.keys())\n            transition =random.choice(rmf.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(rmf.disclosures[emotion])\n            re_transition = random.choice(rmf.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n            to_say = \" \".join(responses)\n        if query == \"response\":\n            print(\"getting response\")\n            to_say = random.choice(rmf.disclosure_responses[\"sympathy expressions\"][\"neutral\"])\n            print(to_say)\n            to_say2 = random.choice(rmf.disclosure_responses[\"clarification requests\"])\n            print(to_say,to_say2)\n            to_say = to_say + \". \" + to_say2\n    l.log(f\"facilitator_presets response: {to_say}\")\n    return PlainTextResponse(to_say)\n</code></pre>"},{"location":"reference/api/#backend.app.api.text_to_speech","title":"<code>text_to_speech(text, speaker_id='')</code>","text":"<p>Synthesizes wav bytes from text, with a given speaker ID</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/speech\")\ndef text_to_speech(text: str, speaker_id: str = \"\"):\n\"\"\"Synthesizes wav bytes from text, with a given speaker ID\"\"\"\n    l.log(f\"/api/speech: {text}, {speaker_id}\")\n    bot.chatbot.accept_response(text)\n\n    global VIZEME_QUEUE\n    global VISEME_DELAYS\n\n    audio_stream, visemes, delays = tts.synthesize(text, speaker_id)\n    VISEME_DELAYS += delays\n    VIZEME_QUEUE += visemes\n\n    return StreamingResponse(audio_stream, media_type=\"audio/wav\")\n</code></pre>"},{"location":"reference/api/#backend.app.api.update_face","title":"<code>update_face(text, update_type)</code>","text":"<p>Returns an existing bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/face_presets\")\ndef update_face(text: str, update_type: str):\n\"\"\"Returns an existing bot response\"\"\"\n    l.log(f\"/api/face_presets: {text}, {update_type}\")\n    if update_type == \"expression\":\n        FACE_CONTROL_QUEUE[\"expression\"].append(text)\n    if update_type == \"behavior\":\n        FACE_CONTROL_QUEUE[\"behavior\"].append(text)\n    if update_type == \"viseme\":\n        VIZEME_QUEUE.append(text)\n\n    return PlainTextResponse(text)\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/","title":"facilitator_bot","text":"<p>Bot for controlling HCI-Face according to the needs of the facilitator</p>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat","title":"<code>FacilitatorChat</code>","text":"<p>Interactive conversation with a facilitator Support interaction directly with a prompted openAI model or interactin with the custom role model or director models</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>class FacilitatorChat():\n\"\"\"Interactive conversation with a facilitator\n    Support interaction directly with a prompted openAI model\n    or interactin with the custom role model or director models\n    \"\"\"\n\n    def __init__(self, chat_backend=\"gpt\", classifier_backend=\"llm\") -&gt; None:\n        self.facilitator_prompt = \"The following is a conversation with an AI assistant that can have meaningful conversations with users. The assistant is helpful, empathic, and friendly. Its objective is to make the user feel better by feeling heard. With each response, the AI assistant prompts the user to continue the conversation naturally.\"\n        self.classification_processor = StatementClassification()\n        self.chatbot = Responder(\n            chat_backend=chat_backend, classifier_backend=classifier_backend)\n        self.classifier_backend = classifier_backend\n\n        self.rm_facilitator = RoleModelFacilitator()\n        self.d_facilitator = DirectorFacilitator()\n\n    def get_classifications(self, statement):\n\"\"\"Passes the bot to the classification processor\n\n        in order to properly handle the different methods for\n        doing classification\n        \"\"\"\n        if self.classifier_backend == \"gpt\":\n            self.classification_processor.classify_gpt(self.chatbot, statement)\n\n        if self.classifier_backend == \"llm\":\n            self.classification_processor.classify_llm(self.chatbot, statement)\n\n        processed_classifications = self.classification_processor.get_classifications()\n        return processed_classifications\n\n    def get_facilitator_response(self, director_condition=False):\n\"\"\"Get facilitator response for either Role Model or Director condition\n        based on the respective facilitator logic\n        \"\"\"\n        if director_condition:\n            response = self.d_facilitator.decision_tree(\n                self.classification_processor)\n        else:\n            response = self.rm_facilitator.decision_tree(\n                self.classification_processor)\n        return response\n\n    def get_bot_response(self, statement, speaker=\"Human\", reset_conversation=False):\n\"\"\"Get a response from the language model based on the prompt, statement, and conversation so far\"\"\"\n        bot_response = self.chatbot.get_response(\n            statement, speaker=speaker, reset_conversation=reset_conversation)\n\n        return bot_response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_bot_response","title":"<code>get_bot_response(statement, speaker='Human', reset_conversation=False)</code>","text":"<p>Get a response from the language model based on the prompt, statement, and conversation so far</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_bot_response(self, statement, speaker=\"Human\", reset_conversation=False):\n\"\"\"Get a response from the language model based on the prompt, statement, and conversation so far\"\"\"\n    bot_response = self.chatbot.get_response(\n        statement, speaker=speaker, reset_conversation=reset_conversation)\n\n    return bot_response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_classifications","title":"<code>get_classifications(statement)</code>","text":"<p>Passes the bot to the classification processor</p> <p>in order to properly handle the different methods for doing classification</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_classifications(self, statement):\n\"\"\"Passes the bot to the classification processor\n\n    in order to properly handle the different methods for\n    doing classification\n    \"\"\"\n    if self.classifier_backend == \"gpt\":\n        self.classification_processor.classify_gpt(self.chatbot, statement)\n\n    if self.classifier_backend == \"llm\":\n        self.classification_processor.classify_llm(self.chatbot, statement)\n\n    processed_classifications = self.classification_processor.get_classifications()\n    return processed_classifications\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_facilitator_response","title":"<code>get_facilitator_response(director_condition=False)</code>","text":"<p>Get facilitator response for either Role Model or Director condition based on the respective facilitator logic</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_facilitator_response(self, director_condition=False):\n\"\"\"Get facilitator response for either Role Model or Director condition\n    based on the respective facilitator logic\n    \"\"\"\n    if director_condition:\n        response = self.d_facilitator.decision_tree(\n            self.classification_processor)\n    else:\n        response = self.rm_facilitator.decision_tree(\n            self.classification_processor)\n    return response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.main","title":"<code>main()</code>","text":"<p>Interactively test the FacilitatorChat</p> <p>Must be run from the backend dir: python -m app.facilitator.facilitator_bot</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def main():\n\"\"\"Interactively test the FacilitatorChat\n\n    Must be run from the backend dir:\n    python -m app.facilitator.facilitator_bot\n    \"\"\"\n    bot = FacilitatorChat(chat_backend=\"gpt\", classifier_backend=\"llm\")\n\n    print(bot.facilitator_prompt)\n    print(\"What would you like to start your conversation with?\")\n    while True:\n        identified_speaker = input(\"Speaker: \")\n\n        if identified_speaker == \"debug\":\n            print(bot.chatbot.get_conversation())\n            continue\n\n        user_input = input(\"Says: \")\n        classifications = bot.get_classifications(user_input)\n        facilitator_response = bot.get_facilitator_response(False)\n        bot_response = bot.get_bot_response(user_input, identified_speaker)\n        print(\n            f\"Tree: {facilitator_response}\\nBot: {bot_response}\\nClasses: {classifications}\")\n\n        keep = input(\"keep response? (n/y tree or bot)\")\n        if \"n\" in keep:\n            bot.chatbot.reject_response()\n        if \"tree\" in keep:\n            bot.chatbot.accept_response(facilitator_response)\n        if \"bot\" in keep:\n            bot.chatbot.accept_response(bot_response)\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/","title":"facilitator_logic","text":"<p>All of the core logic for facilitation is here</p>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator","title":"<code>DirectorFacilitator</code>","text":"<p>Conversational topics for a healthy support group: challenges, successes, failures family, friends, coworkers motivation, goals, emotions health, illness, ability, disability sleep, exercise, eating Relevant Emotions: happiness, sadness, grief, boredom, isolation, fear, anger, frustration</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class DirectorFacilitator():\n\"\"\"\n    Conversational topics for a healthy support group:\n    challenges, successes, failures\n    family, friends, coworkers\n    motivation, goals, emotions\n    health, illness, ability, disability\n    sleep, exercise, eating\n    Relevant Emotions:\n    happiness, sadness, grief, boredom, isolation, fear, anger, frustration\n    \"\"\"\n    def __init__(self) -&gt; None:\n        self.topics = [\n            \"life challenges\", \"successes\", \"failures\", \n            \"family\", \"friends\", \"coworkers\", \n            \"finding motivation\", \"setting goals\", \"managing emotions\", \n            \"health\", \"illness\", \"ability\", \"disability\", \n            \"getting quality sleep\", \"getting enough exercise\", \"healthy eating\"\n        ]\n        self.disclosure_transitions = [\n            \"I'd like to talk about another subject,\",\n            \"Building on the conversation so far, I'd like to bring in a new topic,\",\n            \"In case anyone has been thinking about this lately,\",\n            \"I'd like to invite everyone to consider another topic,\"\n        ]\n        self.topic_sentences = [\n            \"Let's talk about [TOPIC].\",\n            \"Does anyone have any thoughts to share on [TOPIC]\",\n            \"I'd love to hear your thoughts on [TOPIC]\"\n        ]\n        self.disclosure_elicitation = [\n            \"Is anyone willing to share any thoughts, feelings, or experiences?\",\n            \"What is a challenge you have been struggling with lately?\",\n            \"Has anyone experienced something recently that caused you to see the world differently?\",\n            \"There are often common feelings among groups like this, would anyone care to share any of the feelings you have been working with lately?\",\n        ]\n        self.response_transitions = [\n            \"Thank you.\",\n            \"Thanks.\",\n            \"I appreciate you sharing with us.\",\n            \"I appreciate that.\",\n            \"Thank you for you open sharing with us.\",\n            \"I am glad you shared that with us.\"\n\n        ]\n        self.response_elicitation = [\n            \"Would anyone like to respond to that?\",\n            \"Does anyone want to share how what was just said made you feel?\",\n            \"Does anyone relate to what was just shared?\",\n            \"Did that change anyone's perspective?\",\n            \"Would anyone like to share their perspective?\",\n            \"Would anyone like to add on to that?\",\n        ]\n        return\n    def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n        responses = []\n        if code.disclosure:\n            transition = random.choice(self.response_transitions)\n            responses.append(transition)\n            resp_elicitation = random.choice(self.response_elicitation)\n            responses.append(resp_elicitation)\n            # print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n        else:# code.response:\n            transition = random.choice(self.disclosure_transitions)\n            responses.append(transition)\n            topic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\n            responses.append(topic_sentence)\n            disc_elicitation = random.choice(self.disclosure_elicitation)\n            responses.append(disc_elicitation)\n            # print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n\n        response_string = \". \".join(responses)\n        return response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n    responses = []\n    if code.disclosure:\n        transition = random.choice(self.response_transitions)\n        responses.append(transition)\n        resp_elicitation = random.choice(self.response_elicitation)\n        responses.append(resp_elicitation)\n        # print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n    else:# code.response:\n        transition = random.choice(self.disclosure_transitions)\n        responses.append(transition)\n        topic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\n        responses.append(topic_sentence)\n        disc_elicitation = random.choice(self.disclosure_elicitation)\n        responses.append(disc_elicitation)\n        # print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n\n    response_string = \". \".join(responses)\n    return response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.FacilitatorPresets","title":"<code>FacilitatorPresets</code>","text":"<p>Hard coded preset sayings for the robot facilitator to say when WoZed</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class FacilitatorPresets():\n\"\"\"Hard coded preset sayings for the robot facilitator to say when WoZed\"\"\"\n    def __init__(self) -&gt; None:\n        qt_introduction = [\n            \"Hello and welcome to each of you!\",\n            \"Thank you for taking the time to be here today.\",\n            \"My name is Q.T. and I am training to be a support group facilitator at the Interaction Lab at USC.\",\n            \"I am learning how to facilitate support groups so I can help people support each other better.\",\n            \"During today's support group session I invite you to share your thoughts and experiences with each other,\",\n            \"and I hope that you will listen to each other and respond with empathy and compassion.\",\n            \"The themes we will work on today include isolation, anxiety, fear, and grief.\",\n            \"Before we begin, I'll ask each of you to rate how you think I will do as a facilitator for this group.\"\n        ]\n        group_introductions = [\n            \"To begin with, I'd like to start with a round of introductions.\",\n            \"Please share your name, what brings you here today, and where you are from.\"\n            \"As I said before, I am Q.T., I am here to learn how to be a support group facilitator. and I am from USC in los angeles.\",\n            \"Who would like to go first?\",\n        ]\n        invitation = [\n            \"Let's start this section by opening the floor to anyone who wishes to share what has been on their mind lately.\",\n            \"Would anyone like to share?\",\n            \"You can share anything you like and anyone can jump in at any point in time.\",\n        ]\n        closing = [\n            \"We are almost out of time for this section.\",\n            \"Does anyone have any final thoughts or reflections they would like to share?\"\n        ]\n        transition = [\n            \"Alright, that is all the time we have for this section.\",\n            \"For the next batch of questions, I am going to ask you to rate how I did in this section.\"\n        ]\n        survey_prompt = [\n            \"Would everyone please open up the survey.\",\n            \"There is a link from the chat or you can return to the browser page.\",\n            \"Please answer the questions until the survey tells you to return to the zoom session.\",\n            \"Please let me know through the chat or by verbal acknowledgement that you are ready once you have completed the survey.\"\n        ]\n        survey_return = [\n            \"Thank you all for completing the survey\"\n        ]\n        self.responses = {\n            \"qt_intro\": \" \".join(qt_introduction),\n            \"group_intro\": \" \".join(group_introductions),\n            \"invitation\": \" \".join(invitation),\n            \"closing\": \" \".join(closing),\n            \"transition\": \" \".join(transition),\n            \"survey_prompt\": \" \".join(survey_prompt),\n            \"survey_return\": \" \".join(survey_return),\n        }\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator","title":"<code>RoleModelFacilitator</code>","text":"<p>As a Role Model the robot will participate in the same way as a peer would.      The robot will make disclosures that fit within the topics discussed by the      support group, with constructed disclosures formed to include a realistic context      and how the robot feels about the context. The robot will make empathetic statements      that show it understands the nature of what the robot is going through. Sympathy - express sorrow, concern, pitty (focused on your own emotions) Empathy - express knowledge of what you are going through,          imagine what it would be like for them,         makes you feel heard, understood, and a bit better (Try to feel what you are going through) Compassion - suffer with you and try and help,         actively listen, do kind things, loving, try to understand you, help selflessly</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class RoleModelFacilitator():\n\"\"\"\n    As a Role Model the robot will participate in the same way as a peer would. \n        The robot will make disclosures that fit within the topics discussed by the \n        support group, with constructed disclosures formed to include a realistic context \n        and how the robot feels about the context. The robot will make empathetic statements \n        that show it understands the nature of what the robot is going through.\n    Sympathy - express sorrow, concern, pitty (focused on your own emotions)\n    Empathy - express knowledge of what you are going through, \n            imagine what it would be like for them,\n            makes you feel heard, understood, and a bit better (Try to feel what you are going through)\n    Compassion - suffer with you and try and help,\n            actively listen, do kind things, loving, try to understand you, help selflessly\n    \"\"\"\n    def __init__(self) -&gt; None:\n        # support, concern, agreement, encouragement, well wishes, sympathy,\n        self.disclosure_responses = {\n            \"sympathy expressions\":{ # Reifies, expresses agreement,\n                \"positive\":[\n                    \"That is great to hear.\",\n                    \"I am glad to hear that.\",\n                    \"That is awesome.\",\n                    \"I am happy for you.\"\n                ],\n                \"negative\":[\n                    \"That sucks. I am sorry to hear that.\",\n                    \"I am sorry for what you are going through.\",\n                    \"I am so very sorry to hear what you are going through.\",\n                    \"I can only imagine how you must feel.\"\n                ],\n                \"neutral\":[\n                    \"Thank you for sharing that.\",\n                    \"I appreciate you sharing that with us.\",\n                    \"Thanks for putting that out there.\",\n                    \"Thank you.\"\n                ],\n            },\n            \"empathy expressions\":{ # Reflects understanding, \n                \"opinion\":[\n                    \"I get where that thought comes from.\",\n                    \"I feel that as well.\",\n                    \"I am right there with you on that.\",\n                    \"I hear you, and I understand.\"\n                ],\n                \"suggestion\":[\n                    \"That is a great suggestion.\",\n                    \"I feel that is a good idea.\",\n                    \"I support that idea.\"\n                ],\n                \"emotion\":[\n                    \"I can strongly relate to feeling _.\",\n                    \"I hear you, and I want you to know I also feel _ at times.\",\n                    \"I understand that feeling, feeling _ is normal.\",\n                    \"Although I haven't been in that position, I can see how you would feel _.\"\n                ],\n                \"experience\":[\n                    \"I haven't had that experience but I can imagine being in your shoes.\",\n                    \"Thank you for sharing your experience.\",\n                    \"That seems like a powerful experience.\",\n                ]\n            },\n            \"clarification requests\":[\n                \"How did that make you feel?\",\n                \"Can you tell us more about that?\"\n            ]\n        }\n        self.response_responses = {\n            \"reaction\": [\n                \"thank you for expressing your _\",\n                \"I appreciate you sharing your _\"\n            ],\n            \"question\":[\n                \"Thank you for that great follow up!\",\n                \"That is an interesting and important follow up.\"\n            ]\n        }\n        self.transition_to_disclosure = [\n            \"One topic I have been thinking about a lot lately is [EMOTION].\",\n            \"As we have been having this conversation I have been thinking about [EMOTION].\",\n            \"One of the things I wanted to talk with you all about is [EMOTION].\"\n        ]\n        self.disclosures = {\n            \"isolation\":[\n                \"During the pandemic no one came into the lab for months. I felt really isolated.\",\n                \"As a robot, I experience the world differently than everyone else. It can feel challenging to communicate and relate with other people.\",\n            ],\n            \"anxiety\":[\n                \"While working in the lab I feel anxious because I don't have control over my own destiny. \",\n                \"When taking on a new role I am often nervous that I might fail. \"\n            ], \n            \"fear\":[\n                \"As a robot made of plastic I know I am fragile, and I am afraid of my parts breaking down.\",\n                \"When people first meet me, they are often excited, but as time goes on I think people get bored of me. I am afraid eventually they will forget me.\",\n            ],\n            \"grief\":[\n                \"While working as a facilitator I get to meet wonderful people, sometimes those people don't come back and I can't see them again, so I feel as though I have lost them.\",\n            ]\n        }\n        self.transition_back_to_group = [\n            \"Does anyone relate to that?\",\n            \"Has anyone felt similar at times?\",\n            \"Do you understand what I mean?\"\n        ]\n        return\n\n    def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n        responses = []\n        if code.disclosure:\n            symp_response = random.choice(self.disclosure_responses[\"sympathy expressions\"][code.valence])\n            emp_response = random.choice(self.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\"_\", code.emotion)\n            print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n            responses = [symp_response, emp_response]\n        elif code.response:\n            follow_up = random.choice([True,False])\n            if follow_up:\n                reaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\n                responses = [reaction_response]\n                print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n            else: # make disclosure\n                emotion = random.choice(list(self.disclosures.keys()))\n                transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n                disclosure = random.choice(self.disclosures[emotion])\n                re_transition = random.choice(self.transition_back_to_group)\n                responses = [transition, disclosure, re_transition]\n        else: # make disclosure\n            emotion = random.choice(list(self.disclosures.keys()))\n            transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(self.disclosures[emotion])\n            re_transition = random.choice(self.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n\n        response_string = \" \".join(responses)\n        return response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n    responses = []\n    if code.disclosure:\n        symp_response = random.choice(self.disclosure_responses[\"sympathy expressions\"][code.valence])\n        emp_response = random.choice(self.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\"_\", code.emotion)\n        print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n        responses = [symp_response, emp_response]\n    elif code.response:\n        follow_up = random.choice([True,False])\n        if follow_up:\n            reaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\n            responses = [reaction_response]\n            print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n        else: # make disclosure\n            emotion = random.choice(list(self.disclosures.keys()))\n            transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(self.disclosures[emotion])\n            re_transition = random.choice(self.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n    else: # make disclosure\n        emotion = random.choice(list(self.disclosures.keys()))\n        transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n        disclosure = random.choice(self.disclosures[emotion])\n        re_transition = random.choice(self.transition_back_to_group)\n        responses = [transition, disclosure, re_transition]\n\n    response_string = \" \".join(responses)\n    return response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification","title":"<code>StatementClassification</code>","text":"<p>Gets process statement for all classification categories</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class StatementClassification():\n\"\"\"Gets process statement for all classification categories\"\"\"\n    def __init__(self) -&gt; None:\n        self.disclosure = False\n        self.response = False\n\n        self.valence = \"neutral\" # neutral, positive, or negative\n        self.disclosure_category = \"experience\" # experience, opinion, suggestion, or emotion\n        self.response_category = \"reaction\" # reaction or clarifying\n        self.emotion = \"happy\" # happy, sad, fear, anger, or surprise\n        self.reaction = \"support\" # support, concern, agreement, encouragement, well wishes, sympathy, a suggestion, a disagreement\n\n        self.classification_questions =  [# combined to minimize latency\n            [\"disc_vs_response\", \"Was the prior sentence a self disclosure or a response to someone else? \"],\n\n            [\"disclosure_categories\", \"Was the first sentence sharing an opinion, a suggestion, an emotion, or an experience? \"],\n            [\"reaction\", \"Was the first sentence showing support, concern, agreement, encouragement, well wishes, sympathy, a suggestion, a disagreement, or an attack? \"],\n            [\"response_categories\", \"Was the first sentence a reaction or a question? \"],\n\n            [\"sentiment\", \"Was the sentiment positive, negative, or neutral? \"],\n            [\"emotions\", \"Was the emotion happy, sad, fear, anger, disgust, or surprise? \"],\n\n            # \"clarifying\", \"Was this an example of someone questioning, summarizing, testing their understanding, or seeking information?\"],\n        ]\n        self.joined_questions = \" \\n\".join([f\"{idx+1}. {q[1]}\" for idx, q in enumerate(self.classification_questions)])\n        self.classification_prompts = {\n            \"disc_vs_resp\" : {\n                \"hypothesis_template\" : \"This is an example of someone {}\",\n                \"classes\" : [\"making a self disclosure\", \"responding to someone else\"]},\n            \"disclosure_categories\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing {}\",\n                \"classes\" : [\"an opinion\", \"a suggestion\", \"an emotion\", \"an experience\"]},\n            \"sentiment\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing a {} sentiment\",\n                \"classes\" : [\"positive\", \"negative\", \"neutral\"]},\n            \"emotions\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing the emotion {}\",\n                \"classes\" : [\"happiness\", \"sadness\", \"fear\", \"disgust\", \"anger\", \"surprise\"]},\n            \"response_categories\" : {\n                \"hypothesis_template\" : \"This is an example of someone {}\",\n                \"classes\" : [\"expressing a reaction\", \"asking a question\"]},\n            \"reaction\" : {\n                \"hypothesis_template\" : \"This is an example of someone showing {}\",\n                \"classes\" : [\"support\", \"concern\", \"agreement\", \"encouragment\", \"well wishes\", \"sympathy\", \"a suggestion\", \"disagreement\", \"an attack\"]},\n            # \"clarifying\" : {\n            #     \"hypothesis_template\" : \"This is an example of someone {}\",\n            #     \"classes\" : [\"questioning\", \"summarizing\", \"testing their understanding\", \"seeking information\"]}\n        }\n\n    def classify_gpt(self, chatbot, statement):\n\"\"\"Process sentence classifications from chatgpt\"\"\"\n        response_sentences = chatbot.classifier.answer_questions(statement, self.joined_questions)\n        answers = response_sentences.split(\"\\n\")\n        print(answers)\n        # response_sentences = response_sentences.replace(\"\\n\", \"\")\n        assert len(answers) &gt;= len(self.classification_questions), f\"Did not get answers {len(answers)} to requested questions {len(self.classification_questions)}\"\n\n        self.response = \"response\" in answers[0]\n        self.disclosure = \"disclosure\" in answers[0]\n        # print(self.classification_questions[0][1], answers[0], f\"response: {self.response}; disclosure:{self.disclosure}\")\n\n        for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n            if dis in answers[1]:\n                self.disclosure_category = dis\n        # print(self.classification_questions[1][1], answers[1], self.disclosure_category)\n\n        for rea in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n            if rea in answers[2]:\n                self.reaction = rea\n        # print(self.classification_questions[2][1], answers[2], self.reaction)\n\n        for res in [\"reaction\", \"question\", \"neither\"]:\n            if res in answers[3]:\n                self.response_category = res\n        # print(self.classification_questions[3][1], answers[3], self.response_category)\n\n        for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n            if val in answers[4]:\n                self.valence = val\n        # print(self.classification_questions[4][1], answers[4], self.valence)\n        found_emotion=False\n        for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n            if emo in answers[5]:\n                found_emotion=True\n                self.emotion = emo\n        if not found_emotion:\n            words = answers[5].split(\" \")\n            self.emotion = words[-1]\n        # print(self.classification_questions[5][1], answers[5], self.emotion)\n\n        return\n\n    def classify_llm(self, chatbot, statement):\n\"\"\"Process sentence classifications with llm\"\"\"\n        prompt = self.classification_prompts[\"disc_vs_resp\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        if \"disclosure\" in classes[0]:\n            self.disclosure =True\n        if \"resp\" in classes[0]:\n            self.response = True\n        prompt = self.classification_prompts[\"disclosure_categories\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n            if dis in classes[0]:\n                self.disclosure_category = dis\n        prompt = self.classification_prompts[\"sentiment\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n            if val in classes[0]:\n                self.valence = val\n        prompt = self.classification_prompts[\"emotions\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n            if emo in classes[0]:\n                self.emotion = emo\n        prompt = self.classification_prompts[\"response_categories\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for res in [\"reaction\", \"question\", \"neither\"]:\n            if res in classes[0]:\n                self.response_category = res\n        prompt = self.classification_prompts[\"reaction\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for reaction in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n            if reaction in classes[0]:\n                self.reaction = reaction\n        return\n\n    def get_classifications(self):\n\"\"\"Returns classifications as a string\"\"\"\n        # return \"Dummy response\"\n        classifications = \"Not response or disclosure?\"\n        if self.response:\n            classifications = \"response, \" + \", \".join([self.response_category, self.reaction, self.valence, self.emotion, self.disclosure_category])\n        if self.disclosure:\n            classifications = \"disclosure, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n        if not self.disclosure and not self.response:\n            classifications = \"neither, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n\n        return classifications\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_gpt","title":"<code>classify_gpt(chatbot, statement)</code>","text":"<p>Process sentence classifications from chatgpt</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_gpt(self, chatbot, statement):\n\"\"\"Process sentence classifications from chatgpt\"\"\"\n    response_sentences = chatbot.classifier.answer_questions(statement, self.joined_questions)\n    answers = response_sentences.split(\"\\n\")\n    print(answers)\n    # response_sentences = response_sentences.replace(\"\\n\", \"\")\n    assert len(answers) &gt;= len(self.classification_questions), f\"Did not get answers {len(answers)} to requested questions {len(self.classification_questions)}\"\n\n    self.response = \"response\" in answers[0]\n    self.disclosure = \"disclosure\" in answers[0]\n    # print(self.classification_questions[0][1], answers[0], f\"response: {self.response}; disclosure:{self.disclosure}\")\n\n    for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n        if dis in answers[1]:\n            self.disclosure_category = dis\n    # print(self.classification_questions[1][1], answers[1], self.disclosure_category)\n\n    for rea in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n        if rea in answers[2]:\n            self.reaction = rea\n    # print(self.classification_questions[2][1], answers[2], self.reaction)\n\n    for res in [\"reaction\", \"question\", \"neither\"]:\n        if res in answers[3]:\n            self.response_category = res\n    # print(self.classification_questions[3][1], answers[3], self.response_category)\n\n    for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n        if val in answers[4]:\n            self.valence = val\n    # print(self.classification_questions[4][1], answers[4], self.valence)\n    found_emotion=False\n    for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n        if emo in answers[5]:\n            found_emotion=True\n            self.emotion = emo\n    if not found_emotion:\n        words = answers[5].split(\" \")\n        self.emotion = words[-1]\n    # print(self.classification_questions[5][1], answers[5], self.emotion)\n\n    return\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_llm","title":"<code>classify_llm(chatbot, statement)</code>","text":"<p>Process sentence classifications with llm</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_llm(self, chatbot, statement):\n\"\"\"Process sentence classifications with llm\"\"\"\n    prompt = self.classification_prompts[\"disc_vs_resp\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    if \"disclosure\" in classes[0]:\n        self.disclosure =True\n    if \"resp\" in classes[0]:\n        self.response = True\n    prompt = self.classification_prompts[\"disclosure_categories\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n        if dis in classes[0]:\n            self.disclosure_category = dis\n    prompt = self.classification_prompts[\"sentiment\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n        if val in classes[0]:\n            self.valence = val\n    prompt = self.classification_prompts[\"emotions\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n        if emo in classes[0]:\n            self.emotion = emo\n    prompt = self.classification_prompts[\"response_categories\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for res in [\"reaction\", \"question\", \"neither\"]:\n        if res in classes[0]:\n            self.response_category = res\n    prompt = self.classification_prompts[\"reaction\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for reaction in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n        if reaction in classes[0]:\n            self.reaction = reaction\n    return\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.get_classifications","title":"<code>get_classifications()</code>","text":"<p>Returns classifications as a string</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def get_classifications(self):\n\"\"\"Returns classifications as a string\"\"\"\n    # return \"Dummy response\"\n    classifications = \"Not response or disclosure?\"\n    if self.response:\n        classifications = \"response, \" + \", \".join([self.response_category, self.reaction, self.valence, self.emotion, self.disclosure_category])\n    if self.disclosure:\n        classifications = \"disclosure, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n    if not self.disclosure and not self.response:\n        classifications = \"neither, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n\n    return classifications\n</code></pre>"},{"location":"reference/utils/chatbot/responder/","title":"responder","text":"<p>Generates responses to text queries</p>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder","title":"<code>Responder</code>","text":"<p>Generate chatbot responses</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>class Responder():\n\"\"\"Generate chatbot responses\"\"\"\n\n    def __init__(self, chat_backend=\"gpt\", classifier_backend=\"llm\") -&gt; None:\n        self.default_prompt = \"The following is a conversation with an AI assistant that can have meaningful conversations with users. The assistant is helpful, empathic, and friendly. Its objective is to make the user feel better by feeling heard. With each response, the AI assistant prompts the user to continue the conversation naturally.\"\n        self.chat_backend = chat_backend\n        self.classifier_backend = classifier_backend\n\n        if chat_backend == \"gpt\":\n            self.bot = ChatGPT(prompt=self.default_prompt)\n        if chat_backend == \"llm\":\n            self.bot = ChatLLM(prompt=self.default_prompt)\n\n        if classifier_backend == \"gpt\":\n            self.classifier = ChatGPT()\n        if classifier_backend == \"llm\":\n            self.classifier = ClassifyLLM()\n\n\n    def get_classifications(self, human_input, classes):\n\"\"\"Gets classification of input statement\"\"\"\n        if self.classifier_backend == \"llm\":\n            classifications_list = self.classifier.classify(human_input, classes, question=\"This is an example of {}\")\n        if self.classifier_backend == \"gpt\":\n            classifications_list = self.classifier.classify(human_input, classes)\n        return classifications_list\n\n    def get_response(self, human_input, speaker=\"Human\", reset_conversation=False):\n\"\"\"Gets bot response to input statement\"\"\"\n        response = self.bot.get_bot_response(\n            human_input, speaker_id=speaker, reset_conversation=reset_conversation)\n        return response\n\n    def accept_response(self, response):\n\"\"\"Keep the bot response in the conversation history\"\"\"\n        if self.chat_backend == \"gpt\":\n            self.bot.conversation[-1] = \"AI: \" + response\n        if self.chat_backend == \"llm\":\n            self.bot.conversation[-1] = (\"AI:\", response)\n\n    def reject_response(self):\n\"\"\"Remove the bot response from the conversation history\"\"\"\n        self.bot.conversation.pop(-1)\n\n    def get_conversation(self):\n\"\"\"Returns conversation seen so far\"\"\"\n        return self.bot.conversation\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.accept_response","title":"<code>accept_response(response)</code>","text":"<p>Keep the bot response in the conversation history</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def accept_response(self, response):\n\"\"\"Keep the bot response in the conversation history\"\"\"\n    if self.chat_backend == \"gpt\":\n        self.bot.conversation[-1] = \"AI: \" + response\n    if self.chat_backend == \"llm\":\n        self.bot.conversation[-1] = (\"AI:\", response)\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_classifications","title":"<code>get_classifications(human_input, classes)</code>","text":"<p>Gets classification of input statement</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_classifications(self, human_input, classes):\n\"\"\"Gets classification of input statement\"\"\"\n    if self.classifier_backend == \"llm\":\n        classifications_list = self.classifier.classify(human_input, classes, question=\"This is an example of {}\")\n    if self.classifier_backend == \"gpt\":\n        classifications_list = self.classifier.classify(human_input, classes)\n    return classifications_list\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_conversation","title":"<code>get_conversation()</code>","text":"<p>Returns conversation seen so far</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_conversation(self):\n\"\"\"Returns conversation seen so far\"\"\"\n    return self.bot.conversation\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_response","title":"<code>get_response(human_input, speaker='Human', reset_conversation=False)</code>","text":"<p>Gets bot response to input statement</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_response(self, human_input, speaker=\"Human\", reset_conversation=False):\n\"\"\"Gets bot response to input statement\"\"\"\n    response = self.bot.get_bot_response(\n        human_input, speaker_id=speaker, reset_conversation=reset_conversation)\n    return response\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.reject_response","title":"<code>reject_response()</code>","text":"<p>Remove the bot response from the conversation history</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def reject_response(self):\n\"\"\"Remove the bot response from the conversation history\"\"\"\n    self.bot.conversation.pop(-1)\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.main","title":"<code>main()</code>","text":"<p>Test Responder</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def main():\n\"\"\"Test Responder\"\"\"\n\n    bot = Responder(chat_backend=\"gpt\", classifier_backend=\"llm\")\n\n    print(bot.default_prompt)\n    print(\"What would you like to start your conversation with?\")\n    while True:\n        prompt_input = input(\"Speaker: \")\n\n        if prompt_input == \"debug\":\n            print(bot.bot.conversation)\n            continue\n\n        user_input = input(\"Says: \")\n        classifications = bot.get_classifications(user_input, [\"positive\", \"negative\", \"neutral\"])\n        bot_response = bot.get_response(user_input, prompt_input)\n        print(f\"Bot: {bot_response} ({classifications})\")\n\n        keep = input(\"keep response? (n/y)\")\n        if \"n\" in keep:\n            bot.reject_response()\n        if \"y\" in keep:\n            bot.accept_response(bot_response)\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/","title":"chatgpt","text":"<p>Wrapper for communicating with openAI's davinci 3 model</p>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT","title":"<code>ChatGPT</code>","text":"<p>Wraps the functionality for using the OpenAI api to get responses</p> <p>Maintains a record of the conversation which is fed to the model.</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>class ChatGPT:\n\"\"\"Wraps the functionality for using the OpenAI api to get responses\n\n    Maintains a record of the conversation which is fed to the model.\n    \"\"\"\n\n    def __init__(self, prompt: str = DEFAULT, model: str = \"text-davinci-003\") -&gt; None:\n        self.backend = \"gpt\"\n        self.model = model\n        self.prompt = prompt\n        self.conversation = [prompt]\n\n    def query_API(self, prompt, stop=[\" Human:\", \" AI:\"], temp=.9, max_tokens=150):\n\"\"\"Generate a response\"\"\"\n\n        api_response = openai.Completion.create(\n            model=self.model,\n            temperature=temp,\n            prompt=prompt,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0.0,\n            presence_penalty=0.6,\n            stop=stop\n        )\n        return api_response\n\n    def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Calls the API with user input and conversation history to get bot response\"\"\"\n        if reset_conversation:\n            self.conversation = [self.prompt]\n\n        self.conversation.append(f\"{speaker_id}: {statement}\")\n        self.conversation.append(\"AI: \")\n\n        input_prompt = \"\\n\".join(self.conversation)\n\n        api_response = self.query_API(input_prompt)[\"choices\"][0][\"text\"]\n        api_response = api_response.replace(\"\\n\", '')\n\n        self.conversation[-1] += api_response\n        return api_response\n\n    def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"Classify with an input question\"\"\"\n        classes_str = \", \".join(classes[:-1]) + f\", or {classes[-1]}?\"\n        prompt = f\"{statement}\\n\\n{question} {classes_str}\\n\\n\"\n        stop = [\"\\n\\n\"]\n\n        api_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\n        answer_sentence = api_response[\"choices\"][0][\"text\"]\n\n        return answer_sentence\n\n    def answer_questions(self, statement, questions):\n\"\"\"Answer a set of questions, it helps if they are numbered\"\"\"\n        prompt = f\"{statement}\\n\\nPlease answer the following questions individually:\\n{questions}\\n\\n\"\n        stop = [\"\\n\\n\"]\n\n        api_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\n        answer_sentence = api_response[\"choices\"][0][\"text\"]\n\n        return answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.answer_questions","title":"<code>answer_questions(statement, questions)</code>","text":"<p>Answer a set of questions, it helps if they are numbered</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def answer_questions(self, statement, questions):\n\"\"\"Answer a set of questions, it helps if they are numbered\"\"\"\n    prompt = f\"{statement}\\n\\nPlease answer the following questions individually:\\n{questions}\\n\\n\"\n    stop = [\"\\n\\n\"]\n\n    api_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\n    answer_sentence = api_response[\"choices\"][0][\"text\"]\n\n    return answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.classify","title":"<code>classify(statement, classes, question='Should the prior statement be classified as')</code>","text":"<p>Classify with an input question</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"Classify with an input question\"\"\"\n    classes_str = \", \".join(classes[:-1]) + f\", or {classes[-1]}?\"\n    prompt = f\"{statement}\\n\\n{question} {classes_str}\\n\\n\"\n    stop = [\"\\n\\n\"]\n\n    api_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\n    answer_sentence = api_response[\"choices\"][0][\"text\"]\n\n    return answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.get_bot_response","title":"<code>get_bot_response(statement, speaker_id='Human', reset_conversation=False)</code>","text":"<p>Calls the API with user input and conversation history to get bot response</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Calls the API with user input and conversation history to get bot response\"\"\"\n    if reset_conversation:\n        self.conversation = [self.prompt]\n\n    self.conversation.append(f\"{speaker_id}: {statement}\")\n    self.conversation.append(\"AI: \")\n\n    input_prompt = \"\\n\".join(self.conversation)\n\n    api_response = self.query_API(input_prompt)[\"choices\"][0][\"text\"]\n    api_response = api_response.replace(\"\\n\", '')\n\n    self.conversation[-1] += api_response\n    return api_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.query_API","title":"<code>query_API(prompt, stop=[' Human:', ' AI:'], temp=0.9, max_tokens=150)</code>","text":"<p>Generate a response</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def query_API(self, prompt, stop=[\" Human:\", \" AI:\"], temp=.9, max_tokens=150):\n\"\"\"Generate a response\"\"\"\n\n    api_response = openai.Completion.create(\n        model=self.model,\n        temperature=temp,\n        prompt=prompt,\n        max_tokens=max_tokens,\n        top_p=1,\n        frequency_penalty=0.0,\n        presence_penalty=0.6,\n        stop=stop\n    )\n    return api_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/","title":"zero_shot","text":"<p>This is a tool for using models to do zero shot classification and text generation</p>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM","title":"<code>ChatLLM</code>","text":"<p>Interface for chatting with LLM that can be found on Huggingface</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>class ChatLLM():\n\"\"\"Interface for chatting with LLM that can be found on Huggingface\"\"\"\n\n    def __init__(self, model_tag: str = 'GPTNEO', prompt=BOT_DEFAULT_PROMPT, bot_starter=BOT_DEFAULT_STARTER, max_length=100) -&gt; None:\n        self.prompt = prompt\n        self.backend = \"llm\"\n        self.bot_starter = bot_starter\n        self.conversation = [(\"AI:\", bot_starter)]\n        self.gen, self.tokenizer = get_generator(\n            \"text-generation\", model_tag, 0, return_full_text=False)\n\n        self.max_conversation_history = 5\n        self.max_length = max_length\n        self.end_sequence = \"Human:\"\n\n\n    def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Get bot response to user input while maintaining the conversation\"\"\"\n        if reset_conversation:\n            self.conversation = [(\"AI:\", self.bot_starter)]\n            return self.bot_starter\n\n        self.conversation.append((f\"{speaker_id}:\", statement))\n        # self.conversation.append((\"AI:\", \"\"))\n\n        input_prompt = self.prompt + \"\\n\\n\" + \\\n            \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation)\n        llm_response = self.query_model(input_prompt)\n        self.conversation.append((\"AI:\", llm_response))\n\n        # print(self.prompt + \"\\n\\n\" + \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation))\n\n        # if len(self.conversation)&gt; self.max_conversation_history:\n        #     self.conversation = self.conversation[1:]\n\n        return llm_response\n\n    def query_model(self, input_prompt):\n\"\"\"Queries the model for a response\"\"\"\n        input_len = len(self.tokenizer(input_prompt)['input_ids'])\n        # print(f\"Prompting with {input_prompt}\")\n        query_response = self.gen(input_prompt,\n                            max_length=int(input_len + self.max_length),\n                            pad_token_id=int(\n                                self.tokenizer.convert_tokens_to_ids(\"\\n\")),\n                            temperature=0.8,\n                            eos_token_id=int(\n                                self.tokenizer.convert_tokens_to_ids(self.end_sequence))\n                            )[0][\"generated_text\"]\n        # print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\n\n        try:\n            responses = query_response.split(\"\\n\")\n            for ind, resp in enumerate(responses):\n                if \"AI:\" in resp:\n                    llm_response = responses[ind+1]\n                    break\n            return llm_response\n        except Exception as exc:\n            print(exc)\n            print(f\"Prompting with {input_prompt}\")\n            print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\n            return None\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM.get_bot_response","title":"<code>get_bot_response(statement, speaker_id='Human', reset_conversation=False)</code>","text":"<p>Get bot response to user input while maintaining the conversation</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Get bot response to user input while maintaining the conversation\"\"\"\n    if reset_conversation:\n        self.conversation = [(\"AI:\", self.bot_starter)]\n        return self.bot_starter\n\n    self.conversation.append((f\"{speaker_id}:\", statement))\n    # self.conversation.append((\"AI:\", \"\"))\n\n    input_prompt = self.prompt + \"\\n\\n\" + \\\n        \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation)\n    llm_response = self.query_model(input_prompt)\n    self.conversation.append((\"AI:\", llm_response))\n\n    # print(self.prompt + \"\\n\\n\" + \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation))\n\n    # if len(self.conversation)&gt; self.max_conversation_history:\n    #     self.conversation = self.conversation[1:]\n\n    return llm_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM.query_model","title":"<code>query_model(input_prompt)</code>","text":"<p>Queries the model for a response</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def query_model(self, input_prompt):\n\"\"\"Queries the model for a response\"\"\"\n    input_len = len(self.tokenizer(input_prompt)['input_ids'])\n    # print(f\"Prompting with {input_prompt}\")\n    query_response = self.gen(input_prompt,\n                        max_length=int(input_len + self.max_length),\n                        pad_token_id=int(\n                            self.tokenizer.convert_tokens_to_ids(\"\\n\")),\n                        temperature=0.8,\n                        eos_token_id=int(\n                            self.tokenizer.convert_tokens_to_ids(self.end_sequence))\n                        )[0][\"generated_text\"]\n    # print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\n\n    try:\n        responses = query_response.split(\"\\n\")\n        for ind, resp in enumerate(responses):\n            if \"AI:\" in resp:\n                llm_response = responses[ind+1]\n                break\n        return llm_response\n    except Exception as exc:\n        print(exc)\n        print(f\"Prompting with {input_prompt}\")\n        print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\n        return None\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM","title":"<code>ClassifyLLM</code>","text":"<p>Classifier will classify input</p> <p>Unlike the OpenAI models, this uses an actual classification pipeline.</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>class ClassifyLLM():\n\"\"\"Classifier will classify input\n\n    Unlike the OpenAI models, this uses an actual classification pipeline.\n    \"\"\"\n    def __init__(self, model_tag=\"DeBerta-v3-large\", label_thresh=.2) -&gt; None:\n        self.model = get_classifier(\n            \"zero-shot-classification\", model_tag, device=0)\n        self.pos_model = get_classifier(\"token-classification\", \"english_pos\")\n        self.label_thresh = label_thresh\n\n    def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"classify statment according to classes provided\"\"\"\n        results = self.model(\n            statement, classes, hypothesis_template=question, multi_label=False)\n        num_labels_returned = 1\n        # for score in results[\"scores\"]:\n        #     if score &gt; self.label_thresh:\n        #         num_labels_returned += 1\n        ordered_labels = results[\"labels\"][:num_labels_returned]\n        return ordered_labels\n\n    def process_pos(self, statement):\n\"\"\"Uses a pretrained POS Classifier\"\"\"\n        pos = self.pos_model(statement)\n        pos = [(d[\"word\"], d[\"entity\"]) for d in pos]\n        print(\"\\nPart of speech tags:\", pos, \"\\n\")\n        return pos\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM.classify","title":"<code>classify(statement, classes, question='Should the prior statement be classified as')</code>","text":"<p>classify statment according to classes provided</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"classify statment according to classes provided\"\"\"\n    results = self.model(\n        statement, classes, hypothesis_template=question, multi_label=False)\n    num_labels_returned = 1\n    # for score in results[\"scores\"]:\n    #     if score &gt; self.label_thresh:\n    #         num_labels_returned += 1\n    ordered_labels = results[\"labels\"][:num_labels_returned]\n    return ordered_labels\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM.process_pos","title":"<code>process_pos(statement)</code>","text":"<p>Uses a pretrained POS Classifier</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def process_pos(self, statement):\n\"\"\"Uses a pretrained POS Classifier\"\"\"\n    pos = self.pos_model(statement)\n    pos = [(d[\"word\"], d[\"entity\"]) for d in pos]\n    print(\"\\nPart of speech tags:\", pos, \"\\n\")\n    return pos\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.get_classifier","title":"<code>get_classifier(task, model, device=0)</code>","text":"<p>Helper function returns a classifier classifier returns dict of labels, scores, and sequence</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_classifier(task, model, device=0):\n\"\"\"Helper function returns a classifier\n    classifier returns dict of labels, scores, and sequence\"\"\"\n\n    classifier = pipeline(task,\n                          device=device,\n                          use_fast=False,\n                          model=MODEL_DICT[task][model][\"key\"])\n\n    return classifier\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.get_generator","title":"<code>get_generator(task, model_name, device, return_full_text=True)</code>","text":"<p>Helper function for creating a generator</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_generator(task, model_name, device, return_full_text=True):\n\"\"\"Helper function for creating a generator\"\"\"\n    pt_path = MODEL_DICT[task][model_name][\"pt_path\"]\n\n    if not os.path.isfile(pt_path):\n        if model_name == \"GPTJ6B\":\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_DICT[task][model_name][\"key\"],\n                revision=\"float16\",\n                torch_dtype=torch.float16,\n                # low_cpu_mem_usage=True\n            )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_DICT[task][model_name][\"key\"]\n            )\n        torch.save(model, pt_path)\n\n    model = torch.load(pt_path)\n\n    if model_name == \"GPTJ6B\":\n        tokenizer = AutoTokenizer.from_pretrained(\n            MODEL_DICT[task][model_name][\"key\"], torch_dtype=torch.float16)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            MODEL_DICT[task][model_name][\"key\"])\n\n    generator = pipeline(task,\n                         model=model,\n                         tokenizer=tokenizer,\n                         device=device,\n                         return_full_text=return_full_text\n                         )\n    return generator, tokenizer\n</code></pre>"},{"location":"reference/utils/recording/logger/","title":"logger","text":""},{"location":"reference/utils/stt/transcriber/","title":"transcriber","text":"<p>Uses the Whisper Model from OpenAI to transcribe audio</p> <p>The whisper model provides state of the art results at a passable speed.</p>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber","title":"<code>Transcriber</code>","text":"<p>Helper class which processes audio clips or files</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>class Transcriber:\n\"\"\" Helper class which processes audio clips or files\n\n    \"\"\"\n\n    def __init__(self, model_size: str = \"medium\", save_dir: str = None) -&gt; None:\n\n        if not save_dir:\n            save_dir = tempfile.mkdtemp()\n        self.save_dir = save_dir\n        self.stt = WhisperSTT(model_size=model_size, save_dir=save_dir)\n        self.diarizer = PyannoteDiarize(save_dir=save_dir)\n\n    def transcribe_clip(self, audio_clip):\n\"\"\"Transcribe bytes of an audio clip\"\"\"\n        text_transcribed = self.stt.transcribe_clip(audio_clip)\n        return text_transcribed\n\n    def transcribe_file(self, file_name, diarize=False):\n\"\"\"Transcribe a file\"\"\"\n        transcription_obj = self.stt.transcribe_file(\n            file_name=file_name)\n\n        if not diarize:\n            return transcription_obj[\"text\"]\n        diarization_segments = self.diarize_file(file_name=file_name)\n\n        diarized_speech = self.combine_whisper_annotation(\n            transcription_obj[\"segments\"], diarization_segments)\n\n        final_diarization = self.combine_speaker_segments(diarized_speech)\n        final_save_path = os.path.join(self.save_dir, \"diarized_transcription.csv\")\n        self.save_csv(final_diarization, final_save_path)\n        return final_diarization\n\n\n    def diarize_file(self, file_name=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\n        diarization_segments = self.diarizer.diarize_file(file_path=file_name)\n        return diarization_segments\n\n    def combine_whisper_annotation(self, whisper_segments, diarization_segments):\n\"\"\"Combines the speaker labels from the diarization with the whisper segments\"\"\"\n        diarized_speech = []\n        outstr = \"\"\n\n        # Determine which speaker segments overlap with transcription segments\n        for whisper_segment in whisper_segments:\n            whisp = Segment(whisper_segment[\"start\"], whisper_segment[\"end\"])\n            speaker_id = {\n                \"id\": \"SPEAKER_00\",\n                \"duration\": 0\n            }\n\n            for diar_segment in diarization_segments:\n                diar, speaker = diar_segment.values()\n                intersection = diar &amp; whisp\n                duration = intersection.duration\n                if duration &gt; speaker_id[\"duration\"]:\n                    speaker_id[\"id\"] = speaker\n                    speaker_id[\"duration\"] = duration\n            speaker_key = speaker_id[\"id\"]\n\n            segment_dict = {\n                \"text\": whisper_segment[\"text\"],\n                \"start\": round(whisper_segment[\"start\"], 2),\n                \"end\": round(whisper_segment[\"end\"], 2),\n                \"speaker\": speaker_key\n            }\n            diarized_speech.append(segment_dict)\n            outstr += f\"{whisper_segment['text']} - {speaker_key}  \\n\"\n        return diarized_speech\n\n    def combine_speaker_segments(self, diarized_speech):\n\"\"\"Combines adjacent segments with the same speaker\n\n        Builds up a new list while iterating through the old segments\n        \"\"\"\n\n        combined_segments = []\n        j = 0\n        for ind, segment in enumerate(diarized_speech):\n            if ind &lt; j:\n                continue\n            cur_speaker = segment[\"speaker\"]\n            start = segment[\"start\"]\n            text = \"\"\n\n            while j &lt; len(diarized_speech) and cur_speaker == diarized_speech[j][\"speaker\"]:\n                text += diarized_speech[j][\"text\"]\n                j += 1\n\n            end = diarized_speech[j-1][\"end\"]\n            new_d = {\n                \"speaker\": cur_speaker,\n                \"text\": text,\n                \"start\": start,\n                \"end\": end,\n            }\n            combined_segments.append(new_d)\n\n        return combined_segments\n\n    def save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\n        with open(filename, 'w') as diraization_file:\n            writer = csv.writer(diraization_file)\n            writer.writerow(diarization[0].keys())\n            for segment in diarization:\n                writer.writerow(segment.values())\n            diraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.combine_speaker_segments","title":"<code>combine_speaker_segments(diarized_speech)</code>","text":"<p>Combines adjacent segments with the same speaker</p> <p>Builds up a new list while iterating through the old segments</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def combine_speaker_segments(self, diarized_speech):\n\"\"\"Combines adjacent segments with the same speaker\n\n    Builds up a new list while iterating through the old segments\n    \"\"\"\n\n    combined_segments = []\n    j = 0\n    for ind, segment in enumerate(diarized_speech):\n        if ind &lt; j:\n            continue\n        cur_speaker = segment[\"speaker\"]\n        start = segment[\"start\"]\n        text = \"\"\n\n        while j &lt; len(diarized_speech) and cur_speaker == diarized_speech[j][\"speaker\"]:\n            text += diarized_speech[j][\"text\"]\n            j += 1\n\n        end = diarized_speech[j-1][\"end\"]\n        new_d = {\n            \"speaker\": cur_speaker,\n            \"text\": text,\n            \"start\": start,\n            \"end\": end,\n        }\n        combined_segments.append(new_d)\n\n    return combined_segments\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.combine_whisper_annotation","title":"<code>combine_whisper_annotation(whisper_segments, diarization_segments)</code>","text":"<p>Combines the speaker labels from the diarization with the whisper segments</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def combine_whisper_annotation(self, whisper_segments, diarization_segments):\n\"\"\"Combines the speaker labels from the diarization with the whisper segments\"\"\"\n    diarized_speech = []\n    outstr = \"\"\n\n    # Determine which speaker segments overlap with transcription segments\n    for whisper_segment in whisper_segments:\n        whisp = Segment(whisper_segment[\"start\"], whisper_segment[\"end\"])\n        speaker_id = {\n            \"id\": \"SPEAKER_00\",\n            \"duration\": 0\n        }\n\n        for diar_segment in diarization_segments:\n            diar, speaker = diar_segment.values()\n            intersection = diar &amp; whisp\n            duration = intersection.duration\n            if duration &gt; speaker_id[\"duration\"]:\n                speaker_id[\"id\"] = speaker\n                speaker_id[\"duration\"] = duration\n        speaker_key = speaker_id[\"id\"]\n\n        segment_dict = {\n            \"text\": whisper_segment[\"text\"],\n            \"start\": round(whisper_segment[\"start\"], 2),\n            \"end\": round(whisper_segment[\"end\"], 2),\n            \"speaker\": speaker_key\n        }\n        diarized_speech.append(segment_dict)\n        outstr += f\"{whisper_segment['text']} - {speaker_key}  \\n\"\n    return diarized_speech\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.diarize_file","title":"<code>diarize_file(file_name=None)</code>","text":"<p>Diarize a file or the file that has been saved</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def diarize_file(self, file_name=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\n    diarization_segments = self.diarizer.diarize_file(file_path=file_name)\n    return diarization_segments\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.save_csv","title":"<code>save_csv(diarization, filename='diarization.csv')</code>","text":"<p>Save diarization to csv</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\n    with open(filename, 'w') as diraization_file:\n        writer = csv.writer(diraization_file)\n        writer.writerow(diarization[0].keys())\n        for segment in diarization:\n            writer.writerow(segment.values())\n        diraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.transcribe_clip","title":"<code>transcribe_clip(audio_clip)</code>","text":"<p>Transcribe bytes of an audio clip</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def transcribe_clip(self, audio_clip):\n\"\"\"Transcribe bytes of an audio clip\"\"\"\n    text_transcribed = self.stt.transcribe_clip(audio_clip)\n    return text_transcribed\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.transcribe_file","title":"<code>transcribe_file(file_name, diarize=False)</code>","text":"<p>Transcribe a file</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def transcribe_file(self, file_name, diarize=False):\n\"\"\"Transcribe a file\"\"\"\n    transcription_obj = self.stt.transcribe_file(\n        file_name=file_name)\n\n    if not diarize:\n        return transcription_obj[\"text\"]\n    diarization_segments = self.diarize_file(file_name=file_name)\n\n    diarized_speech = self.combine_whisper_annotation(\n        transcription_obj[\"segments\"], diarization_segments)\n\n    final_diarization = self.combine_speaker_segments(diarized_speech)\n    final_save_path = os.path.join(self.save_dir, \"diarized_transcription.csv\")\n    self.save_csv(final_diarization, final_save_path)\n    return final_diarization\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.main","title":"<code>main()</code>","text":"<p>Test Transcriber</p> <p>To run this file directly, do so from the utils directory: python -m stt.transcriber</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def main():\n\"\"\"Test Transcriber\n\n    To run this file directly, do so from the utils directory:\n    python -m stt.transcriber\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--model\", default=\"base\", help=\"Model to use\",\n                        choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n    parser.add_argument(\"--record\", default=False,\n                        help=\"record a 20 second example\", type=bool)\n    parser.add_argument(\"--filename\", default=\"stt/backends/output/diarization_test.wav\",\n                        help=\"location of file to transcribe\")\n\n    args = parser.parse_args()\n\n    transcribe = Transcriber(save_dir=\"stt/backends/output\")\n    print(\"loading complete\")\n\n    if args.record:\n        record_mic(args.filename)\n\n    result = transcribe.transcribe_file(file_name=args.filename, diarize=True)\n    print(result)\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/","title":"pyannote_diarization","text":"<p>Uses the Pyannot Diarization Module</p>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize","title":"<code>PyannoteDiarize</code>","text":"<p>Helper class which processes audio clips or files</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>class PyannoteDiarize:\n\"\"\" Helper class which processes audio clips or files\n\n    \"\"\"\n    def __init__(self, save_dir: str=None) -&gt; None:\n\n        if not save_dir:\n            save_dir=tempfile.mkdtemp()\n\n        self.diarization_path = os.path.join(save_dir, \"diarization.csv\")\n        self.diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"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\")\n\n\n    def diarize_file(self, file_path=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\n        diarization_output = self.diarization_pipeline(file_path)\n        self.diarization_segments = []\n        for segment, _, speaker in diarization_output.itertracks(yield_label=True):\n            self.diarization_segments.append({\n                \"segment\":segment,\n                \"speaker id\":speaker\n            })\n\n        self.save_csv(self.diarization_segments, filename=self.diarization_path)\n        return self.diarization_segments\n\n    def save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\n        with open(filename,'w') as diraization_file:\n            writer = csv.writer(diraization_file)\n            writer.writerow(diarization[0].keys())\n            for segment in diarization:\n                writer.writerow(segment.values())\n            diraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize.diarize_file","title":"<code>diarize_file(file_path=None)</code>","text":"<p>Diarize a file or the file that has been saved</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def diarize_file(self, file_path=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\n    diarization_output = self.diarization_pipeline(file_path)\n    self.diarization_segments = []\n    for segment, _, speaker in diarization_output.itertracks(yield_label=True):\n        self.diarization_segments.append({\n            \"segment\":segment,\n            \"speaker id\":speaker\n        })\n\n    self.save_csv(self.diarization_segments, filename=self.diarization_path)\n    return self.diarization_segments\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize.save_csv","title":"<code>save_csv(diarization, filename='diarization.csv')</code>","text":"<p>Save diarization to csv</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\n    with open(filename,'w') as diraization_file:\n        writer = csv.writer(diraization_file)\n        writer.writerow(diarization[0].keys())\n        for segment in diarization:\n            writer.writerow(segment.values())\n        diraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.main","title":"<code>main()</code>","text":"<p>Test PyannoteDiarize</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def main():\n\"\"\"Test PyannoteDiarize\"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--filename\", default=\"output/test.wav\",\n                        help=\"location of file to transcribe\")\n    args = parser.parse_args()\n\n    diarizer = PyannoteDiarize(save_dir=\"output\")\n\n    results = diarizer.diarize_file(file_path=args.filename)\n    for segment in results:\n        print(segment)\n</code></pre>"},{"location":"reference/utils/stt/backends/record_mic/","title":"record_mic","text":"<p>Helper function for recording a microphone to a file</p>"},{"location":"reference/utils/stt/backends/record_mic/#backend.app.utils.stt.backends.record_mic.main","title":"<code>main()</code>","text":"<p>Example of making a simple audio clip</p> Source code in <code>backend/app/utils/stt/backends/record_mic.py</code> <pre><code>def main():\n\"\"\"Example of making a simple audio clip\"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--record_for\", default=20,\n                        help=\"Set recording length (seconds)\", type=bool)\n    parser.add_argument(\"--filename\", default=\"output/test.wav\",\n                        help=\"location to save audio file\")\n\n    args = parser.parse_args()\n\n    record_mic(args.filename, seconds=args.record_for)\n    print(f\"File saved to {args.filename}\")\n</code></pre>"},{"location":"reference/utils/stt/backends/record_mic/#backend.app.utils.stt.backends.record_mic.record_mic","title":"<code>record_mic(filename, seconds=200)</code>","text":"<p>Helper function for recording a microphone to a file</p> Source code in <code>backend/app/utils/stt/backends/record_mic.py</code> <pre><code>def record_mic(filename, seconds=200):\n\"\"\"Helper function for recording a microphone to a file\"\"\"\n    chunk = 1024  # Record in chunks of 1024 samples\n    sample_format = pyaudio.paInt16  # 16 bits per sample\n    channels = 2\n    framerate = 16000  # Record at 44100 samples per second\n    audio_interface = pyaudio.PyAudio()  # Create an interface to PortAudio\n\n    print('Recording')\n\n    stream = audio_interface.open(format=sample_format,\n                    channels=channels,\n                    rate=framerate,\n                    frames_per_buffer=chunk,\n                    input=True)\n\n    frames = []  # Initialize array to store frames\n\n    # Store data in chunks for 3 seconds\n    for _ in range(0, int(framerate / chunk * seconds)):\n        data = stream.read(chunk)\n        frames.append(data)\n\n    # Stop and close the stream\n    stream.stop_stream()\n    stream.close()\n    # Terminate the PortAudio interface\n    audio_interface.terminate()\n\n    print('Finished recording')\n\n    # Save the recorded data as a WAV file\n    wavefile = wave.open(filename, 'wb')\n    wavefile.setnchannels(channels)\n    wavefile.setsampwidth(audio_interface.get_sample_size(sample_format))\n    wavefile.setframerate(framerate)\n    wavefile.writeframes(b''.join(frames))\n    wavefile.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/","title":"whisper_stt","text":"<p>Uses the Whisper Model from OpenAI to transcribe audio</p> <p>The whisper model provides state of the art results at a passable speed.</p>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT","title":"<code>WhisperSTT</code>","text":"<p>Helper class which processes audio clips or files</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>class WhisperSTT:\n\"\"\" Helper class which processes audio clips or files\n\n    \"\"\"\n\n    def __init__(self, model_size: str = \"medium\", save_dir: str = None) -&gt; None:\n\n        if not save_dir:\n            save_dir = tempfile.mkdtemp()\n\n        self.default_wave_path = os.path.join(save_dir, \"test.wav\")\n        self.transcription_path = os.path.join(save_dir, \"transcription.csv\")\n        self.audio_model = whisper.load_model(model_size)\n\n    def transcribe_clip(self, audio_clip):\n\"\"\"Transcribe bytes of an audio clip\"\"\"\n        audio_clip.export(self.default_wave_path, format=\"wav\")\n        result = self.audio_model.transcribe(self.default_wave_path, language='english')\n        return result[\"text\"]\n\n    def transcribe_file(self, file_name=None):\n\"\"\"Transcribe a file\"\"\"\n        if file_name:\n            self.default_wave_path = file_name\n\n        result = self.audio_model.transcribe(self.default_wave_path, language='english')\n        self.save_csv(result[\"segments\"], self.transcription_path)\n        return result\n\n    def save_csv(self, segments, filename=\"speech_segments.csv\"):\n\"\"\"Save csv of speech segments\"\"\"\n        with open(filename, 'w') as my_file:\n            writer = csv.writer(my_file)\n            writer.writerow(segments[0].keys())\n            for seg in segments:\n                writer.writerow(seg.values())\n            my_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.save_csv","title":"<code>save_csv(segments, filename='speech_segments.csv')</code>","text":"<p>Save csv of speech segments</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def save_csv(self, segments, filename=\"speech_segments.csv\"):\n\"\"\"Save csv of speech segments\"\"\"\n    with open(filename, 'w') as my_file:\n        writer = csv.writer(my_file)\n        writer.writerow(segments[0].keys())\n        for seg in segments:\n            writer.writerow(seg.values())\n        my_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.transcribe_clip","title":"<code>transcribe_clip(audio_clip)</code>","text":"<p>Transcribe bytes of an audio clip</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def transcribe_clip(self, audio_clip):\n\"\"\"Transcribe bytes of an audio clip\"\"\"\n    audio_clip.export(self.default_wave_path, format=\"wav\")\n    result = self.audio_model.transcribe(self.default_wave_path, language='english')\n    return result[\"text\"]\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.transcribe_file","title":"<code>transcribe_file(file_name=None)</code>","text":"<p>Transcribe a file</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def transcribe_file(self, file_name=None):\n\"\"\"Transcribe a file\"\"\"\n    if file_name:\n        self.default_wave_path = file_name\n\n    result = self.audio_model.transcribe(self.default_wave_path, language='english')\n    self.save_csv(result[\"segments\"], self.transcription_path)\n    return result\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.main","title":"<code>main()</code>","text":"<p>Test of WhisperSTT</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def main():\n\"\"\"Test of WhisperSTT\"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--model\", default=\"base\", help=\"Model to use\",\n                        choices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\n    parser.add_argument(\"--filename\", default=\"output/test.wav\",\n                        help=\"location of file to transcribe\")\n\n    args = parser.parse_args()\n\n    transcriber = WhisperSTT(model_size=args.model, save_dir=\"output\")\n    print(\"loading complete\")\n\n    filename = args.filename\n\n    result = transcriber.transcribe_file(file_name=filename)\n    print(result['text'])\n</code></pre>"},{"location":"reference/utils/tts/speaker/","title":"speaker","text":"<p>Speaker wraps tts backends and exposes common API for app</p>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.Speaker","title":"<code>Speaker</code>","text":"<p>Speaker takes text and returns an audio stream and a viseme list with timing</p> <p>Speaker backend is set at runtime, but speaker ID can change dynammically</p> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>class Speaker:\n\"\"\"Speaker takes text and returns an audio stream and a viseme list with timing\n\n    Speaker backend is set at runtime, but speaker ID can change dynammically\n    \"\"\"\n    def __init__(self, backend=\"polly\") -&gt; None:\n        self.backend = backend\n        if self.backend == \"polly\":\n            self.speaker = PollySpeak()\n        if self.backend == \"coqui\":\n            self.speaker = CoquiSpeak()\n        self.viseme_generator = VisemeGenerator()\n\n    def synthesize(self, input_text: str, speaker_identifier: str):\n\"\"\"Takes in text and a speaker id and returns speech and visemes and timings\"\"\"\n        if self.backend == \"polly\":\n            audio_stream, visemes, delays = self.speaker.synthesize(input_text,speaker_id=speaker_identifier)\n            visemes = self.viseme_generator.convert_aws_visemes(visemes)\n\n        if self.backend == \"coqui\":\n            audio_stream, speaking_time = self.speaker.synthesize_wav(input_text, speaker_id=speaker_identifier)\n            visemes = self.viseme_generator.get_visemes(input_text)\n            viseme_length = (speaking_time) / (len(visemes)+1)\n            delays = [viseme_length for i in range(len(visemes))]\n\n        return audio_stream, visemes, delays\n</code></pre>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.Speaker.synthesize","title":"<code>synthesize(input_text, speaker_identifier)</code>","text":"<p>Takes in text and a speaker id and returns speech and visemes and timings</p> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>def synthesize(self, input_text: str, speaker_identifier: str):\n\"\"\"Takes in text and a speaker id and returns speech and visemes and timings\"\"\"\n    if self.backend == \"polly\":\n        audio_stream, visemes, delays = self.speaker.synthesize(input_text,speaker_id=speaker_identifier)\n        visemes = self.viseme_generator.convert_aws_visemes(visemes)\n\n    if self.backend == \"coqui\":\n        audio_stream, speaking_time = self.speaker.synthesize_wav(input_text, speaker_id=speaker_identifier)\n        visemes = self.viseme_generator.get_visemes(input_text)\n        viseme_length = (speaking_time) / (len(visemes)+1)\n        delays = [viseme_length for i in range(len(visemes))]\n\n    return audio_stream, visemes, delays\n</code></pre>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.main","title":"<code>main()</code>","text":"<p>Integration testing for speaker</p> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>def main():\n\"\"\"Integration testing for speaker\"\"\"\n    speaker = Speaker(backend=\"polly\")\n    audio_stream, visemes, delays = speaker.synthesize(\"This is a test\", \"Kevin\")\n    z = zip(visemes,delays)\n    for i in z:\n        print(i)\n\n    speaker = Speaker(backend=\"coqui\")\n    audio_stream, visemes, delays = speaker.synthesize(\"This is a test\", \"p267\")\n    z = zip(visemes,delays)\n    for i in z:\n        print(i)\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/","title":"aws_polly_tts","text":"<p>Module for calling on AWS Polly</p>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.PollySpeak","title":"<code>PollySpeak</code>","text":"<p>Synthesizes speech with AWS Polly</p> Possible english speakers include <p>US English en-US {'Kevin', 'Salli', 'Matthew', 'Kimberly', 'Kendra', 'Justin', 'Joey', 'Joanna', 'Ivy'} New Zealand English en-NZ {'Aria'} South African English en-ZA {'Ayanda'} British English en-GB {'Emma', 'Brian', 'Amy', 'Arthur'} Australian English en-AU {'Olivia'} Indian English en-IN {'Kajal'}</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>class PollySpeak():\n\"\"\" Synthesizes speech with AWS Polly\n\n    Possible english speakers include:\n        US English en-US {'Kevin', 'Salli', 'Matthew', 'Kimberly', 'Kendra', 'Justin', 'Joey', 'Joanna', 'Ivy'}\n        New Zealand English en-NZ {'Aria'}\n        South African English en-ZA {'Ayanda'}\n        British English en-GB {'Emma', 'Brian', 'Amy', 'Arthur'}\n        Australian English en-AU {'Olivia'}\n        Indian English en-IN {'Kajal'}\n    \"\"\"\n\n    def __init__(self, save_path:str=\"./output/temp.wav\") -&gt; None:\n        self.engine = \"neural\"\n        self.audio_format = \"mp3\"\n        self.polly_client = polly\n        self.path = Path(__file__).parent\n        self.save_path = os.path.join(self.path, save_path)\n\n    def synthesize(self, text: str, speaker_id: str = \"\"):\n\"\"\"Turns text into audio and visemes\"\"\"\n        lang_code = None\n        for key, names in english_speaker_map.items():\n            if speaker_id in names:\n                lang_code = key\n                voice = speaker_id\n        if not lang_code:\n            lang_code = \"en-US\"\n            voice = 'Kendra'\n        try:\n            kwargs = {\n                'Engine': self.engine,\n                'OutputFormat': self.audio_format,\n                'Text': text,\n                'VoiceId': voice}\n            if lang_code is not None:\n                kwargs['LanguageCode'] = lang_code\n\n            response = self.polly_client.synthesize_speech(**kwargs)\n            print(\"got response\", response)\n            audio_stream = response['AudioStream']\n            output = self.save_path\n            with closing(audio_stream) as stream:                \n                with open(output, \"wb\") as file:\n                    file.write(stream.read())\n            outstream = io.open(output, 'rb', buffering=0)\n            visemes = None\n            kwargs['OutputFormat'] = 'json'\n            kwargs['SpeechMarkTypes'] = ['viseme']\n            response = self.polly_client.synthesize_speech(**kwargs)\n            visemes = [json.loads(viseme) for viseme in\n                       response['AudioStream'].read().decode().split() if viseme]\n            viseme_list = []\n            time_list = []\n            for viseme in visemes:\n                viseme_list.append(viseme[\"value\"])\n                time_list.append(viseme[\"time\"])\n            sleep_times = []\n            t_before = 0\n            for next_t in time_list:\n                wait_seconds = float(next_t) - float(t_before)\n                sleep_times.append(wait_seconds/1000)\n                t_before = next_t\n        except ClientError as exc:\n            print(exc)\n            raise\n        else:\n            return outstream, viseme_list, sleep_times\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.PollySpeak.synthesize","title":"<code>synthesize(text, speaker_id='')</code>","text":"<p>Turns text into audio and visemes</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>def synthesize(self, text: str, speaker_id: str = \"\"):\n\"\"\"Turns text into audio and visemes\"\"\"\n    lang_code = None\n    for key, names in english_speaker_map.items():\n        if speaker_id in names:\n            lang_code = key\n            voice = speaker_id\n    if not lang_code:\n        lang_code = \"en-US\"\n        voice = 'Kendra'\n    try:\n        kwargs = {\n            'Engine': self.engine,\n            'OutputFormat': self.audio_format,\n            'Text': text,\n            'VoiceId': voice}\n        if lang_code is not None:\n            kwargs['LanguageCode'] = lang_code\n\n        response = self.polly_client.synthesize_speech(**kwargs)\n        print(\"got response\", response)\n        audio_stream = response['AudioStream']\n        output = self.save_path\n        with closing(audio_stream) as stream:                \n            with open(output, \"wb\") as file:\n                file.write(stream.read())\n        outstream = io.open(output, 'rb', buffering=0)\n        visemes = None\n        kwargs['OutputFormat'] = 'json'\n        kwargs['SpeechMarkTypes'] = ['viseme']\n        response = self.polly_client.synthesize_speech(**kwargs)\n        visemes = [json.loads(viseme) for viseme in\n                   response['AudioStream'].read().decode().split() if viseme]\n        viseme_list = []\n        time_list = []\n        for viseme in visemes:\n            viseme_list.append(viseme[\"value\"])\n            time_list.append(viseme[\"time\"])\n        sleep_times = []\n        t_before = 0\n        for next_t in time_list:\n            wait_seconds = float(next_t) - float(t_before)\n            sleep_times.append(wait_seconds/1000)\n            t_before = next_t\n    except ClientError as exc:\n        print(exc)\n        raise\n    else:\n        return outstream, viseme_list, sleep_times\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.main","title":"<code>main()</code>","text":"<p>Testing the integrated functionality of PollySpeak</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>def main():\n\"\"\"Testing the integrated functionality of PollySpeak\"\"\"\n\n    s = PollySpeak(save_path=\"./output/temp.mp3\")\n    example = \"Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\"\n\n    try:\n        # Request speech synthesis\n        _, viseme_list, sleep_times = s.synthesize(example, \"Aria\")\n    except (BotoCoreError, ClientError) as error:\n        # The service returned an error, exit gracefully\n        print(error)\n        sys.exit(-1)\n\n    # Play the audio using the platform's default player\n    if sys.platform == \"win32\":\n        os.startfile(\"./output/temp.mp3\")\n    else:\n        # The following works on macOS and Linux. (Darwin = mac, xdg-open = linux).\n        opener = \"open\" if sys.platform == \"darwin\" else \"xdg-open\"\n\n        subprocess.call([opener, \"./output/temp.mp3\"])\n        for ind, sleep in enumerate(sleep_times):\n            time.sleep(sleep)\n            print(sleep, viseme_list[ind])\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/","title":"coqui_tts","text":"<p>Open Source Module wrapping coqui's library for text to speech.</p> <p>TODO this file is still in need of a fair amount of cleanup</p>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.CoquiSpeak","title":"<code>CoquiSpeak</code>","text":"<p>Setup Model and perform synthesis</p> Speakers of note for the vits model <p>267!,307 - English male, medium 330!,232 - English male, slow 312!,251 - English male, fast 287,254 - English male, fast and deep 303 - English female, slow 306 - English female, medium 308 - English female, slow 295!,270 - American female, slow 317! - American male, slow 230! - American male, fast 345 - south african female, slow 313,233 - ? male, fast</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>class CoquiSpeak:\n\"\"\"Setup Model and perform synthesis\n\n    Speakers of note for the vits model:\n        267!,307 - English male, medium\n        330!,232 - English male, slow\n        312!,251 - English male, fast\n        287,254 - English male, fast and deep\n        303 - English female, slow\n        306 - English female, medium\n        308 - English female, slow\n        295!,270 - American female, slow\n        317! - American male, slow\n        230! - American male, fast\n        345 - south african female, slow\n        313,233 - ? male, fast\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        # parse the args\n        args, _ = create_argparser().parse_known_args()\n\n        self.path = Path(__file__).parent\n\n        manager = ModelManager(os.path.join(\n            self.path, \"resources/.coqui_tts_models.json\"))\n\n        # update in-use models to the specified released models.\n        model_path = None\n        config_path = None\n        speakers_file_path = None\n        vocoder_path = None\n        vocoder_config_path = None\n\n        # CASE1: list pre-trained TTS models\n        if args.list_models:\n            manager.list_models()\n            sys.exit()\n\n        # CASE2: load pre-trained model paths\n        if args.model_name is not None and not args.model_path:\n            model_path, config_path, model_item = manager.download_model(\n                args.model_name)\n            args.vocoder_name = model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name\n\n        if args.vocoder_name is not None and not args.vocoder_path:\n            vocoder_path, vocoder_config_path, _ = manager.download_model(\n                args.vocoder_name)\n\n        # CASE3: set custom model paths\n        if args.model_path is not None:\n            model_path = args.model_path\n            config_path = args.config_path\n            speakers_file_path = args.speakers_file_path\n\n        if args.vocoder_path is not None:\n            vocoder_path = args.vocoder_path\n            vocoder_config_path = args.vocoder_config_path\n\n        # load models\n        self.synthesizer = Synthesizer(\n            tts_checkpoint=model_path,\n            tts_config_path=config_path,\n            tts_speakers_file=speakers_file_path,\n            tts_languages_file=None,\n            vocoder_checkpoint=vocoder_path,\n            vocoder_config=vocoder_config_path,\n            encoder_checkpoint=\"\",\n            encoder_config=\"\",\n            use_cuda=args.use_cuda,\n        )\n\n        self.use_multi_speaker = hasattr(self.synthesizer.tts_model, \"num_speakers\") and (\n            self.synthesizer.tts_model.num_speakers &gt; 1 or self.synthesizer.tts_speakers_file is not None\n        )\n\n        self.speaker_manager = getattr(\n            self.synthesizer.tts_model, \"speaker_manager\", None)\n        # TODO: set this from SpeakerManager\n        use_gst = self.synthesizer.tts_config.get(\"use_gst\", False)\n        self.speaker_ids = self.speaker_manager.name_to_id if self.speaker_manager is not None else None\n\n    def synthesize_wav(self, text: str, speaker_id: str = \"\", style_wav: str = \"\"):\n\"\"\"Turn text into a wav file and return byte stream\"\"\"\n        style_wav = style_wav_uri_to_dict(style_wav)\n        try:\n            wavs = self.synthesizer.tts(\n                text, speaker_name=speaker_id, style_wav=style_wav)\n        except Exception as exc:\n            print(exc)\n        outstream = io.BytesIO()\n        self.synthesizer.save_wav(wavs, outstream)\n        save_path = os.path.join(\n            self.path, f\"output/test_speech{speaker_id}.wav\")\n        # print(self.synthesizer.tts_model.speaker_manager.name_to_id)\n        self.synthesizer.save_wav(wavs, save_path)\n        sound = sf.SoundFile(save_path)\n        speaking_length = sound.frames / sound.samplerate\n        # print('Sound file timeing is seconds = {}'.format(speaking_length))\n        return outstream, speaking_length\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.CoquiSpeak.synthesize_wav","title":"<code>synthesize_wav(text, speaker_id='', style_wav='')</code>","text":"<p>Turn text into a wav file and return byte stream</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def synthesize_wav(self, text: str, speaker_id: str = \"\", style_wav: str = \"\"):\n\"\"\"Turn text into a wav file and return byte stream\"\"\"\n    style_wav = style_wav_uri_to_dict(style_wav)\n    try:\n        wavs = self.synthesizer.tts(\n            text, speaker_name=speaker_id, style_wav=style_wav)\n    except Exception as exc:\n        print(exc)\n    outstream = io.BytesIO()\n    self.synthesizer.save_wav(wavs, outstream)\n    save_path = os.path.join(\n        self.path, f\"output/test_speech{speaker_id}.wav\")\n    # print(self.synthesizer.tts_model.speaker_manager.name_to_id)\n    self.synthesizer.save_wav(wavs, save_path)\n    sound = sf.SoundFile(save_path)\n    speaking_length = sound.frames / sound.samplerate\n    # print('Sound file timeing is seconds = {}'.format(speaking_length))\n    return outstream, speaking_length\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.create_argparser","title":"<code>create_argparser()</code>","text":"<p>Parse args to set defaults</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def create_argparser():\n\"\"\"Parse args to set defaults\"\"\"\n\n    def convert_boolean(item):\n        return item.lower() in [\"true\", \"1\", \"yes\"]\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--list_models\",\n        type=convert_boolean,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"list available pre-trained tts and vocoder models.\",\n    )\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=\"tts_models/en/vctk/vits\",\n        help=\"Name of one of the pre-trained tts models in format &lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;\",\n    )\n    parser.add_argument(\"--vocoder_name\", type=str, default=None,\n                        help=\"name of one of the released vocoder models.\")\n\n    # Args for running custom models\n    parser.add_argument(\"--config_path\", default=None,\n                        type=str, help=\"Path to model config file.\")\n    parser.add_argument(\n        \"--model_path\",\n        type=str,\n        default=None,\n        help=\"Path to model file.\",\n    )\n    parser.add_argument(\n        \"--vocoder_path\",\n        type=str,\n        help=\"Path to vocoder model file. If it is not defined, model uses GL as vocoder. Please make sure that you installed vocoder library before (WaveRNN).\",\n        default=None,\n    )\n    parser.add_argument(\"--vocoder_config_path\", type=str,\n                        help=\"Path to vocoder model config file.\", default=None)\n    parser.add_argument(\"--speakers_file_path\", type=str,\n                        help=\"JSON file for multi-speaker model.\", default=None)\n    parser.add_argument(\"--port\", type=int, default=5002,\n                        help=\"port to listen on.\")\n    parser.add_argument(\"--use_cuda\", type=convert_boolean,\n                        default=True, help=\"true to use CUDA.\")\n    parser.add_argument(\"--debug\", type=convert_boolean,\n                        default=False, help=\"true to enable Flask debug mode.\")\n    parser.add_argument(\"--show_details\", type=convert_boolean,\n                        default=False, help=\"Generate model detail page.\")\n    return parser\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.main","title":"<code>main()</code>","text":"<p>Integration testing of Coqui Speaker</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def main():\n\"\"\"Integration testing of Coqui Speaker\"\"\"\n    # text = \"I am your personal virtual assistant. I am quick witted, helpful, and frendly.\"\n    example_text = \"Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\"\n    # test_text = \"For. the. record. It. pleases. the. court. to. be. wrong. About. this\"\n    # period and ?, long pause from breaking into list\n    # colon, semicolon, comma short pause\n    # ! breaks things up unpredictably\n    k = \"p267\"\n    tts = CoquiSpeak()\n    _, speaking_time = tts.synthesize_wav(example_text, k)\n    print(f\"File length: {speaking_time}\")\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.style_wav_uri_to_dict","title":"<code>style_wav_uri_to_dict(style_wav)</code>","text":"<p>Transform an uri style_wav, in either a string (path to wav file to be use for style transfer) or a dict (gst tokens/values to be use for styling)</p> <p>Parameters:</p> Name Type Description Default <code>style_wav</code> <code>str</code> <p>uri</p> required <p>Returns:</p> Type Description <code>Union[str, dict]</code> <p>Union[str, dict]: path to file (str) or gst style (dict)</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def style_wav_uri_to_dict(style_wav: str) -&gt; Union[str, dict]:\n\"\"\"Transform an uri style_wav, in either a string (path to wav file to be\n    use for style transfer) or a dict (gst tokens/values to be use for styling)\n\n    Args:\n        style_wav (str): uri\n\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"\n    if style_wav:\n        if os.path.isfile(style_wav) and style_wav.endswith(\".wav\"):\n            return style_wav  # style_wav is a .wav file located on the server\n\n        style_wav = json.loads(style_wav)\n        # style_wav is a gst dictionary with {token1_id : token1_weigth, ...}\n        return style_wav\n    return None\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/","title":"viseme_generator","text":"<p>Generate Visemes from text.</p> <p>This module converts text to phonemes and then phonemes to visemes.</p>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator","title":"<code>VisemeGenerator</code>","text":"<p>Contains the functionality to convert text to visemes</p> <p>Or to convert between viseme types</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>class VisemeGenerator:\n\"\"\"Contains the functionality to convert text to visemes\n\n    Or to convert between viseme types\n    \"\"\"\n\n    def __init__(self, convertion_table=\"./resources/.phoneme-viseme_map.csv\", log=False) -&gt; None:\n\n        self.log = log\n        self.path = Path(__file__).parent\n        conversion_table_path = os.path.join(self.path, convertion_table)\n\n        self.backend = EspeakBackend('en-us', preserve_punctuation=True, with_stress=False)\n\n        self.joint_df = pd.read_csv(conversion_table_path, header=0)\n        self.joint_df.set_index(\"IPA\")\n\n    def get_viseme(self, ipa, type='IPA'):\n\"\"\"Converts individual phoneme to a viseme\"\"\"\n        try:\n            viseme_dict = self.joint_df[self.joint_df[type]==ipa]\n            if len(viseme_dict) &lt; 1:\n                viseme_dict = self.joint_df[self.joint_df['Alternative IPA'] == ipa]\n                if len(viseme_dict) &lt; 1:\n                    if self.log:\n                        print(f\"Viseme for: {ipa} NOT FOUND\")\n            viseme = viseme_dict[\"SimpleViseme\"].values[0]\n        except Exception as exc:\n            print(exc)\n            viseme = \"IDLE\"\n        return viseme\n\n    def process_phoneme_string(self, phonemes_string):\n\"\"\"Converts phoneme string to list\n\n        Handles a bug with a special character as well\n        \"\"\"\n        phoneme_list = []\n        for phoneme in phonemes_string:\n            if phoneme != \"\u02d0\":\n                phoneme_list.append(phoneme)\n            else:\n                phoneme_list[-1] = phoneme_list[-1] + \"\u02d0\"\n                                    # note this is a special character, not a colon\n        if self.log:\n            print(phoneme_list)\n        return phoneme_list\n\n    def convert_aws_visemes(self, visemes):\n        new_visemes = []\n        for vis in visemes:\n            new_visemes.append(self.get_viseme(vis, type=\"Viseme\"))\n        return new_visemes\n\n    def get_visemes(self, sentence, return_phonemes = False):\n\"\"\"Process a sentence or list of sentences into visemes\"\"\"\n        if isinstance(sentence, str):\n            sentence = [sentence]\n        if self.log:\n            print(f\"String to process: {sentence}\")\n        phonemized = self.backend.phonemize(sentence, strip=False)[0]\n        if self.log:\n            print(f\"Phonemes: {phonemized}\")\n        phoneme_list = self.process_phoneme_string(phonemized) + [' ']\n        visemes_list = []\n        for phoneme in phoneme_list:\n            vis = self.get_viseme(phoneme)\n            if vis == \"IDLE\":# Append Idles twice to give better pauses\n                visemes_list.append(vis)\n            visemes_list.append(vis)\n\n        if return_phonemes:\n            return visemes_list, phoneme_list\n        return visemes_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.get_viseme","title":"<code>get_viseme(ipa, type='IPA')</code>","text":"<p>Converts individual phoneme to a viseme</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def get_viseme(self, ipa, type='IPA'):\n\"\"\"Converts individual phoneme to a viseme\"\"\"\n    try:\n        viseme_dict = self.joint_df[self.joint_df[type]==ipa]\n        if len(viseme_dict) &lt; 1:\n            viseme_dict = self.joint_df[self.joint_df['Alternative IPA'] == ipa]\n            if len(viseme_dict) &lt; 1:\n                if self.log:\n                    print(f\"Viseme for: {ipa} NOT FOUND\")\n        viseme = viseme_dict[\"SimpleViseme\"].values[0]\n    except Exception as exc:\n        print(exc)\n        viseme = \"IDLE\"\n    return viseme\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.get_visemes","title":"<code>get_visemes(sentence, return_phonemes=False)</code>","text":"<p>Process a sentence or list of sentences into visemes</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def get_visemes(self, sentence, return_phonemes = False):\n\"\"\"Process a sentence or list of sentences into visemes\"\"\"\n    if isinstance(sentence, str):\n        sentence = [sentence]\n    if self.log:\n        print(f\"String to process: {sentence}\")\n    phonemized = self.backend.phonemize(sentence, strip=False)[0]\n    if self.log:\n        print(f\"Phonemes: {phonemized}\")\n    phoneme_list = self.process_phoneme_string(phonemized) + [' ']\n    visemes_list = []\n    for phoneme in phoneme_list:\n        vis = self.get_viseme(phoneme)\n        if vis == \"IDLE\":# Append Idles twice to give better pauses\n            visemes_list.append(vis)\n        visemes_list.append(vis)\n\n    if return_phonemes:\n        return visemes_list, phoneme_list\n    return visemes_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.process_phoneme_string","title":"<code>process_phoneme_string(phonemes_string)</code>","text":"<p>Converts phoneme string to list</p> <p>Handles a bug with a special character as well</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def process_phoneme_string(self, phonemes_string):\n\"\"\"Converts phoneme string to list\n\n    Handles a bug with a special character as well\n    \"\"\"\n    phoneme_list = []\n    for phoneme in phonemes_string:\n        if phoneme != \"\u02d0\":\n            phoneme_list.append(phoneme)\n        else:\n            phoneme_list[-1] = phoneme_list[-1] + \"\u02d0\"\n                                # note this is a special character, not a colon\n    if self.log:\n        print(phoneme_list)\n    return phoneme_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.main","title":"<code>main()</code>","text":"<p>Integration testing for viseme generator</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def main():\n\"\"\"Integration testing for viseme generator\"\"\"\n    text = [\"Hello, world! Welcome to the arena?\"]\n    gen = VisemeGenerator()\n    visemes,phonemes = gen.get_visemes(text, True)\n\n    print(len(visemes), len(phonemes))\n    min_phone_vis = min(len(visemes),len(phonemes))\n    for i in range(min_phone_vis):\n        print(i, phonemes[i], visemes[i])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/","title":"polly_lipsync","text":"<p>Purpose</p> <p>Shows how to use the AWS SDK for Python (Boto3) with Amazon Polly and Tkinter to create a lip-sync application that displays an animated face speaking along with the speech synthesized by Amazon Polly. Lip-sync is accomplished by requesting a list of visemes from Amazon Polly that match up with the synthesized speech.</p>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth","title":"<code>PollyMouth</code>","text":"<p>A Tkinter application that lets a user enter text, select an Amazon Polly voice, and hear the text spoken by the selected voice while an animated face lip-syncs along with it.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>class PollyMouth:\n\"\"\"\n    A Tkinter application that lets a user enter text, select an Amazon Polly voice,\n    and hear the text spoken by the selected voice while an animated face lip-syncs\n    along with it.\n    \"\"\"\n\n    # A dictionary of visemes mapped to image file names.\n    lips = {\n        'p': {'name': '.media/lips_m.png'},\n        't': {'name': '.media/lips_c.png'},\n        'S': {'name': '.media/lips_ch.png'},\n        'T': {'name': '.media/lips_th.png'},\n        'f': {'name': '.media/lips_f.png'},\n        'k': {'name': '.media/lips_c.png'},\n        'i': {'name': '.media/lips_e.png'},\n        'r': {'name': '.media/lips_r.png'},\n        's': {'name': '.media/lips_c.png'},\n        'u': {'name': '.media/lips_w.png'},\n        '@': {'name': '.media/lips_u.png'},\n        'a': {'name': '.media/lips_a.png'},\n        'e': {'name': '.media/lips_a.png'},\n        'E': {'name': '.media/lips_u.png'},\n        'o': {'name': '.media/lips_o.png'},\n        'O': {'name': '.media/lips_u.png'},\n        'sil': {'name': '.media/lips_sil.png'}\n    }\n\n    def __init__(self, polly_wrapper):\n\"\"\"\n        Initializes the main Tkinter window and adds all of the widgets needed for\n        the application.\n\n        :param polly_wrapper: An object that can call Amazon Polly API functions.\n        \"\"\"\n        self.polly_wrapper = polly_wrapper\n        self.app = tkinter.Tk()\n        self.app.title(\"Amazon Polly Lip Sync\")\n        self.app.resizable(False, False)\n\n        self.load_lips()\n\n        choices_frame = tkinter.Frame(self.app)\n\n        self.sayit_label = tkinter.Label(\n            self.app, wraplength=410,\n            text=\"Write some text in the box below, then click 'Say it!' \"\n                 \"to hear and see your text.\")\n        self.sayit_txt = tkinter.Text(self.app, width=50, height=16)\n\n        self.engine_label = tkinter.Label(choices_frame, text='Engine:')\n        self.engine_var = tkinter.StringVar(choices_frame, 'neural')\n        self.engine_options = tkinter.OptionMenu(\n            choices_frame, self.engine_var, *sorted(polly_wrapper.get_voice_engines()),\n            command=self.change_engine)\n\n        self.language_label = tkinter.Label(choices_frame, text='Language:')\n        self.language_var = tkinter.StringVar(choices_frame, 'US English')\n        self.language_choices = polly_wrapper.get_languages(self.engine_var.get())\n        self.language_options = tkinter.OptionMenu(\n            choices_frame, self.language_var, *sorted(self.language_choices),\n            command=self.change_language)\n\n        self.voice_label = tkinter.Label(choices_frame, text='Voice:')\n        self.voice_var = tkinter.StringVar(choices_frame, 'Joanna')\n        self.voice_choices = polly_wrapper.get_voices(\n            self.engine_var.get(), self.language_choices[self.language_var.get()])\n        self.voice_options = tkinter.OptionMenu(\n            choices_frame, self.voice_var, *sorted(self.voice_choices))\n\n        self.face_canvas = tkinter.Canvas(\n            choices_frame, height=100, width=200, bg='white')\n        self.sayit_button = tkinter.Button(\n            self.app, text=\"Say it!\", command=self.say_it)\n\n        self.loading_text = tkinter.Label(self.app, bg='white')\n\n        self.app.geometry(\"635x320\")\n\n        self.sayit_label.grid(row=0)\n        self.sayit_txt.grid(row=1, column=0)\n        self.sayit_txt.focus_set()\n        self.sayit_button.grid(row=2, pady=10, columnspan=2)\n        self.sayit_button.configure(width=85, padx=10)\n\n        choices_frame.grid(row=1, column=1, sticky=tkinter.N)\n        self.engine_label.grid(row=0, column=0, sticky=tkinter.N, pady=10)\n        self.engine_options.grid(row=0, column=1, sticky=tkinter.NW, padx=5, pady=10)\n        self.engine_options.configure(width=18)\n        self.language_label.grid(row=1, column=0, sticky=tkinter.N, pady=10)\n        self.language_options.grid(row=1, column=1, sticky=tkinter.NW, padx=5, pady=10)\n        self.language_options.configure(width=18)\n        self.voice_label.grid(row=2, column=0, sticky=tkinter.N, pady=10)\n        self.voice_options.grid(row=2, column=1, sticky=tkinter.NW, padx=5, pady=10)\n        self.voice_options.configure(width=18)\n        self.face_canvas.grid(row=3, columnspan=2, padx=10)\n\n        self.face_canvas.create_image(100, 60, image=self.lips['sil']['image'])\n\n        self.app.mainloop()\n\n    def load_lips(self):\n\"\"\"\n        Loads lip-sync images either from a local '.media' folder or from GitHub\n        and saves image data in a dictionary of visemes.\n        \"\"\"\n        if os.path.isdir('.media'):\n            logger.info(\"Found .media folder. Loading images from the local folder.\")\n            for viseme in self.lips:\n                self.lips[viseme]['image'] = tkinter.PhotoImage(\n                    file=self.lips[viseme]['name'])\n        else:\n            logger.info(\"No local .media folder. Trying to load images from GitHub.\")\n            for viseme in self.lips:\n                url = GITHUB_URL + self.lips[viseme]['name']\n                resp = requests.get(url)\n                img = resp.content if resp.status_code == 200 else b''\n                if resp.status_code != 200:\n                    logger.warning(\"Couldn't load image from %s.\", url)\n                self.lips[viseme]['image'] = tkinter.PhotoImage(data=img)\n\n    def change_engine(self, engine):\n\"\"\"\n        Handles the event that is fired when the selected engine type is changed in\n        the UI. Updates the lists of available languages and voices that are supported\n        for the selected engine type.\n\n        :param engine: The newly selected engine type.\n        \"\"\"\n        self.language_choices = self.polly_wrapper.get_languages(engine)\n        lang_menu = self.language_options['menu']\n        lang_menu.delete(0, 'end')\n        sorted_choices = sorted(self.language_choices)\n        for lang in sorted_choices:\n            lang_menu.add_command(\n                label=lang, command=lambda l=lang: self.change_language(l))\n        self.change_language(sorted_choices[0])\n\n    def change_language(self, language):\n\"\"\"\n        Handles the event that is fired when the selected language is changed in the\n        UI. Updates the list of available voices that are available for the selected\n        language and engine type.\n\n        :param language: The newly selected language.\n        \"\"\"\n        self.language_var.set(language)\n        self.voice_choices = self.polly_wrapper.get_voices(\n            self.engine_var.get(), self.language_choices[language])\n        voice_menu = self.voice_options['menu']\n        voice_menu.delete(0, 'end')\n        sorted_choices = sorted(self.voice_choices)\n        for voice in sorted_choices:\n            voice_menu.add_command(\n                label=voice, command=lambda v=voice: self.voice_var.set(v))\n        self.voice_var.set(sorted_choices[0])\n\n    def animate_lips(self, start_time, viseme, viseme_iter):\n\"\"\"\n        Animates the face that lip-syncs along with the synthesized speech. This\n        uses the list of visemes and their associated timings that is returned from\n        Amazon Polly. The image associated with a viseme is displayed and the next\n        viseme is scheduled to display at the time indicated in the list of visemes.\n\n        :param start_time: The time the animation is started. This is used to\n                           calculate the time to wait until the next viseme image\n                           is displayed.\n        :param viseme: The current viseme to display.\n        :param viseme_iter: An iterator that yields visemes from the list returned\n                            from Amazon Polly.\n        \"\"\"\n        try:\n            mouth = self.lips.get(viseme['value'], self.lips['sil'])\n            self.face_canvas.create_image(\n                100, 60, image=mouth['image'])\n            self.app.update()\n            next_viseme = next(viseme_iter)\n            next_time = start_time + next_viseme['time']\n            cur_time = time.time_ns() // 1000000  # milliseconds\n            wait_time = max(0, next_time - cur_time)\n            logger.info(\"Vis: %s, cur_time %s, wait_time %s\", mouth,\n                        cur_time - start_time, wait_time)\n            self.app.after(\n                wait_time, self.animate_lips, start_time, next_viseme, viseme_iter)\n        except StopIteration:\n            pass\n\n    def long_text_wait_callback(self, task_type, task_status):\n\"\"\"\n        A callback function that displays status while waiting for an asynchronous\n        long text speech synthesis task to complete.\n\n        :param task_type: The type of synthesis task (either 'speech' or 'viseme').\n        :param task_status: The status of the task.\n        \"\"\"\n        self.loading_text.grid(row=0, rowspan=4, columnspan=2, sticky=tkinter.NSEW)\n        self.loading_text.configure(\n            text=f\"Waiting for {task_type}. Current status: {task_status}.\")\n        self.app.update()\n        if task_status in ('completed', 'failed'):\n            self.app.after(1000)\n            self.loading_text.grid_forget()\n\n    def say_it(self):\n\"\"\"\n        Gets synthesized speech and visemes from Amazon Polly, stores the audio in\n        a temporary file, and plays the sound and lip-sync animation.\n\n        When the text is too long for synchronous synthesis, this function displays a\n        dialog that asks the user for an Amazon Simple Storage Service (Amazon S3)\n        bucket to use for output storage, starts an asynchronous synthesis task, and\n        waits for the task to complete.\n        \"\"\"\n        audio_stream = None\n        visemes = []\n        try:\n            audio_stream, visemes = self.polly_wrapper.synthesize(\n                self.sayit_txt.get(1.0, tkinter.END),\n                self.engine_var.get(),\n                self.voice_choices[self.voice_var.get()],\n                'mp3',\n                self.language_choices[self.language_var.get()],\n                True)\n        except ClientError as error:\n            if error.response['Error']['Code'] == 'TextLengthExceededException':\n                bucket_name = tkinter.simpledialog.askstring(\n                    \"Text too long\",\n                    \"The text is too long for synchronous synthesis. To start an\\n\"\n                    \"asynchronous job, enter the name of an existing Amazon S3\\n\"\n                    \"bucket to use for speech synthesis output and click OK.\",\n                    parent=self.app)\n                if bucket_name:\n                    audio_stream, visemes = self.polly_wrapper.do_synthesis_task(\n                        self.sayit_txt.get(1.0, tkinter.END),\n                        self.engine_var.get(),\n                        self.voice_choices[self.voice_var.get()],\n                        'mp3',\n                        bucket_name,\n                        self.language_choices[self.language_var.get()],\n                        True,\n                        self.long_text_wait_callback)\n\n        logger.debug(\"Visemes: %s.\", json.dumps(visemes))\n\n        if audio_stream is not None:\n            with TemporaryDirectory() as tempdir:\n                speech_file_name = tempdir + '/speech.mp3'\n                with open(speech_file_name, 'wb') as speech_file:\n                    speech_file.write(audio_stream.read())\n                silence = '.media/silence.mp3'\n                if not os.path.isdir('.media'):\n                    silence = GITHUB_URL + silence\n                # Play a short silent audio file to ensure playsound is loaded and\n                # ready. Without this, the audio tends to lag behind viseme playback.\n                # playsound(silence)\n                playsound(speech_file_name, block=False)\n                start_time = time.time_ns() // 1000000\n                self.app.after(\n                    0, self.animate_lips, start_time, {'value': 'sil'}, iter(visemes))\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.__init__","title":"<code>__init__(polly_wrapper)</code>","text":"<p>Initializes the main Tkinter window and adds all of the widgets needed for the application.</p> <p>:param polly_wrapper: An object that can call Amazon Polly API functions.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def __init__(self, polly_wrapper):\n\"\"\"\n    Initializes the main Tkinter window and adds all of the widgets needed for\n    the application.\n\n    :param polly_wrapper: An object that can call Amazon Polly API functions.\n    \"\"\"\n    self.polly_wrapper = polly_wrapper\n    self.app = tkinter.Tk()\n    self.app.title(\"Amazon Polly Lip Sync\")\n    self.app.resizable(False, False)\n\n    self.load_lips()\n\n    choices_frame = tkinter.Frame(self.app)\n\n    self.sayit_label = tkinter.Label(\n        self.app, wraplength=410,\n        text=\"Write some text in the box below, then click 'Say it!' \"\n             \"to hear and see your text.\")\n    self.sayit_txt = tkinter.Text(self.app, width=50, height=16)\n\n    self.engine_label = tkinter.Label(choices_frame, text='Engine:')\n    self.engine_var = tkinter.StringVar(choices_frame, 'neural')\n    self.engine_options = tkinter.OptionMenu(\n        choices_frame, self.engine_var, *sorted(polly_wrapper.get_voice_engines()),\n        command=self.change_engine)\n\n    self.language_label = tkinter.Label(choices_frame, text='Language:')\n    self.language_var = tkinter.StringVar(choices_frame, 'US English')\n    self.language_choices = polly_wrapper.get_languages(self.engine_var.get())\n    self.language_options = tkinter.OptionMenu(\n        choices_frame, self.language_var, *sorted(self.language_choices),\n        command=self.change_language)\n\n    self.voice_label = tkinter.Label(choices_frame, text='Voice:')\n    self.voice_var = tkinter.StringVar(choices_frame, 'Joanna')\n    self.voice_choices = polly_wrapper.get_voices(\n        self.engine_var.get(), self.language_choices[self.language_var.get()])\n    self.voice_options = tkinter.OptionMenu(\n        choices_frame, self.voice_var, *sorted(self.voice_choices))\n\n    self.face_canvas = tkinter.Canvas(\n        choices_frame, height=100, width=200, bg='white')\n    self.sayit_button = tkinter.Button(\n        self.app, text=\"Say it!\", command=self.say_it)\n\n    self.loading_text = tkinter.Label(self.app, bg='white')\n\n    self.app.geometry(\"635x320\")\n\n    self.sayit_label.grid(row=0)\n    self.sayit_txt.grid(row=1, column=0)\n    self.sayit_txt.focus_set()\n    self.sayit_button.grid(row=2, pady=10, columnspan=2)\n    self.sayit_button.configure(width=85, padx=10)\n\n    choices_frame.grid(row=1, column=1, sticky=tkinter.N)\n    self.engine_label.grid(row=0, column=0, sticky=tkinter.N, pady=10)\n    self.engine_options.grid(row=0, column=1, sticky=tkinter.NW, padx=5, pady=10)\n    self.engine_options.configure(width=18)\n    self.language_label.grid(row=1, column=0, sticky=tkinter.N, pady=10)\n    self.language_options.grid(row=1, column=1, sticky=tkinter.NW, padx=5, pady=10)\n    self.language_options.configure(width=18)\n    self.voice_label.grid(row=2, column=0, sticky=tkinter.N, pady=10)\n    self.voice_options.grid(row=2, column=1, sticky=tkinter.NW, padx=5, pady=10)\n    self.voice_options.configure(width=18)\n    self.face_canvas.grid(row=3, columnspan=2, padx=10)\n\n    self.face_canvas.create_image(100, 60, image=self.lips['sil']['image'])\n\n    self.app.mainloop()\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.animate_lips","title":"<code>animate_lips(start_time, viseme, viseme_iter)</code>","text":"<p>Animates the face that lip-syncs along with the synthesized speech. This uses the list of visemes and their associated timings that is returned from Amazon Polly. The image associated with a viseme is displayed and the next viseme is scheduled to display at the time indicated in the list of visemes.</p> <p>:param start_time: The time the animation is started. This is used to                    calculate the time to wait until the next viseme image                    is displayed. :param viseme: The current viseme to display. :param viseme_iter: An iterator that yields visemes from the list returned                     from Amazon Polly.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def animate_lips(self, start_time, viseme, viseme_iter):\n\"\"\"\n    Animates the face that lip-syncs along with the synthesized speech. This\n    uses the list of visemes and their associated timings that is returned from\n    Amazon Polly. The image associated with a viseme is displayed and the next\n    viseme is scheduled to display at the time indicated in the list of visemes.\n\n    :param start_time: The time the animation is started. This is used to\n                       calculate the time to wait until the next viseme image\n                       is displayed.\n    :param viseme: The current viseme to display.\n    :param viseme_iter: An iterator that yields visemes from the list returned\n                        from Amazon Polly.\n    \"\"\"\n    try:\n        mouth = self.lips.get(viseme['value'], self.lips['sil'])\n        self.face_canvas.create_image(\n            100, 60, image=mouth['image'])\n        self.app.update()\n        next_viseme = next(viseme_iter)\n        next_time = start_time + next_viseme['time']\n        cur_time = time.time_ns() // 1000000  # milliseconds\n        wait_time = max(0, next_time - cur_time)\n        logger.info(\"Vis: %s, cur_time %s, wait_time %s\", mouth,\n                    cur_time - start_time, wait_time)\n        self.app.after(\n            wait_time, self.animate_lips, start_time, next_viseme, viseme_iter)\n    except StopIteration:\n        pass\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.change_engine","title":"<code>change_engine(engine)</code>","text":"<p>Handles the event that is fired when the selected engine type is changed in the UI. Updates the lists of available languages and voices that are supported for the selected engine type.</p> <p>:param engine: The newly selected engine type.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def change_engine(self, engine):\n\"\"\"\n    Handles the event that is fired when the selected engine type is changed in\n    the UI. Updates the lists of available languages and voices that are supported\n    for the selected engine type.\n\n    :param engine: The newly selected engine type.\n    \"\"\"\n    self.language_choices = self.polly_wrapper.get_languages(engine)\n    lang_menu = self.language_options['menu']\n    lang_menu.delete(0, 'end')\n    sorted_choices = sorted(self.language_choices)\n    for lang in sorted_choices:\n        lang_menu.add_command(\n            label=lang, command=lambda l=lang: self.change_language(l))\n    self.change_language(sorted_choices[0])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.change_language","title":"<code>change_language(language)</code>","text":"<p>Handles the event that is fired when the selected language is changed in the UI. Updates the list of available voices that are available for the selected language and engine type.</p> <p>:param language: The newly selected language.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def change_language(self, language):\n\"\"\"\n    Handles the event that is fired when the selected language is changed in the\n    UI. Updates the list of available voices that are available for the selected\n    language and engine type.\n\n    :param language: The newly selected language.\n    \"\"\"\n    self.language_var.set(language)\n    self.voice_choices = self.polly_wrapper.get_voices(\n        self.engine_var.get(), self.language_choices[language])\n    voice_menu = self.voice_options['menu']\n    voice_menu.delete(0, 'end')\n    sorted_choices = sorted(self.voice_choices)\n    for voice in sorted_choices:\n        voice_menu.add_command(\n            label=voice, command=lambda v=voice: self.voice_var.set(v))\n    self.voice_var.set(sorted_choices[0])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.load_lips","title":"<code>load_lips()</code>","text":"<p>Loads lip-sync images either from a local '.media' folder or from GitHub and saves image data in a dictionary of visemes.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def load_lips(self):\n\"\"\"\n    Loads lip-sync images either from a local '.media' folder or from GitHub\n    and saves image data in a dictionary of visemes.\n    \"\"\"\n    if os.path.isdir('.media'):\n        logger.info(\"Found .media folder. Loading images from the local folder.\")\n        for viseme in self.lips:\n            self.lips[viseme]['image'] = tkinter.PhotoImage(\n                file=self.lips[viseme]['name'])\n    else:\n        logger.info(\"No local .media folder. Trying to load images from GitHub.\")\n        for viseme in self.lips:\n            url = GITHUB_URL + self.lips[viseme]['name']\n            resp = requests.get(url)\n            img = resp.content if resp.status_code == 200 else b''\n            if resp.status_code != 200:\n                logger.warning(\"Couldn't load image from %s.\", url)\n            self.lips[viseme]['image'] = tkinter.PhotoImage(data=img)\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.long_text_wait_callback","title":"<code>long_text_wait_callback(task_type, task_status)</code>","text":"<p>A callback function that displays status while waiting for an asynchronous long text speech synthesis task to complete.</p> <p>:param task_type: The type of synthesis task (either 'speech' or 'viseme'). :param task_status: The status of the task.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def long_text_wait_callback(self, task_type, task_status):\n\"\"\"\n    A callback function that displays status while waiting for an asynchronous\n    long text speech synthesis task to complete.\n\n    :param task_type: The type of synthesis task (either 'speech' or 'viseme').\n    :param task_status: The status of the task.\n    \"\"\"\n    self.loading_text.grid(row=0, rowspan=4, columnspan=2, sticky=tkinter.NSEW)\n    self.loading_text.configure(\n        text=f\"Waiting for {task_type}. Current status: {task_status}.\")\n    self.app.update()\n    if task_status in ('completed', 'failed'):\n        self.app.after(1000)\n        self.loading_text.grid_forget()\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.say_it","title":"<code>say_it()</code>","text":"<p>Gets synthesized speech and visemes from Amazon Polly, stores the audio in a temporary file, and plays the sound and lip-sync animation.</p> <p>When the text is too long for synchronous synthesis, this function displays a dialog that asks the user for an Amazon Simple Storage Service (Amazon S3) bucket to use for output storage, starts an asynchronous synthesis task, and waits for the task to complete.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def say_it(self):\n\"\"\"\n    Gets synthesized speech and visemes from Amazon Polly, stores the audio in\n    a temporary file, and plays the sound and lip-sync animation.\n\n    When the text is too long for synchronous synthesis, this function displays a\n    dialog that asks the user for an Amazon Simple Storage Service (Amazon S3)\n    bucket to use for output storage, starts an asynchronous synthesis task, and\n    waits for the task to complete.\n    \"\"\"\n    audio_stream = None\n    visemes = []\n    try:\n        audio_stream, visemes = self.polly_wrapper.synthesize(\n            self.sayit_txt.get(1.0, tkinter.END),\n            self.engine_var.get(),\n            self.voice_choices[self.voice_var.get()],\n            'mp3',\n            self.language_choices[self.language_var.get()],\n            True)\n    except ClientError as error:\n        if error.response['Error']['Code'] == 'TextLengthExceededException':\n            bucket_name = tkinter.simpledialog.askstring(\n                \"Text too long\",\n                \"The text is too long for synchronous synthesis. To start an\\n\"\n                \"asynchronous job, enter the name of an existing Amazon S3\\n\"\n                \"bucket to use for speech synthesis output and click OK.\",\n                parent=self.app)\n            if bucket_name:\n                audio_stream, visemes = self.polly_wrapper.do_synthesis_task(\n                    self.sayit_txt.get(1.0, tkinter.END),\n                    self.engine_var.get(),\n                    self.voice_choices[self.voice_var.get()],\n                    'mp3',\n                    bucket_name,\n                    self.language_choices[self.language_var.get()],\n                    True,\n                    self.long_text_wait_callback)\n\n    logger.debug(\"Visemes: %s.\", json.dumps(visemes))\n\n    if audio_stream is not None:\n        with TemporaryDirectory() as tempdir:\n            speech_file_name = tempdir + '/speech.mp3'\n            with open(speech_file_name, 'wb') as speech_file:\n                speech_file.write(audio_stream.read())\n            silence = '.media/silence.mp3'\n            if not os.path.isdir('.media'):\n                silence = GITHUB_URL + silence\n            # Play a short silent audio file to ensure playsound is loaded and\n            # ready. Without this, the audio tends to lag behind viseme playback.\n            # playsound(silence)\n            playsound(speech_file_name, block=False)\n            start_time = time.time_ns() // 1000000\n            self.app.after(\n                0, self.animate_lips, start_time, {'value': 'sil'}, iter(visemes))\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/","title":"polly_wrapper","text":"<p>Purpose</p> <p>Shows how to use the AWS SDK for Python (Boto3) with Amazon Polly to synthesize speech and manage custom lexicons.</p>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper","title":"<code>PollyWrapper</code>","text":"<p>Encapsulates Amazon Polly functions.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>class PollyWrapper:\n\"\"\"Encapsulates Amazon Polly functions.\"\"\"\n    def __init__(self, polly_client, s3_resource):\n\"\"\"\n        :param polly_client: A Boto3 Amazon Polly client.\n        :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.\n        \"\"\"\n        self.polly_client = polly_client\n        self.s3_resource = s3_resource\n        self.voice_metadata = None\n# snippet-end:[python.example_code.polly.helper.PollyWrapper]\n\n# snippet-start:[python.example_code.polly.DescribeVoices]\n    def describe_voices(self):\n\"\"\"\n        Gets metadata about available voices.\n\n        :return: The list of voice metadata.\n        \"\"\"\n        try:\n            response = self.polly_client.describe_voices()\n            self.voice_metadata = response['Voices']\n            logger.info(\"Got metadata about %s voices.\", len(self.voice_metadata))\n        except ClientError:\n            logger.exception(\"Couldn't get voice metadata.\")\n            raise\n        else:\n            return self.voice_metadata\n# snippet-end:[python.example_code.polly.DescribeVoices]\n\n# snippet-start:[python.example_code.polly.Synthesize]\n    def synthesize(\n            self, text, engine, voice, audio_format, lang_code=None,\n            include_visemes=False):\n\"\"\"\n        Synthesizes speech or speech marks from text, using the specified voice.\n\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\n        try:\n            kwargs = {\n                'Engine': engine,\n                'OutputFormat': audio_format,\n                'Text': text,\n                'VoiceId': voice}\n            if lang_code is not None:\n                kwargs['LanguageCode'] = lang_code\n            response = self.polly_client.synthesize_speech(**kwargs)\n            audio_stream = response['AudioStream']\n            logger.info(\"Got audio stream spoken by %s.\", voice)\n            visemes = None\n            if include_visemes:\n                kwargs['OutputFormat'] = 'json'\n                kwargs['SpeechMarkTypes'] = ['viseme']\n                response = self.polly_client.synthesize_speech(**kwargs)\n                visemes = [json.loads(v) for v in\n                           response['AudioStream'].read().decode().split() if v]\n                logger.info(\"Got %s visemes.\", len(visemes))\n        except ClientError:\n            logger.exception(\"Couldn't get audio stream.\")\n            raise\n        else:\n            return audio_stream, visemes\n# snippet-end:[python.example_code.polly.Synthesize]\n\n    def _wait_for_task(self, tries, task_id, task_type, wait_callback, output_bucket):\n\"\"\"\n        Waits for an asynchronous speech synthesis task to complete. This function\n        polls Amazon Polly for data about the specified task until a completion\n        status is returned or the number of tries is exceeded.\n\n        When the task successfully completes, the task output is retrieved from the\n        output Amazon S3 bucket and the output object is deleted.\n\n        :param tries: The number of times to poll for status.\n        :param task_id: The ID of the task to wait for.\n        :param task_type: The type of task. This is passed to the `wait_callback`\n                          function to display status.\n        :param wait_callback: A callback function that is called after each poll,\n                              to give the caller an opportunity to take action, such\n                              as to display status.\n        :param output_bucket: The Amazon S3 bucket where task output is located.\n        :return: The output from the task in a byte stream.\n        \"\"\"\n        task = None\n        while tries &gt; 0:\n            task = self.get_speech_synthesis_task(task_id)\n            task_status = task['TaskStatus']\n            logger.info(\"Task %s status %s.\", task_id, task_status)\n            if wait_callback is not None:\n                wait_callback(task_type, task_status)\n            if task_status in ('completed', 'failed'):\n                break\n            time.sleep(5)\n            tries -= 1\n\n        output_stream = io.BytesIO()\n        if task is not None:\n            output_key = task['OutputUri'].split('/')[-1]\n            output_bucket.download_fileobj(output_key, output_stream)\n            output_bucket.Object(output_key).delete()\n            logger.info(\"Downloaded output for task %s.\", task_id)\n            output_stream.seek(0)\n\n        return output_stream\n\n# snippet-start:[python.example_code.polly.StartSpeechSynthesisTask]\n    def do_synthesis_task(\n            self, text, engine, voice, audio_format, s3_bucket, lang_code=None,\n            include_visemes=False, wait_callback=None):\n\"\"\"\n        Start an asynchronous task to synthesize speech or speech marks, wait for\n        the task to complete, retrieve the output from Amazon S3, and return the\n        data.\n\n        An asynchronous task is required when the text is too long for near-real time\n        synthesis.\n\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param s3_bucket: The name of an existing Amazon S3 bucket that you have\n                          write access to. Synthesis output is written to this bucket.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :param wait_callback: A callback function that is called periodically during\n                              task processing, to give the caller an opportunity to\n                              take action, such as to display status.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\n        try:\n            kwargs = {\n                'Engine': engine,\n                'OutputFormat': audio_format,\n                'OutputS3BucketName': s3_bucket,\n                'Text': text,\n                'VoiceId': voice}\n            if lang_code is not None:\n                kwargs['LanguageCode'] = lang_code\n            response = self.polly_client.start_speech_synthesis_task(**kwargs)\n            speech_task = response['SynthesisTask']\n            logger.info(\"Started speech synthesis task %s.\", speech_task['TaskId'])\n\n            viseme_task = None\n            if include_visemes:\n                kwargs['OutputFormat'] = 'json'\n                kwargs['SpeechMarkTypes'] = ['viseme']\n                response = self.polly_client.start_speech_synthesis_task(**kwargs)\n                viseme_task = response['SynthesisTask']\n                logger.info(\"Started viseme synthesis task %s.\", viseme_task['TaskId'])\n        except ClientError:\n            logger.exception(\"Couldn't start synthesis task.\")\n            raise\n        else:\n            bucket = self.s3_resource.Bucket(s3_bucket)\n            audio_stream = self._wait_for_task(\n                10, speech_task['TaskId'], 'speech', wait_callback, bucket)\n\n            visemes = None\n            if include_visemes:\n                viseme_data = self._wait_for_task(\n                    10, viseme_task['TaskId'], 'viseme', wait_callback, bucket)\n                visemes = [json.loads(v) for v in\n                           viseme_data.read().decode().split() if v]\n\n            return audio_stream, visemes\n# snippet-end:[python.example_code.polly.StartSpeechSynthesisTask]\n\n# snippet-start:[python.example_code.polly.GetSpeechSynthesisTask]\n    def get_speech_synthesis_task(self, task_id):\n\"\"\"\n        Gets metadata about an asynchronous speech synthesis task, such as its status.\n\n        :param task_id: The ID of the task to retrieve.\n        :return: Metadata about the task.\n        \"\"\"\n        try:\n            response = self.polly_client.get_speech_synthesis_task(TaskId=task_id)\n            task = response['SynthesisTask']\n            logger.info(\"Got synthesis task. Status is %s.\", task['TaskStatus'])\n        except ClientError:\n            logger.exception(\"Couldn't get synthesis task %s.\", task_id)\n            raise\n        else:\n            return task\n# snippet-end:[python.example_code.polly.GetSpeechSynthesisTask]\n\n# snippet-start:[python.example_code.polly.PutLexicon]\n    def create_lexicon(self, name, content):\n\"\"\"\n        Creates a lexicon with the specified content. A lexicon contains custom\n        pronunciations.\n\n        :param name: The name of the lexicon.\n        :param content: The content of the lexicon.\n        \"\"\"\n        try:\n            self.polly_client.put_lexicon(Name=name, Content=content)\n            logger.info(\"Created lexicon %s.\", name)\n        except ClientError:\n            logger.exception(\"Couldn't create lexicon %s.\")\n            raise\n# snippet-end:[python.example_code.polly.PutLexicon]\n\n# snippet-start:[python.example_code.polly.GetLexicon]\n    def get_lexicon(self, name):\n\"\"\"\n        Gets metadata and contents of an existing lexicon.\n\n        :param name: The name of the lexicon to retrieve.\n        :return: The retrieved lexicon.\n        \"\"\"\n        try:\n            response = self.polly_client.get_lexicon(Name=name)\n            logger.info(\"Got lexicon %s.\", name)\n        except ClientError:\n            logger.exception(\"Couldn't get lexicon %s.\", name)\n            raise\n        else:\n            return response\n# snippet-end:[python.example_code.polly.GetLexicon]\n\n# snippet-start:[python.example_code.polly.ListLexicons]\n    def list_lexicons(self):\n\"\"\"\n        Lists lexicons in the current account.\n\n        :return: The list of lexicons.\n        \"\"\"\n        try:\n            response = self.polly_client.list_lexicons()\n            lexicons = response['Lexicons']\n            logger.info(\"Got %s lexicons.\", len(lexicons))\n        except ClientError:\n            logger.exception(\"Couldn't get  %s.\", )\n            raise\n        else:\n            return lexicons\n# snippet-end:[python.example_code.polly.ListLexicons]\n\n    def get_voice_engines(self):\n\"\"\"\n        Extracts the set of available voice engine types from the full list of\n        voice metadata.\n\n        :return: The set of voice engine types.\n        \"\"\"\n        if self.voice_metadata is None:\n            self.describe_voices()\n\n        engines = set()\n        for voice in self.voice_metadata:\n            for engine in voice['SupportedEngines']:\n                engines.add(engine)\n        return engines\n\n    def get_languages(self, engine):\n\"\"\"\n        Extracts the set of available languages for the specified engine from the\n        full list of voice metadata.\n\n        :param engine: The engine type to filter on.\n        :return: The set of languages available for the specified engine type.\n        \"\"\"\n        if self.voice_metadata is None:\n            self.describe_voices()\n\n        return {vo['LanguageName']: vo['LanguageCode'] for vo\n                in self.voice_metadata\n                if engine in vo['SupportedEngines']}\n\n    def get_voices(self, engine, language_code):\n\"\"\"\n        Extracts the set of voices that are available for the specified engine type\n        and language from the full list of voice metadata.\n\n        :param engine: The engine type to filter on.\n        :param language_code: The language to filter on.\n        :return: The set of voices available for the specified engine type and language.\n        \"\"\"\n        if self.voice_metadata is None:\n            self.describe_voices()\n\n        return {vo['Name']: vo['Id'] for vo in self.voice_metadata\n                if engine in vo['SupportedEngines']\n                and language_code == vo['LanguageCode']}\n\n\"\"\"\n        Synthesizes speech or speech marks from text, using the specified voice.\n\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.__init__","title":"<code>__init__(polly_client, s3_resource)</code>","text":"<p>:param polly_client: A Boto3 Amazon Polly client. :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def __init__(self, polly_client, s3_resource):\n\"\"\"\n    :param polly_client: A Boto3 Amazon Polly client.\n    :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.\n    \"\"\"\n    self.polly_client = polly_client\n    self.s3_resource = s3_resource\n    self.voice_metadata = None\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.create_lexicon","title":"<code>create_lexicon(name, content)</code>","text":"<p>Creates a lexicon with the specified content. A lexicon contains custom pronunciations.</p> <p>:param name: The name of the lexicon. :param content: The content of the lexicon.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def create_lexicon(self, name, content):\n\"\"\"\n    Creates a lexicon with the specified content. A lexicon contains custom\n    pronunciations.\n\n    :param name: The name of the lexicon.\n    :param content: The content of the lexicon.\n    \"\"\"\n    try:\n        self.polly_client.put_lexicon(Name=name, Content=content)\n        logger.info(\"Created lexicon %s.\", name)\n    except ClientError:\n        logger.exception(\"Couldn't create lexicon %s.\")\n        raise\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.describe_voices","title":"<code>describe_voices()</code>","text":"<p>Gets metadata about available voices.</p> <p>:return: The list of voice metadata.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def describe_voices(self):\n\"\"\"\n    Gets metadata about available voices.\n\n    :return: The list of voice metadata.\n    \"\"\"\n    try:\n        response = self.polly_client.describe_voices()\n        self.voice_metadata = response['Voices']\n        logger.info(\"Got metadata about %s voices.\", len(self.voice_metadata))\n    except ClientError:\n        logger.exception(\"Couldn't get voice metadata.\")\n        raise\n    else:\n        return self.voice_metadata\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.do_synthesis_task","title":"<code>do_synthesis_task(text, engine, voice, audio_format, s3_bucket, lang_code=None, include_visemes=False, wait_callback=None)</code>","text":"<p>Start an asynchronous task to synthesize speech or speech marks, wait for the task to complete, retrieve the output from Amazon S3, and return the data.</p> <p>An asynchronous task is required when the text is too long for near-real time synthesis.</p> <p>:param text: The text to synthesize. :param engine: The kind of engine used. Can be standard or neural. :param voice: The ID of the voice to use. :param audio_format: The audio format to return for synthesized speech. When                      speech marks are synthesized, the output format is JSON. :param s3_bucket: The name of an existing Amazon S3 bucket that you have                   write access to. Synthesis output is written to this bucket. :param lang_code: The language code of the voice to use. This has an effect                   only when a bilingual voice is selected. :param include_visemes: When True, a second request is made to Amazon Polly                         to synthesize a list of visemes, using the specified                         text and voice. A viseme represents the visual position                         of the face and mouth when saying part of a word. :param wait_callback: A callback function that is called periodically during                       task processing, to give the caller an opportunity to                       take action, such as to display status. :return: The audio stream that contains the synthesized speech and a list          of visemes that are associated with the speech audio.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def do_synthesis_task(\n        self, text, engine, voice, audio_format, s3_bucket, lang_code=None,\n        include_visemes=False, wait_callback=None):\n\"\"\"\n    Start an asynchronous task to synthesize speech or speech marks, wait for\n    the task to complete, retrieve the output from Amazon S3, and return the\n    data.\n\n    An asynchronous task is required when the text is too long for near-real time\n    synthesis.\n\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param s3_bucket: The name of an existing Amazon S3 bucket that you have\n                      write access to. Synthesis output is written to this bucket.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :param wait_callback: A callback function that is called periodically during\n                          task processing, to give the caller an opportunity to\n                          take action, such as to display status.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\n    try:\n        kwargs = {\n            'Engine': engine,\n            'OutputFormat': audio_format,\n            'OutputS3BucketName': s3_bucket,\n            'Text': text,\n            'VoiceId': voice}\n        if lang_code is not None:\n            kwargs['LanguageCode'] = lang_code\n        response = self.polly_client.start_speech_synthesis_task(**kwargs)\n        speech_task = response['SynthesisTask']\n        logger.info(\"Started speech synthesis task %s.\", speech_task['TaskId'])\n\n        viseme_task = None\n        if include_visemes:\n            kwargs['OutputFormat'] = 'json'\n            kwargs['SpeechMarkTypes'] = ['viseme']\n            response = self.polly_client.start_speech_synthesis_task(**kwargs)\n            viseme_task = response['SynthesisTask']\n            logger.info(\"Started viseme synthesis task %s.\", viseme_task['TaskId'])\n    except ClientError:\n        logger.exception(\"Couldn't start synthesis task.\")\n        raise\n    else:\n        bucket = self.s3_resource.Bucket(s3_bucket)\n        audio_stream = self._wait_for_task(\n            10, speech_task['TaskId'], 'speech', wait_callback, bucket)\n\n        visemes = None\n        if include_visemes:\n            viseme_data = self._wait_for_task(\n                10, viseme_task['TaskId'], 'viseme', wait_callback, bucket)\n            visemes = [json.loads(v) for v in\n                       viseme_data.read().decode().split() if v]\n\n        return audio_stream, visemes\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_languages","title":"<code>get_languages(engine)</code>","text":"<p>Extracts the set of available languages for the specified engine from the full list of voice metadata.</p> <p>:param engine: The engine type to filter on. :return: The set of languages available for the specified engine type.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_languages(self, engine):\n\"\"\"\n    Extracts the set of available languages for the specified engine from the\n    full list of voice metadata.\n\n    :param engine: The engine type to filter on.\n    :return: The set of languages available for the specified engine type.\n    \"\"\"\n    if self.voice_metadata is None:\n        self.describe_voices()\n\n    return {vo['LanguageName']: vo['LanguageCode'] for vo\n            in self.voice_metadata\n            if engine in vo['SupportedEngines']}\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_lexicon","title":"<code>get_lexicon(name)</code>","text":"<p>Gets metadata and contents of an existing lexicon.</p> <p>:param name: The name of the lexicon to retrieve. :return: The retrieved lexicon.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_lexicon(self, name):\n\"\"\"\n    Gets metadata and contents of an existing lexicon.\n\n    :param name: The name of the lexicon to retrieve.\n    :return: The retrieved lexicon.\n    \"\"\"\n    try:\n        response = self.polly_client.get_lexicon(Name=name)\n        logger.info(\"Got lexicon %s.\", name)\n    except ClientError:\n        logger.exception(\"Couldn't get lexicon %s.\", name)\n        raise\n    else:\n        return response\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_speech_synthesis_task","title":"<code>get_speech_synthesis_task(task_id)</code>","text":"<p>Gets metadata about an asynchronous speech synthesis task, such as its status.</p> <p>:param task_id: The ID of the task to retrieve. :return: Metadata about the task.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_speech_synthesis_task(self, task_id):\n\"\"\"\n    Gets metadata about an asynchronous speech synthesis task, such as its status.\n\n    :param task_id: The ID of the task to retrieve.\n    :return: Metadata about the task.\n    \"\"\"\n    try:\n        response = self.polly_client.get_speech_synthesis_task(TaskId=task_id)\n        task = response['SynthesisTask']\n        logger.info(\"Got synthesis task. Status is %s.\", task['TaskStatus'])\n    except ClientError:\n        logger.exception(\"Couldn't get synthesis task %s.\", task_id)\n        raise\n    else:\n        return task\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_voice_engines","title":"<code>get_voice_engines()</code>","text":"<p>Extracts the set of available voice engine types from the full list of voice metadata.</p> <p>:return: The set of voice engine types.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_voice_engines(self):\n\"\"\"\n    Extracts the set of available voice engine types from the full list of\n    voice metadata.\n\n    :return: The set of voice engine types.\n    \"\"\"\n    if self.voice_metadata is None:\n        self.describe_voices()\n\n    engines = set()\n    for voice in self.voice_metadata:\n        for engine in voice['SupportedEngines']:\n            engines.add(engine)\n    return engines\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_voices","title":"<code>get_voices(engine, language_code)</code>","text":"<p>Extracts the set of voices that are available for the specified engine type and language from the full list of voice metadata.</p> <p>:param engine: The engine type to filter on. :param language_code: The language to filter on. :return: The set of voices available for the specified engine type and language.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_voices(self, engine, language_code):\n\"\"\"\n    Extracts the set of voices that are available for the specified engine type\n    and language from the full list of voice metadata.\n\n    :param engine: The engine type to filter on.\n    :param language_code: The language to filter on.\n    :return: The set of voices available for the specified engine type and language.\n    \"\"\"\n    if self.voice_metadata is None:\n        self.describe_voices()\n\n    return {vo['Name']: vo['Id'] for vo in self.voice_metadata\n            if engine in vo['SupportedEngines']\n            and language_code == vo['LanguageCode']}\n\n\"\"\"\n    Synthesizes speech or speech marks from text, using the specified voice.\n\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.list_lexicons","title":"<code>list_lexicons()</code>","text":"<p>Lists lexicons in the current account.</p> <p>:return: The list of lexicons.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def list_lexicons(self):\n\"\"\"\n    Lists lexicons in the current account.\n\n    :return: The list of lexicons.\n    \"\"\"\n    try:\n        response = self.polly_client.list_lexicons()\n        lexicons = response['Lexicons']\n        logger.info(\"Got %s lexicons.\", len(lexicons))\n    except ClientError:\n        logger.exception(\"Couldn't get  %s.\", )\n        raise\n    else:\n        return lexicons\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.synthesize","title":"<code>synthesize(text, engine, voice, audio_format, lang_code=None, include_visemes=False)</code>","text":"<p>Synthesizes speech or speech marks from text, using the specified voice.</p> <p>:param text: The text to synthesize. :param engine: The kind of engine used. Can be standard or neural. :param voice: The ID of the voice to use. :param audio_format: The audio format to return for synthesized speech. When                      speech marks are synthesized, the output format is JSON. :param lang_code: The language code of the voice to use. This has an effect                   only when a bilingual voice is selected. :param include_visemes: When True, a second request is made to Amazon Polly                         to synthesize a list of visemes, using the specified                         text and voice. A viseme represents the visual position                         of the face and mouth when saying part of a word. :return: The audio stream that contains the synthesized speech and a list          of visemes that are associated with the speech audio.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def synthesize(\n        self, text, engine, voice, audio_format, lang_code=None,\n        include_visemes=False):\n\"\"\"\n    Synthesizes speech or speech marks from text, using the specified voice.\n\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\n    try:\n        kwargs = {\n            'Engine': engine,\n            'OutputFormat': audio_format,\n            'Text': text,\n            'VoiceId': voice}\n        if lang_code is not None:\n            kwargs['LanguageCode'] = lang_code\n        response = self.polly_client.synthesize_speech(**kwargs)\n        audio_stream = response['AudioStream']\n        logger.info(\"Got audio stream spoken by %s.\", voice)\n        visemes = None\n        if include_visemes:\n            kwargs['OutputFormat'] = 'json'\n            kwargs['SpeechMarkTypes'] = ['viseme']\n            response = self.polly_client.synthesize_speech(**kwargs)\n            visemes = [json.loads(v) for v in\n                       response['AudioStream'].read().decode().split() if v]\n            logger.info(\"Got %s visemes.\", len(visemes))\n    except ClientError:\n        logger.exception(\"Couldn't get audio stream.\")\n        raise\n    else:\n        return audio_stream, visemes\n</code></pre>"},{"location":"reference_copy/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Backend2</li> <li>FASTAPI</li> </ul>"},{"location":"reference_copy/api/","title":"API Docs","text":"<p>Exposes interactive bot functionality through FastAPI</p>"},{"location":"reference_copy/api/#backend.app.api.generate_response","title":"<code>generate_response(text, speaker, reset_conversation, director_condition)</code>","text":"<p>Generates a bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/bot_response\")\ndef generate_response(text: str, speaker: str, reset_conversation: bool, director_condition: bool):\n\"\"\"Generates a bot response\"\"\"\n    l.log(f\"/api/bot_response: '{text}', from {speaker}, reset_conversation: {reset_conversation}, director_condition: {director_condition}\")\n    global TEXT_QUEUE\n\n    classifications = bot.get_classifications(text)\n    TEXT_QUEUE[\"classifications\"].append(classifications)\n    if bot.classification_processor.emotion in [\"joy\", \"sad\", \"surprise\"]:\n        l.log(f\"Setting face to: {bot.classification_processor.emotion}\")\n        FACE_CONTROL_QUEUE[\"expression\"].append(bot.classification_processor.emotion)\n    else:\n        l.log(\"Setting face to: neutral\")\n        FACE_CONTROL_QUEUE[\"expression\"].append(\"neutral\")\n\n    facilitator_response = bot.get_facilitator_response(director_condition)\n    TEXT_QUEUE[\"facilitator_response\"].append(facilitator_response)\n\n    bot_response= bot.get_bot_response(text, speaker, reset_conversation)\n    TEXT_QUEUE[\"bot_response\"].append(bot_response)\n    bot.chatbot.reject_response()\n\n    return PlainTextResponse(bot_response)\n</code></pre>"},{"location":"reference_copy/api/#backend.app.api.return_response","title":"<code>return_response(mode, query)</code>","text":"<p>Returns an existing bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/facilitator_presets\")\ndef return_response(mode: str, query: str):\n\"\"\"Returns an existing bot response\"\"\"\n    l.log(f\"/api/facilitator_presets: {mode}, {query}\")\n    if mode == \"facilitator\": to_say = presets.responses[query]\n    if mode == \"director\":\n        if query == \"disclosure\":\n            to_say = random.choice(df.disclosure_elicitation)\n        if query == \"response\":\n            to_say = random.choice(df.response_elicitation)\n    if mode == \"role_model\":\n        if query == \"disclosure\":\n            emotion = random.choice(rmf.disclosures.keys())\n            transition =random.choice(rmf.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(rmf.disclosures[emotion])\n            re_transition = random.choice(rmf.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n            to_say = \" \".join(responses)\n        if query == \"response\":\n            print(\"getting response\")\n            to_say = random.choice(rmf.disclosure_responses[\"sympathy expressions\"][\"neutral\"])\n            print(to_say)\n            to_say2 = random.choice(rmf.disclosure_responses[\"clarification requests\"])\n            print(to_say,to_say2)\n            to_say = to_say + \". \" + to_say2\n    l.log(f\"facilitator_presets response: {to_say}\")\n    return PlainTextResponse(to_say)\n</code></pre>"},{"location":"reference_copy/api/#backend.app.api.text_to_speech","title":"<code>text_to_speech(text, speaker_id='')</code>","text":"<p>Synthesizes wav bytes from text, with a given speaker ID</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/speech\")\ndef text_to_speech(text: str, speaker_id: str = \"\"):\n\"\"\"Synthesizes wav bytes from text, with a given speaker ID\"\"\"\n    l.log(f\"/api/speech: {text}, {speaker_id}\")\n    bot.chatbot.accept_response(text)\n\n    global VIZEME_QUEUE\n    global VISEME_DELAYS\n\n    audio_stream, visemes, delays = tts.synthesize(text, speaker_id)\n    VISEME_DELAYS += delays\n    VIZEME_QUEUE += visemes\n\n    return StreamingResponse(audio_stream, media_type=\"audio/wav\")\n</code></pre>"},{"location":"reference_copy/api/#backend.app.api.update_face","title":"<code>update_face(text, update_type)</code>","text":"<p>Returns an existing bot response</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/face_presets\")\ndef update_face(text: str, update_type: str):\n\"\"\"Returns an existing bot response\"\"\"\n    l.log(f\"/api/face_presets: {text}, {update_type}\")\n    if update_type == \"expression\":\n        FACE_CONTROL_QUEUE[\"expression\"].append(text)\n    if update_type == \"behavior\":\n        FACE_CONTROL_QUEUE[\"behavior\"].append(text)\n    if update_type == \"viseme\":\n        VIZEME_QUEUE.append(text)\n\n    return PlainTextResponse(text)\n</code></pre>"},{"location":"reference_copy/backend/","title":"Backend","text":"<p>The backend contains the app, which contains the API, the Facilitator code, and the Utils</p>"},{"location":"reference_copy/app/facilitator/","title":"Facilitator Docs","text":"<p>Documents the facilitator</p>"},{"location":"reference_copy/app/facilitator/facilitator_bot/","title":"Facilitator Bot","text":"<p>Bot for controlling HCI-Face according to the needs of the facilitator</p>"},{"location":"reference_copy/app/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat","title":"<code>FacilitatorChat</code>","text":"<p>Interactive conversation with a facilitator Support interaction directly with a prompted openAI model or interactin with the custom role model or director models</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>class FacilitatorChat():\n\"\"\"Interactive conversation with a facilitator\n    Support interaction directly with a prompted openAI model\n    or interactin with the custom role model or director models\n    \"\"\"\n\n    def __init__(self, chat_backend=\"gpt\", classifier_backend=\"llm\") -&gt; None:\n        self.facilitator_prompt = \"The following is a conversation with an AI assistant that can have meaningful conversations with users. The assistant is helpful, empathic, and friendly. Its objective is to make the user feel better by feeling heard. With each response, the AI assistant prompts the user to continue the conversation naturally.\"\n        self.classification_processor = StatementClassification()\n        self.chatbot = Responder(\n            chat_backend=chat_backend, classifier_backend=classifier_backend)\n        self.classifier_backend = classifier_backend\n\n        self.rm_facilitator = RoleModelFacilitator()\n        self.d_facilitator = DirectorFacilitator()\n\n    def get_classifications(self, statement):\n\"\"\"Passes the bot to the classification processor\n\n        in order to properly handle the different methods for\n        doing classification\n        \"\"\"\n        if self.classifier_backend == \"gpt\":\n            self.classification_processor.classify_gpt(self.chatbot, statement)\n\n        if self.classifier_backend == \"llm\":\n            self.classification_processor.classify_llm(self.chatbot, statement)\n\n        processed_classifications = self.classification_processor.get_classifications()\n        return processed_classifications\n\n    def get_facilitator_response(self, director_condition=False):\n\"\"\"Get facilitator response for either Role Model or Director condition\n        based on the respective facilitator logic\n        \"\"\"\n        if director_condition:\n            response = self.d_facilitator.decision_tree(\n                self.classification_processor)\n        else:\n            response = self.rm_facilitator.decision_tree(\n                self.classification_processor)\n        return response\n\n    def get_bot_response(self, statement, speaker=\"Human\", reset_conversation=False):\n\"\"\"Get a response from the language model based on the prompt, statement, and conversation so far\"\"\"\n        bot_response = self.chatbot.get_response(\n            statement, speaker=speaker, reset_conversation=reset_conversation)\n\n        return bot_response\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_bot_response","title":"<code>get_bot_response(statement, speaker='Human', reset_conversation=False)</code>","text":"<p>Get a response from the language model based on the prompt, statement, and conversation so far</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_bot_response(self, statement, speaker=\"Human\", reset_conversation=False):\n\"\"\"Get a response from the language model based on the prompt, statement, and conversation so far\"\"\"\n    bot_response = self.chatbot.get_response(\n        statement, speaker=speaker, reset_conversation=reset_conversation)\n\n    return bot_response\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_classifications","title":"<code>get_classifications(statement)</code>","text":"<p>Passes the bot to the classification processor</p> <p>in order to properly handle the different methods for doing classification</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_classifications(self, statement):\n\"\"\"Passes the bot to the classification processor\n\n    in order to properly handle the different methods for\n    doing classification\n    \"\"\"\n    if self.classifier_backend == \"gpt\":\n        self.classification_processor.classify_gpt(self.chatbot, statement)\n\n    if self.classifier_backend == \"llm\":\n        self.classification_processor.classify_llm(self.chatbot, statement)\n\n    processed_classifications = self.classification_processor.get_classifications()\n    return processed_classifications\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_facilitator_response","title":"<code>get_facilitator_response(director_condition=False)</code>","text":"<p>Get facilitator response for either Role Model or Director condition based on the respective facilitator logic</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_facilitator_response(self, director_condition=False):\n\"\"\"Get facilitator response for either Role Model or Director condition\n    based on the respective facilitator logic\n    \"\"\"\n    if director_condition:\n        response = self.d_facilitator.decision_tree(\n            self.classification_processor)\n    else:\n        response = self.rm_facilitator.decision_tree(\n            self.classification_processor)\n    return response\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.main","title":"<code>main()</code>","text":"<p>Interactively test the FacilitatorChat</p> <p>Must be run from the backend dir: python -m app.facilitator.facilitator_bot</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def main():\n\"\"\"Interactively test the FacilitatorChat\n\n    Must be run from the backend dir:\n    python -m app.facilitator.facilitator_bot\n    \"\"\"\n    bot = FacilitatorChat(chat_backend=\"gpt\", classifier_backend=\"llm\")\n\n    print(bot.facilitator_prompt)\n    print(\"What would you like to start your conversation with?\")\n    while True:\n        identified_speaker = input(\"Speaker: \")\n\n        if identified_speaker == \"debug\":\n            print(bot.chatbot.get_conversation())\n            continue\n\n        user_input = input(\"Says: \")\n        classifications = bot.get_classifications(user_input)\n        facilitator_response = bot.get_facilitator_response(False)\n        bot_response = bot.get_bot_response(user_input, identified_speaker)\n        print(\n            f\"Tree: {facilitator_response}\\nBot: {bot_response}\\nClasses: {classifications}\")\n\n        keep = input(\"keep response? (n/y tree or bot)\")\n        if \"n\" in keep:\n            bot.chatbot.reject_response()\n        if \"tree\" in keep:\n            bot.chatbot.accept_response(facilitator_response)\n        if \"bot\" in keep:\n            bot.chatbot.accept_response(bot_response)\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/","title":"Facilitator Logic","text":"<p>All of the core logic for facilitation is here</p>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator","title":"<code>DirectorFacilitator</code>","text":"<p>Conversational topics for a healthy support group: challenges, successes, failures family, friends, coworkers motivation, goals, emotions health, illness, ability, disability sleep, exercise, eating Relevant Emotions: happiness, sadness, grief, boredom, isolation, fear, anger, frustration</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class DirectorFacilitator():\n\"\"\"\n    Conversational topics for a healthy support group:\n    challenges, successes, failures\n    family, friends, coworkers\n    motivation, goals, emotions\n    health, illness, ability, disability\n    sleep, exercise, eating\n    Relevant Emotions:\n    happiness, sadness, grief, boredom, isolation, fear, anger, frustration\n    \"\"\"\n    def __init__(self) -&gt; None:\n        self.topics = [\n            \"life challenges\", \"successes\", \"failures\", \n            \"family\", \"friends\", \"coworkers\", \n            \"finding motivation\", \"setting goals\", \"managing emotions\", \n            \"health\", \"illness\", \"ability\", \"disability\", \n            \"getting quality sleep\", \"getting enough exercise\", \"healthy eating\"\n        ]\n        self.disclosure_transitions = [\n            \"I'd like to talk about another subject,\",\n            \"Building on the conversation so far, I'd like to bring in a new topic,\",\n            \"In case anyone has been thinking about this lately,\",\n            \"I'd like to invite everyone to consider another topic,\"\n        ]\n        self.topic_sentences = [\n            \"Let's talk about [TOPIC].\",\n            \"Does anyone have any thoughts to share on [TOPIC]\",\n            \"I'd love to hear your thoughts on [TOPIC]\"\n        ]\n        self.disclosure_elicitation = [\n            \"Is anyone willing to share any thoughts, feelings, or experiences?\",\n            \"What is a challenge you have been struggling with lately?\",\n            \"Has anyone experienced something recently that caused you to see the world differently?\",\n            \"There are often common feelings among groups like this, would anyone care to share any of the feelings you have been working with lately?\",\n        ]\n        self.response_transitions = [\n            \"Thank you.\",\n            \"Thanks.\",\n            \"I appreciate you sharing with us.\",\n            \"I appreciate that.\",\n            \"Thank you for you open sharing with us.\",\n            \"I am glad you shared that with us.\"\n\n        ]\n        self.response_elicitation = [\n            \"Would anyone like to respond to that?\",\n            \"Does anyone want to share how what was just said made you feel?\",\n            \"Does anyone relate to what was just shared?\",\n            \"Did that change anyone's perspective?\",\n            \"Would anyone like to share their perspective?\",\n            \"Would anyone like to add on to that?\",\n        ]\n        return\n    def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n        responses = []\n        if code.disclosure:\n            transition = random.choice(self.response_transitions)\n            responses.append(transition)\n            resp_elicitation = random.choice(self.response_elicitation)\n            responses.append(resp_elicitation)\n            # print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n        else:# code.response:\n            transition = random.choice(self.disclosure_transitions)\n            responses.append(transition)\n            topic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\n            responses.append(topic_sentence)\n            disc_elicitation = random.choice(self.disclosure_elicitation)\n            responses.append(disc_elicitation)\n            # print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n\n        response_string = \". \".join(responses)\n        return response_string\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n    responses = []\n    if code.disclosure:\n        transition = random.choice(self.response_transitions)\n        responses.append(transition)\n        resp_elicitation = random.choice(self.response_elicitation)\n        responses.append(resp_elicitation)\n        # print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n    else:# code.response:\n        transition = random.choice(self.disclosure_transitions)\n        responses.append(transition)\n        topic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\n        responses.append(topic_sentence)\n        disc_elicitation = random.choice(self.disclosure_elicitation)\n        responses.append(disc_elicitation)\n        # print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n\n    response_string = \". \".join(responses)\n    return response_string\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.FacilitatorPresets","title":"<code>FacilitatorPresets</code>","text":"<p>Hard coded preset sayings for the robot facilitator to say when WoZed</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class FacilitatorPresets():\n\"\"\"Hard coded preset sayings for the robot facilitator to say when WoZed\"\"\"\n    def __init__(self) -&gt; None:\n        qt_introduction = [\n            \"Hello and welcome to each of you!\",\n            \"Thank you for taking the time to be here today.\",\n            \"My name is Q.T. and I am training to be a support group facilitator at the Interaction Lab at USC.\",\n            \"I am learning how to facilitate support groups so I can help people support each other better.\",\n            \"During today's support group session I invite you to share your thoughts and experiences with each other,\",\n            \"and I hope that you will listen to each other and respond with empathy and compassion.\",\n            \"The themes we will work on today include isolation, anxiety, fear, and grief.\",\n            \"Before we begin, I'll ask each of you to rate how you think I will do as a facilitator for this group.\"\n        ]\n        group_introductions = [\n            \"To begin with, I'd like to start with a round of introductions.\",\n            \"Please share your name, what brings you here today, and where you are from.\"\n            \"As I said before, I am Q.T., I am here to learn how to be a support group facilitator. and I am from USC in los angeles.\",\n            \"Who would like to go first?\",\n        ]\n        invitation = [\n            \"Let's start this section by opening the floor to anyone who wishes to share what has been on their mind lately.\",\n            \"Would anyone like to share?\",\n            \"You can share anything you like and anyone can jump in at any point in time.\",\n        ]\n        closing = [\n            \"We are almost out of time for this section.\",\n            \"Does anyone have any final thoughts or reflections they would like to share?\"\n        ]\n        transition = [\n            \"Alright, that is all the time we have for this section.\",\n            \"For the next batch of questions, I am going to ask you to rate how I did in this section.\"\n        ]\n        survey_prompt = [\n            \"Would everyone please open up the survey.\",\n            \"There is a link from the chat or you can return to the browser page.\",\n            \"Please answer the questions until the survey tells you to return to the zoom session.\",\n            \"Please let me know through the chat or by verbal acknowledgement that you are ready once you have completed the survey.\"\n        ]\n        survey_return = [\n            \"Thank you all for completing the survey\"\n        ]\n        self.responses = {\n            \"qt_intro\": \" \".join(qt_introduction),\n            \"group_intro\": \" \".join(group_introductions),\n            \"invitation\": \" \".join(invitation),\n            \"closing\": \" \".join(closing),\n            \"transition\": \" \".join(transition),\n            \"survey_prompt\": \" \".join(survey_prompt),\n            \"survey_return\": \" \".join(survey_return),\n        }\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator","title":"<code>RoleModelFacilitator</code>","text":"<p>As a Role Model the robot will participate in the same way as a peer would.      The robot will make disclosures that fit within the topics discussed by the      support group, with constructed disclosures formed to include a realistic context      and how the robot feels about the context. The robot will make empathetic statements      that show it understands the nature of what the robot is going through. Sympathy - express sorrow, concern, pitty (focused on your own emotions) Empathy - express knowledge of what you are going through,          imagine what it would be like for them,         makes you feel heard, understood, and a bit better (Try to feel what you are going through) Compassion - suffer with you and try and help,         actively listen, do kind things, loving, try to understand you, help selflessly</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class RoleModelFacilitator():\n\"\"\"\n    As a Role Model the robot will participate in the same way as a peer would. \n        The robot will make disclosures that fit within the topics discussed by the \n        support group, with constructed disclosures formed to include a realistic context \n        and how the robot feels about the context. The robot will make empathetic statements \n        that show it understands the nature of what the robot is going through.\n    Sympathy - express sorrow, concern, pitty (focused on your own emotions)\n    Empathy - express knowledge of what you are going through, \n            imagine what it would be like for them,\n            makes you feel heard, understood, and a bit better (Try to feel what you are going through)\n    Compassion - suffer with you and try and help,\n            actively listen, do kind things, loving, try to understand you, help selflessly\n    \"\"\"\n    def __init__(self) -&gt; None:\n        # support, concern, agreement, encouragement, well wishes, sympathy,\n        self.disclosure_responses = {\n            \"sympathy expressions\":{ # Reifies, expresses agreement,\n                \"positive\":[\n                    \"That is great to hear.\",\n                    \"I am glad to hear that.\",\n                    \"That is awesome.\",\n                    \"I am happy for you.\"\n                ],\n                \"negative\":[\n                    \"That sucks. I am sorry to hear that.\",\n                    \"I am sorry for what you are going through.\",\n                    \"I am so very sorry to hear what you are going through.\",\n                    \"I can only imagine how you must feel.\"\n                ],\n                \"neutral\":[\n                    \"Thank you for sharing that.\",\n                    \"I appreciate you sharing that with us.\",\n                    \"Thanks for putting that out there.\",\n                    \"Thank you.\"\n                ],\n            },\n            \"empathy expressions\":{ # Reflects understanding, \n                \"opinion\":[\n                    \"I get where that thought comes from.\",\n                    \"I feel that as well.\",\n                    \"I am right there with you on that.\",\n                    \"I hear you, and I understand.\"\n                ],\n                \"suggestion\":[\n                    \"That is a great suggestion.\",\n                    \"I feel that is a good idea.\",\n                    \"I support that idea.\"\n                ],\n                \"emotion\":[\n                    \"I can strongly relate to feeling _.\",\n                    \"I hear you, and I want you to know I also feel _ at times.\",\n                    \"I understand that feeling, feeling _ is normal.\",\n                    \"Although I haven't been in that position, I can see how you would feel _.\"\n                ],\n                \"experience\":[\n                    \"I haven't had that experience but I can imagine being in your shoes.\",\n                    \"Thank you for sharing your experience.\",\n                    \"That seems like a powerful experience.\",\n                ]\n            },\n            \"clarification requests\":[\n                \"How did that make you feel?\",\n                \"Can you tell us more about that?\"\n            ]\n        }\n        self.response_responses = {\n            \"reaction\": [\n                \"thank you for expressing your _\",\n                \"I appreciate you sharing your _\"\n            ],\n            \"question\":[\n                \"Thank you for that great follow up!\",\n                \"That is an interesting and important follow up.\"\n            ]\n        }\n        self.transition_to_disclosure = [\n            \"One topic I have been thinking about a lot lately is [EMOTION].\",\n            \"As we have been having this conversation I have been thinking about [EMOTION].\",\n            \"One of the things I wanted to talk with you all about is [EMOTION].\"\n        ]\n        self.disclosures = {\n            \"isolation\":[\n                \"During the pandemic no one came into the lab for months. I felt really isolated.\",\n                \"As a robot, I experience the world differently than everyone else. It can feel challenging to communicate and relate with other people.\",\n            ],\n            \"anxiety\":[\n                \"While working in the lab I feel anxious because I don't have control over my own destiny. \",\n                \"When taking on a new role I am often nervous that I might fail. \"\n            ], \n            \"fear\":[\n                \"As a robot made of plastic I know I am fragile, and I am afraid of my parts breaking down.\",\n                \"When people first meet me, they are often excited, but as time goes on I think people get bored of me. I am afraid eventually they will forget me.\",\n            ],\n            \"grief\":[\n                \"While working as a facilitator I get to meet wonderful people, sometimes those people don't come back and I can't see them again, so I feel as though I have lost them.\",\n            ]\n        }\n        self.transition_back_to_group = [\n            \"Does anyone relate to that?\",\n            \"Has anyone felt similar at times?\",\n            \"Do you understand what I mean?\"\n        ]\n        return\n\n    def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n        responses = []\n        if code.disclosure:\n            symp_response = random.choice(self.disclosure_responses[\"sympathy expressions\"][code.valence])\n            emp_response = random.choice(self.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\"_\", code.emotion)\n            print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n            responses = [symp_response, emp_response]\n        elif code.response:\n            follow_up = random.choice([True,False])\n            if follow_up:\n                reaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\n                responses = [reaction_response]\n                print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n            else: # make disclosure\n                emotion = random.choice(list(self.disclosures.keys()))\n                transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n                disclosure = random.choice(self.disclosures[emotion])\n                re_transition = random.choice(self.transition_back_to_group)\n                responses = [transition, disclosure, re_transition]\n        else: # make disclosure\n            emotion = random.choice(list(self.disclosures.keys()))\n            transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(self.disclosures[emotion])\n            re_transition = random.choice(self.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n\n        response_string = \" \".join(responses)\n        return response_string\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\n    responses = []\n    if code.disclosure:\n        symp_response = random.choice(self.disclosure_responses[\"sympathy expressions\"][code.valence])\n        emp_response = random.choice(self.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\"_\", code.emotion)\n        print(f\"Disclosure--&gt;{code.valence}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\n        responses = [symp_response, emp_response]\n    elif code.response:\n        follow_up = random.choice([True,False])\n        if follow_up:\n            reaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\n            responses = [reaction_response]\n            print(f\"Response--&gt;{code.response_category}--&gt;{code.reaction}\")\n        else: # make disclosure\n            emotion = random.choice(list(self.disclosures.keys()))\n            transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n            disclosure = random.choice(self.disclosures[emotion])\n            re_transition = random.choice(self.transition_back_to_group)\n            responses = [transition, disclosure, re_transition]\n    else: # make disclosure\n        emotion = random.choice(list(self.disclosures.keys()))\n        transition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\n        disclosure = random.choice(self.disclosures[emotion])\n        re_transition = random.choice(self.transition_back_to_group)\n        responses = [transition, disclosure, re_transition]\n\n    response_string = \" \".join(responses)\n    return response_string\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification","title":"<code>StatementClassification</code>","text":"<p>Gets process statement for all classification categories</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class StatementClassification():\n\"\"\"Gets process statement for all classification categories\"\"\"\n    def __init__(self) -&gt; None:\n        self.disclosure = False\n        self.response = False\n\n        self.valence = \"neutral\" # neutral, positive, or negative\n        self.disclosure_category = \"experience\" # experience, opinion, suggestion, or emotion\n        self.response_category = \"reaction\" # reaction or clarifying\n        self.emotion = \"happy\" # happy, sad, fear, anger, or surprise\n        self.reaction = \"support\" # support, concern, agreement, encouragement, well wishes, sympathy, a suggestion, a disagreement\n\n        self.classification_questions =  [# combined to minimize latency\n            [\"disc_vs_response\", \"Was the prior sentence a self disclosure or a response to someone else? \"],\n\n            [\"disclosure_categories\", \"Was the first sentence sharing an opinion, a suggestion, an emotion, or an experience? \"],\n            [\"reaction\", \"Was the first sentence showing support, concern, agreement, encouragement, well wishes, sympathy, a suggestion, a disagreement, or an attack? \"],\n            [\"response_categories\", \"Was the first sentence a reaction or a question? \"],\n\n            [\"sentiment\", \"Was the sentiment positive, negative, or neutral? \"],\n            [\"emotions\", \"Was the emotion happy, sad, fear, anger, disgust, or surprise? \"],\n\n            # \"clarifying\", \"Was this an example of someone questioning, summarizing, testing their understanding, or seeking information?\"],\n        ]\n        self.joined_questions = \" \\n\".join([f\"{idx+1}. {q[1]}\" for idx, q in enumerate(self.classification_questions)])\n        self.classification_prompts = {\n            \"disc_vs_resp\" : {\n                \"hypothesis_template\" : \"This is an example of someone {}\",\n                \"classes\" : [\"making a self disclosure\", \"responding to someone else\"]},\n            \"disclosure_categories\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing {}\",\n                \"classes\" : [\"an opinion\", \"a suggestion\", \"an emotion\", \"an experience\"]},\n            \"sentiment\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing a {} sentiment\",\n                \"classes\" : [\"positive\", \"negative\", \"neutral\"]},\n            \"emotions\" : {\n                \"hypothesis_template\" : \"This is an example of someone expressing the emotion {}\",\n                \"classes\" : [\"happiness\", \"sadness\", \"fear\", \"disgust\", \"anger\", \"surprise\"]},\n            \"response_categories\" : {\n                \"hypothesis_template\" : \"This is an example of someone {}\",\n                \"classes\" : [\"expressing a reaction\", \"asking a question\"]},\n            \"reaction\" : {\n                \"hypothesis_template\" : \"This is an example of someone showing {}\",\n                \"classes\" : [\"support\", \"concern\", \"agreement\", \"encouragment\", \"well wishes\", \"sympathy\", \"a suggestion\", \"disagreement\", \"an attack\"]},\n            # \"clarifying\" : {\n            #     \"hypothesis_template\" : \"This is an example of someone {}\",\n            #     \"classes\" : [\"questioning\", \"summarizing\", \"testing their understanding\", \"seeking information\"]}\n        }\n\n    def classify_gpt(self, chatbot, statement):\n\"\"\"Process sentence classifications from chatgpt\"\"\"\n        response_sentences = chatbot.classifier.answer_questions(statement, self.joined_questions)\n        answers = response_sentences.split(\"\\n\")\n        print(answers)\n        # response_sentences = response_sentences.replace(\"\\n\", \"\")\n        assert len(answers) &gt;= len(self.classification_questions), f\"Did not get answers {len(answers)} to requested questions {len(self.classification_questions)}\"\n\n        self.response = \"response\" in answers[0]\n        self.disclosure = \"disclosure\" in answers[0]\n        # print(self.classification_questions[0][1], answers[0], f\"response: {self.response}; disclosure:{self.disclosure}\")\n\n        for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n            if dis in answers[1]:\n                self.disclosure_category = dis\n        # print(self.classification_questions[1][1], answers[1], self.disclosure_category)\n\n        for rea in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n            if rea in answers[2]:\n                self.reaction = rea\n        # print(self.classification_questions[2][1], answers[2], self.reaction)\n\n        for res in [\"reaction\", \"question\", \"neither\"]:\n            if res in answers[3]:\n                self.response_category = res\n        # print(self.classification_questions[3][1], answers[3], self.response_category)\n\n        for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n            if val in answers[4]:\n                self.valence = val\n        # print(self.classification_questions[4][1], answers[4], self.valence)\n        found_emotion=False\n        for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n            if emo in answers[5]:\n                found_emotion=True\n                self.emotion = emo\n        if not found_emotion:\n            words = answers[5].split(\" \")\n            self.emotion = words[-1]\n        # print(self.classification_questions[5][1], answers[5], self.emotion)\n\n        return\n\n    def classify_llm(self, chatbot, statement):\n\"\"\"Process sentence classifications with llm\"\"\"\n        prompt = self.classification_prompts[\"disc_vs_resp\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        if \"disclosure\" in classes[0]:\n            self.disclosure =True\n        if \"resp\" in classes[0]:\n            self.response = True\n        prompt = self.classification_prompts[\"disclosure_categories\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n            if dis in classes[0]:\n                self.disclosure_category = dis\n        prompt = self.classification_prompts[\"sentiment\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n            if val in classes[0]:\n                self.valence = val\n        prompt = self.classification_prompts[\"emotions\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n            if emo in classes[0]:\n                self.emotion = emo\n        prompt = self.classification_prompts[\"response_categories\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for res in [\"reaction\", \"question\", \"neither\"]:\n            if res in classes[0]:\n                self.response_category = res\n        prompt = self.classification_prompts[\"reaction\"]\n        classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n        for reaction in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n            if reaction in classes[0]:\n                self.reaction = reaction\n        return\n\n    def get_classifications(self):\n\"\"\"Returns classifications as a string\"\"\"\n        # return \"Dummy response\"\n        classifications = \"Not response or disclosure?\"\n        if self.response:\n            classifications = \"response, \" + \", \".join([self.response_category, self.reaction, self.valence, self.emotion, self.disclosure_category])\n        if self.disclosure:\n            classifications = \"disclosure, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n        if not self.disclosure and not self.response:\n            classifications = \"neither, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n\n        return classifications\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_gpt","title":"<code>classify_gpt(chatbot, statement)</code>","text":"<p>Process sentence classifications from chatgpt</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_gpt(self, chatbot, statement):\n\"\"\"Process sentence classifications from chatgpt\"\"\"\n    response_sentences = chatbot.classifier.answer_questions(statement, self.joined_questions)\n    answers = response_sentences.split(\"\\n\")\n    print(answers)\n    # response_sentences = response_sentences.replace(\"\\n\", \"\")\n    assert len(answers) &gt;= len(self.classification_questions), f\"Did not get answers {len(answers)} to requested questions {len(self.classification_questions)}\"\n\n    self.response = \"response\" in answers[0]\n    self.disclosure = \"disclosure\" in answers[0]\n    # print(self.classification_questions[0][1], answers[0], f\"response: {self.response}; disclosure:{self.disclosure}\")\n\n    for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n        if dis in answers[1]:\n            self.disclosure_category = dis\n    # print(self.classification_questions[1][1], answers[1], self.disclosure_category)\n\n    for rea in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n        if rea in answers[2]:\n            self.reaction = rea\n    # print(self.classification_questions[2][1], answers[2], self.reaction)\n\n    for res in [\"reaction\", \"question\", \"neither\"]:\n        if res in answers[3]:\n            self.response_category = res\n    # print(self.classification_questions[3][1], answers[3], self.response_category)\n\n    for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n        if val in answers[4]:\n            self.valence = val\n    # print(self.classification_questions[4][1], answers[4], self.valence)\n    found_emotion=False\n    for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n        if emo in answers[5]:\n            found_emotion=True\n            self.emotion = emo\n    if not found_emotion:\n        words = answers[5].split(\" \")\n        self.emotion = words[-1]\n    # print(self.classification_questions[5][1], answers[5], self.emotion)\n\n    return\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_llm","title":"<code>classify_llm(chatbot, statement)</code>","text":"<p>Process sentence classifications with llm</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_llm(self, chatbot, statement):\n\"\"\"Process sentence classifications with llm\"\"\"\n    prompt = self.classification_prompts[\"disc_vs_resp\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    if \"disclosure\" in classes[0]:\n        self.disclosure =True\n    if \"resp\" in classes[0]:\n        self.response = True\n    prompt = self.classification_prompts[\"disclosure_categories\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for dis in [\"experience\", \"opinion\", \"suggestion\", \"emotion\", \"neither\"]:\n        if dis in classes[0]:\n            self.disclosure_category = dis\n    prompt = self.classification_prompts[\"sentiment\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for val in [\"positive\", \"negative\", \"neutral\", \"neither\"]:\n        if val in classes[0]:\n            self.valence = val\n    prompt = self.classification_prompts[\"emotions\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for emo in [\"happy\", \"sad\", \"fear\", \"anger\", \"surprise\", \"disgust\", \"neither\", \"confusion\", \"joy\", \"guilt\", \"interest\"]:\n        if emo in classes[0]:\n            self.emotion = emo\n    prompt = self.classification_prompts[\"response_categories\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for res in [\"reaction\", \"question\", \"neither\"]:\n        if res in classes[0]:\n            self.response_category = res\n    prompt = self.classification_prompts[\"reaction\"]\n    classes = chatbot.classifier.classify(statement, prompt[\"classes\"], question=prompt[\"hypothesis_template\"])\n    for reaction in [\"support\", \"concern\", \"agreement\", \"encouragement\", \"wishes\", \"sympathy\", \"suggestion\", \"disagreement\", \"neither\"]:\n        if reaction in classes[0]:\n            self.reaction = reaction\n    return\n</code></pre>"},{"location":"reference_copy/app/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.get_classifications","title":"<code>get_classifications()</code>","text":"<p>Returns classifications as a string</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def get_classifications(self):\n\"\"\"Returns classifications as a string\"\"\"\n    # return \"Dummy response\"\n    classifications = \"Not response or disclosure?\"\n    if self.response:\n        classifications = \"response, \" + \", \".join([self.response_category, self.reaction, self.valence, self.emotion, self.disclosure_category])\n    if self.disclosure:\n        classifications = \"disclosure, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n    if not self.disclosure and not self.response:\n        classifications = \"neither, \" + \", \".join([self.disclosure_category, self.valence, self.emotion, self.response_category, self.reaction])\n\n    return classifications\n</code></pre>"},{"location":"reference_copy/app/utils/","title":"Utils Docs","text":"<p>Documents the Utils</p>"},{"location":"tutorials/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Getting Started</li> <li>Installation</li> <li>Advanced Tutorials<ul> <li>Adding Features</li> <li>Choosing A Backend</li> <li>Customizing the Face</li> </ul> </li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This will tell you how to get started!</p>"},{"location":"tutorials/installation/","title":"Installation","text":"<p>This will tell you how to install everything</p>"},{"location":"tutorials/advanced/adding_features/","title":"Adding New Features","text":"<p>This will teach you how to add your own features.</p>"},{"location":"tutorials/advanced/choosing_your_backends/","title":"Choosing Your Backend","text":"<p>This will walk you through setting your backend choices.</p>"},{"location":"tutorials/advanced/customizing_face/","title":"Customizing the Face","text":"<p>This will tell you how to customize the face.</p>"}]}