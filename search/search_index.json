{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the HCI-FACE Documentation","text":"<p> This repository contains all you need to create an interactive conversational bot. With a customizable animated face (left) and a customizable set of controls (right) and everything you need to do interactive conversation (not pictured), HCI-FACE is everything you need.</p>"},{"location":"#about-hci-face","title":"About HCI-FACE","text":""},{"location":"#human-computer-interaction-facial-animation-and-conversation-engine","title":"Human-Computer Interaction Facial Animation and Conversation Engine","text":"<p>Also Known As: OSCAR, your Open Source Communication AnimatoR</p> <p>HCI-FACE is an open source, fully customizable, full stack, interaction engine. It combines a React front end with a python back end to power an animated face capable of real time voice based interaction.</p> <p>HCI-FACE utilizes freely available, open source elements to power all the interaction components, but also supports easily swapping modules (TTS, STT, chatbot, etc.) for use with paid cloud services.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<p>You will need to have nodejs (&gt;=16 - (Option 2 or 3 in these instructions)[https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-ubuntu-20-04]) and a package manager installed.</p>"},{"location":"#front-end","title":"Front End","text":"<p>To get started you will need to install the face and WoZ front ends. Run the following command in frontend/face and frontend/WoZ folders:</p> <p><code>npm install</code></p> <p>Then to launch each webpage (in a development environment), run the following command in frontend/face and frontend/WoZ folders:</p> <p><code>npm start</code></p> <p>The browser should launch with each page automatically, but you may need to past the address into the url bar.</p> <p>For further details see the README in the face and WoZ folders.</p>"},{"location":"#qt-front-end","title":"QT Front End","text":"<p>See the hci_face_bridge README for more information on setting up QT with HCI Face</p>"},{"location":"#back-end","title":"Back End","text":"<p>To use the back end, install the requirements.txt with your favorite package manager. I suggest anaconda </p> <pre><code>conda create --name hci-face python=3.9\n\nconda activate hci-face\n\ncd backend\n\npip install -r requirements.txt\n</code></pre> <p>If you have trouble installing pyaudio, I recommend following these directions.</p> <p>This may take a while, depending on your internet connection.  </p> <p>Any huggingface models you choose to use should be installed automatically on first use.</p> <p>If you wish to use chatGPT (enabled by default) you will need to add your API key to your environment.</p> <p>For further details see the README in the backend folder.</p>"},{"location":"#components","title":"Components","text":""},{"location":"#front-end_1","title":"Front End","text":"<p>The front end is a single react web page with the following elements:</p> <ul> <li>An animated face of SVG elements animated with the react-spring library  </li> <li>Microphone input passed through a websocket to the python backend  </li> <li>A form input for testing and controlling elements of the face  </li> </ul> <p>The face design is easily modified and customized, while utilizing FACs for animation control to maintain animation accross differing visual renderings.</p>"},{"location":"#back-end_1","title":"Back End","text":"<p>The back end consists of python libraries for TTS, STT, and chatbot interaction. The back end is connected to the front end with fastapi. The open source backend components are:</p> <ul> <li>Text-To-Speech (TTS): TTS uses models powered by the coqui tts library.  </li> <li>Speech-To-Text (STT): STT uses whisper models from huggingface.</li> <li>Chatbot: The chatbot functionality is currently provided by generative models from huggingface such as GPT-NEO.</li> </ul> <p>These modules are imported into the app/api.py and can be replaced with private/paid modules by switching the imports.</p>"},{"location":"#helpful-resources","title":"Helpful Resources","text":""},{"location":"#front-end_2","title":"Front End","text":"<p>Facial Action Units Resources </p> <p>SVG Drawing Tool</p> <p>Emotion Expression</p> <p>Viseme Cheat Sheet and Lip Synchronization</p>"},{"location":"#back-end_2","title":"Back End","text":"<p>Coqui-ai</p> <p>Whisper</p> <p>HuggingFace</p> <p>OpenAI</p> <p>Amazon</p>"},{"location":"#todo-list","title":"TODO List","text":"<p>\"*\" Items open for contribution</p>"},{"location":"#front-end_3","title":"Front End","text":"<ul> <li> Different visuals presets  <ul> <li> Additional idle behaviors such as breathing, thinking, etc.</li> </ul> </li> <li> Improve accuracy of lip sync for Coqui*<ul> <li>Lip sync seems effective for polly</li> </ul> </li> <li> Allow exporting faces from AU form for custom expressions</li> <li> Seperate left and right control for facial expressions</li> <li> Support for multi-page WoZ controlls<ul> <li>Possibly merge face and WoZ back together?</li> </ul> </li> </ul>"},{"location":"#back-end_3","title":"Back End","text":"<ul> <li> finish face control api (AU and left-right control) </li> </ul>"},{"location":"#long-term-goals","title":"Long Term Goals","text":"<ul> <li> Interaction Lab greeter</li> <li> meeting note taker  <ul> <li> Fully automatic speaker detection and labeling.</li> </ul> </li> <li> Timer work encouragement  </li> <li> Philosophy Teacher</li> <li> Meditation Leader</li> <li> Basic Q&amp;A</li> <li> Deploy to the cloud</li> </ul>"},{"location":"#docs","title":"Docs","text":"<ul> <li> Make tutorials<ul> <li> How to Customizing the bot appearance * </li> <li> How to contribute</li> <li> How to customize responses</li> <li> How to add a new API service</li> <li> Organize dependency requirements</li> </ul> </li> </ul>"},{"location":"backend/","title":"HCI-FACE Backend","text":"<p>This backend provides an API for stt, tts, and chatbot functionality to the HCI-FACE. </p>"},{"location":"backend/#getting-started","title":"Getting Started","text":"<p>To start up the api, run: <pre><code>python -m main\n</code></pre></p> <p>The stt, tts, and chatbot functionality is placed into the utils folder. Each can be used and tested individually.</p> <p>Detailed descriptions of each module can be found in the Modules Section.</p> <p>Full code references are included in the Reference Section.</p>"},{"location":"backend/#tasks","title":"Tasks","text":"<ul> <li> finish face control api (AU and left-right control) </li> </ul>"},{"location":"backend/#roadmap","title":"Roadmap","text":""},{"location":"backend/#chatbot","title":"Chatbot","text":"<ul> <li> Additional local backends<ul> <li>currently just have support for hugginface models and pipeline, will add support for custom pytorch or other ml models</li> </ul> </li> <li> Additional cloud backends<ul> <li>currently just support OpenAI, but in future will add support for other classification and chatbot services, starting with aws.</li> </ul> </li> </ul>"},{"location":"backend/#stt","title":"STT","text":"<p>The current STT module is stable, but future plans include:</p> <ul> <li> Live diarization<ul> <li>Diarization is currently only available for transcribing full audio files. Future work will enable integrate the stt more closely with the conversation, so that speakers can be tracked while they are transcribed from audio clips.</li> </ul> </li> <li> Stream handling<ul> <li>Stream segmentation has been moved to the frontend for now. Future work will enable command line support for transcribing live streams.</li> </ul> </li> <li> Cloud STT &amp; Diarization<ul> <li>Currently the stt module is entirely local, as it doesn't require heavy computational resources or a GPU. In the future we will add support for cloud transcription to better support use on lightweight/edge devices.</li> </ul> </li> </ul>"},{"location":"backend/#tts","title":"TTS","text":"<p>The current TTS module is stable, particularly with polly. Coqui and Viseme generation will need a fair amount of work, but that should not change the fundamental module api.</p> <ul> <li> Improved viseme processing<ul> <li>This will bee done in conjunction with the frontend, to improve the accuracy and number of visemes provided. Additional work needs to be done to improve the timing of the custom viseme generation.</li> </ul> </li> <li> Coqui documentation<ul> <li>The current module is a mess, I'll clean it up and standardize the documentation.</li> </ul> </li> </ul>"},{"location":"backend/#recording","title":"Recording","text":"<p>In the future we will support more sophisticated logging accross all modules.</p>"},{"location":"frontend/","title":"Frontend","text":"<p>The front end renders the face and the interactive elements of HCI-FACE.</p> <p>These can typically be run with the following commands in the face and WoZ directories: <pre><code>npm install\nnpm start\n</code></pre></p>"},{"location":"frontend/#face","title":"Face","text":"<p>The design of the face is based on the CoRDial framework. The app recieves server sent events from the backend API in order to know what expressions to make. The faceControl api can send expressions or behaviors, while visemes are process seperately to allow talking and expression to take place at the same time.</p> <p>There are currently three faces available: default, cordial, and qt. You can choose the face by changing the face option in the Head component in the App.js:</p> <p><code>&lt;Head face=\"cordial\" ... \\&gt;</code></p> <p>The face package is made up of the form and head components as well as helpers for converting text (behaviors, expressions, visemes) into action units.</p>"},{"location":"frontend/#rendering-the-head","title":"Rendering the Head","text":"<p>The head component is where the head is drawn in SVG. The eyes, brows, and mouth are rendered seperately based on the facial action units. The face adjusts the location of the drawing to new action unit positions by interpolating with the react-spring library.</p>"},{"location":"frontend/#form-control","title":"Form Control","text":"<p>The Form component provides detailed AU control through form input with sliders for all available action units. Not all faces take advantage of all action units. This form is useful for designing what expressions or behaviors look like.</p>"},{"location":"frontend/#helpers","title":"Helpers","text":"<p>Helpers are functions where visemes, expressions, and behaviors are encoded into the action units that will be consumed by the face. The visemes and expressions are static, but behaviors is based on repeating loops of expressions or eye movements.</p>"},{"location":"frontend/#woz","title":"WoZ","text":"<p>The Wizard of Oz (spelled WoZ) app is used to send communicate with and control the backend. The buttons and forms and inputs can be easily arranged to suit the needs of the experimentor.</p> <p>The WoZ interface does not communicate directly with the face but instead with the backend api. </p>"},{"location":"frontend/#tasks","title":"Tasks","text":"<ul> <li> Different visuals presets  <ul> <li> Additional idle behaviors such as breathing, thinking, etc.</li> </ul> </li> <li> Improve accuracy of lip sync for Coqui*<ul> <li>Lip sync seems effective for polly</li> </ul> </li> <li> Allow exporting faces from AU form for custom expressions</li> <li> Seperate left and right control for facial expressions</li> <li> Support for multi-page WoZ controlls<ul> <li>Possibly merge face and WoZ back together?</li> </ul> </li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Help","text":"<p>This is a record of all the errors that have cropped up and how they were fixed.</p>"},{"location":"troubleshooting/#address-already-in-use","title":"Address already in use","text":"<p>If the address of the mkdocs is already in use, you can change the dev_addr parameter in mkdocs.ym</p>"},{"location":"troubleshooting/#frontend-troubleshooting","title":"Frontend Troubleshooting","text":""},{"location":"troubleshooting/#permission-denied-when-running-npm-start","title":"Permission Denied when running npm start","text":"<p>The npm package may not be installed, particularly if you deleted it or switched branches.</p> <p>Fix: Reinstall with <code>npm install</code> before attempting to start again</p>"},{"location":"troubleshooting/#the-visemes-dont-appear-to-run-as-long-as-the-audio","title":"The visemes don't appear to run as long as the audio","text":"<p>This is probably caused by more than one client to the viseme stream. Make sure you have only one face open.</p>"},{"location":"troubleshooting/#no-expressions-or-visemes-appear-as-directed","title":"No expressions or visemes appear as directed","text":"<p>Make sure the face is requesting messages from your computer's IP. You can find this with ifconfig and update the server_ip in face/src/app.js</p>"},{"location":"troubleshooting/#backend-troubleshooting","title":"Backend Troubleshooting","text":""},{"location":"troubleshooting/#i-cant-seem-to-install-pyaudio","title":"I can't seem to install pyaudio","text":"<p>Pyaudio is based on PortAudio which allows for cross system audio.</p> <p>If you have trouble installing pyaudio, I recommend following (these directions)[https://people.csail.mit.edu/hubert/pyaudio/].</p>"},{"location":"troubleshooting/#pyannote-requires-an-authorization-token","title":"pyannote requires an authorization token:","text":"<ol> <li>visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation    and accept user conditions (only if requested)</li> <li>visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)</li> </ol>"},{"location":"troubleshooting/#cuda-gpu-requirements","title":"CUDA &amp; GPU Requirements","text":"<p>Because of cloud supported models, CUDA is optional. CUDA is only required for the zero_shot module in the chatbot utility. For the coqui tts module gpu acceleration is optional and is currently off by default.</p> <p>If you would like to use HCI-FACE with GPU support and are running into issues with CUDA or NVIDIA drivers, I typically recommend purging your system and installing them from scratch.</p>"},{"location":"modules/","title":"Modules","text":"<p>The HCI-FACE is designed to take advantage of python modules flexibility. By creating a set of utility modules and seperating them from the app modules we keep the code well organized and clean.</p>"},{"location":"modules/#hci-face-utility-module-logic","title":"HCI-FACE Utility Module Logic","text":"<p>The basic components of a control loop are Sense-Plan-Act (repeat). For a conversational agent, we map the basic control loop as follows:</p> <p>Sense -&gt; Takes human speech and turns it into text (STT)</p> <p>Plan -&gt; The chatbot must then decide how to respond (Chatbot)</p> <p>Act -&gt; The robot response must be converted back into speech, including audio and visemes (TTS)</p> <p>For each of these modules (STT, Chatbot, TTS), we would like to support options for local and cloud based methods, or backends. These backends must have a common interface to expose to the API, so we have wrapped the backend module with a common interface where the backend can be chosen at run time.</p> <p>We plan to add more supported backends for each module as soon as possible.</p>"},{"location":"modules/chatbot/","title":"Chatbot","text":"<p>The interface to chatbot functionality is through the responder module. We currently include huggingface models and an interface for ChatGPT. Both can be tested by directly running the zero_shot and chatgpt files.</p>"},{"location":"modules/chatbot/#installation","title":"Installation","text":"<p>You can follow the huggingface installation instructions and the OpenAI python api instructions to set up your chatbot.</p>"},{"location":"modules/chatbot/#getting-started","title":"Getting Started","text":"<p>There are three basic ways to interact with the chatbot.</p> <ol> <li> <p>You can interact directly with the zero_shot or chatgpt modules.     These can be run directly from the command line.</p> </li> <li> <p>You can strike up an interactive conversation with the responder module     either through the command line or by importing it into a different script.</p> </li> <li> <p>For more complex projects I recommend looking at the facilitator module to see      how the chatbot backend was incorporated for a combination of controlled and     uncontrolled responses.</p> </li> </ol>"},{"location":"modules/chatbot/#details","title":"Details","text":""},{"location":"modules/chatbot/#roadmap","title":"Roadmap","text":"<ul> <li> Additional local backends<ul> <li>currently just have support for hugginface models and pipeline, will add support for custom pytorch or other ml models</li> </ul> </li> <li> Additional cloud backends<ul> <li>currently just support OpenAI, but in future will add support for other classification and chatbot services, starting with aws.</li> </ul> </li> </ul>"},{"location":"modules/facilitator/","title":"Facilitator","text":"<p>The Facilitator is an attempt to create a support group facilitator. </p> <p>The logic is contained in facilitator_logic.py, with a set of custom classifcations, methods for generating role model and director type facilitation responses, and presets for a facilitation study.</p> <p>To talk with the facilitator run the following from the /backend directory <pre><code>python -m app.facilitator.facilitator_bot\n</code></pre></p>"},{"location":"modules/recording/","title":"Logging","text":"<p>Right now we support logging what passes through the fastapi in api.py.</p>"},{"location":"modules/recording/#installation","title":"Installation","text":"<p>No instillation needed.</p>"},{"location":"modules/recording/#getting-started","title":"Getting Started","text":"<p>Logging is automatic in the api.</p>"},{"location":"modules/recording/#details","title":"Details","text":"<p>Logs are saved to a logging folder with a timestamp.</p>"},{"location":"modules/recording/#roadmap","title":"Roadmap","text":"<p>In the future we will support more sophisticated logging accross all modules.</p>"},{"location":"modules/stt/","title":"Speech To Text","text":"<p>We use a downloaded copy of the whisper speech-to-text model and pyannote to diarize the output. </p>"},{"location":"modules/stt/#installation","title":"Installation","text":"<p>To run this sub package directly you will need to have pyaudio, whisper and pyannote installed. Please follow the directions from MIT for pyaudio, the directions from openai for whisper, and these directions for pyanote</p>"},{"location":"modules/stt/#getting-started","title":"Getting Started","text":"<p>To transcribe an audio file it is easiest to use the transcriber module.</p> <p>To run this file directly, do so from the utils directory: <pre><code>    cd backend/app/utils/\npython -m stt.transcriber --filename [path/to/file]\n</code></pre></p>"},{"location":"modules/stt/#backend-components","title":"Backend Components","text":"<p>The backend components (whisper, diarization, and recording) are available as self contained modules for individual usage as well.</p>"},{"location":"modules/stt/#details","title":"Details","text":"<p>Whisper has a variety of model sizes and model types. Currently we default to the english models but it is relatively simple to set up other languages as well.</p> <p>For full details on all modules please see the References in the documentation.</p>"},{"location":"modules/stt/#roadmap","title":"Roadmap","text":"<p>The current STT module is stable, but future plans include:</p> <ul> <li> Live diarization<ul> <li>Diarization is currently only available for transcribing full audio files. Future work will enable integrate the stt more closely with the conversation, so that speakers can be tracked while they are transcribed from audio clips.</li> </ul> </li> <li> Stream handling<ul> <li>Stream segmentation has been moved to the frontend for now. Future work will enable command line support for transcribing live streams.</li> </ul> </li> <li> Cloud STT &amp; Diarization<ul> <li>Currently the stt module is entirely local, as it doesn't require heavy computational resources or a GPU. In the future we will add support for cloud transcription to better support use on lightweight/edge devices.</li> </ul> </li> </ul>"},{"location":"modules/tts/","title":"Text To Speech","text":"<p>The text to speech is done through an open source library courtesy of coqui.ai. This produces a wav file for a given text input. Many models are available through coqui, but we utilize the VITS end-to-end model trained on the VCTK multi-speaker dataset, as this provides the clearest voices in a variety of accents.</p> <p>To produce an approximation of the vizemes that correspond to the phonemes, we have a phoneme-viseme table that we use to produce simple mouth shapes corresponding to the sounds being produced. This is currently an approximation and could use further improvement.</p> <p>Polly is the recommended option, as it is higher quality, but it requires setting up an AWS account and enabling polly. Coqui is an open source synthesizer that runs locally, but the voices are not as high quality.</p>"},{"location":"modules/tts/#installation","title":"Installation","text":"<p>To use polly you will need to get set up with AWS. To use coqui TTS you will just need to pip install TTS, through more information can be found on coqui's github</p>"},{"location":"modules/tts/#getting-started","title":"Getting started","text":"<p>To generate speech it is easiest to use the speaker module.</p> <p>To run this file directly, do so from the utils directory: <pre><code>    cd backend/app/utils/\npython -m tts.speaker\n</code></pre></p>"},{"location":"modules/tts/#backend-components","title":"Backend Components","text":"<p>The backend components (polly, coqui tts) are available as self contained modules for individual usage as well.</p>"},{"location":"modules/tts/#details","title":"Details","text":"<p>For Coqui, we are using the /en/vctk/vits model, as it has a wide range of voices with reasonable acccuracy. Other models are supported and can be found through their github, including instructions on how to train  your own voice encoder.</p> <p>Unfortunately a bug in their training process means the model speaker labels don't match the training set, but we have list of a few speakers that we have matched to the model input names.</p>"},{"location":"modules/tts/#roadmap","title":"Roadmap","text":"<p>The current TTS module is stable, particularly with polly. Coqui and Viseme generation will need a fair amount of work, but that should not change the fundamental module api.</p> <ul> <li> Improved viseme processing<ul> <li>This will bee done in conjunction with the frontend, to improve the accuracy and number of visemes provided. Additional work needs to be done to improve the timing of the custom viseme generation.</li> </ul> </li> <li> Coqui documentation<ul> <li>The current module is a mess, I'll clean it up and standardize the documentation.</li> </ul> </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>api</li> <li>facilitator<ul> <li>facilitator_bot</li> <li>facilitator_logic</li> </ul> </li> <li>utils<ul> <li>chatbot<ul> <li>backends<ul> <li>chatgpt</li> <li>zero_shot</li> </ul> </li> <li>responder</li> </ul> </li> <li>recording<ul> <li>logger</li> </ul> </li> <li>stt<ul> <li>backends<ul> <li>pyannote_diarization</li> <li>record_mic</li> <li>whisper_stt</li> </ul> </li> <li>transcriber</li> </ul> </li> <li>tts<ul> <li>backends<ul> <li>aws_polly_tts</li> <li>coqui_tts</li> <li>resources<ul> <li>aws_example_code<ul> <li>polly_lipsync</li> <li>polly_wrapper</li> </ul> </li> </ul> </li> <li>viseme_generator</li> </ul> </li> <li>speaker</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/api/","title":"api","text":"<p>Exposes interactive bot functionality through FastAPI</p> <p>The backend consists of modules that expose the functionality needed for a interactive multi-modal chatbot. This module provides an api to that functionality that can be served to any frontend application.</p> <p>The api provides methods for various forms of synchronous and asynchronous communication</p> Synchronous vs Asynchronous <ul> <li> <p>Functions that start with 'get' are synchronous.</p> </li> <li> <p>Functions that start with 'run' are asynchronous, and the result are returned through functions that start with 'stream'.</p> </li> <li> <p>Functions that start with 'add_to' are semi-synchronous in that they synchronously add to a queue which will be asynchronously processed by the face/bot.</p> </li> </ul> <p>The api also allows communication between two different clients, such as how the WoZ interface can send a gesture to the api, and the api will essentially forward that gesture to the robot.</p> Typical usage <p>As documented in https://fastapi.tiangolo.com/deployment/manually/ The api can be run from another module: <pre><code>uvicorn.run(\"app.api:app\", host=\"0.0.0.0\", port=8000, reload=True, log_level=\"error\")\n</code></pre> or it can be run directly: <pre><code>uvicorn api:app --host 0.0.0.0 --port 80\n</code></pre></p>"},{"location":"reference/api/#backend.app.api.add_to_face","title":"<code>add_to_face(text, update_type)</code>","text":"<p>Sends desired expression, behavior, or viseme to the face</p> <p>Face presets are sent from the WoZ to this API, and are then loaded to the queue for the face to read.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>what command to send</p> required <code>update_type</code> <code>str</code> <p>what type of update to send, either expression behavior, or viseme.</p> required <p>Returns:</p> Name Type Description <code>PlainTextResponse</code> <code>PlainTextResponse</code> <p>returns the requested text.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/face_presets\")\ndef add_to_face(text: str, update_type: str) -&gt; PlainTextResponse:\n\"\"\"Sends desired expression, behavior, or viseme to the face\n        Face presets are sent from the WoZ to this API, and are then\n        loaded to the queue for the face to read.\n        Args:\n            text (str): what command to send\n            update_type (str): what type of update to send, either expression\n                behavior, or viseme.\n        Returns:\n            PlainTextResponse: returns the requested text.\"\"\"\nl.log(f\"/api/face_presets: {text}, {update_type}\")\nif update_type == \"expression\":\nFACE_CONTROL_QUEUE[\"expression\"].append(text)\nif update_type == \"behavior\":\nFACE_CONTROL_QUEUE[\"behavior\"].append(text)\nif update_type == \"viseme\":\nFACE_CONTROL_QUEUE[\"viseme\"].append(text)\nreturn PlainTextResponse(text)\n</code></pre>"},{"location":"reference/api/#backend.app.api.add_to_gesture","title":"<code>add_to_gesture(text)</code>","text":"<p>Add desired gesture to queue for the robot</p> <p>This function provides a passthrough from the WoZ to the robot bridge.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Name of the gesture</p> required <p>Returns:</p> Name Type Description <code>PlainTextResponse</code> <code>PlainTextResponse</code> <p>Returns the specified gesture.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/qt_gesture\")\ndef add_to_gesture(text: str) -&gt; PlainTextResponse:\n\"\"\"Add desired gesture to queue for the robot\n        This function provides a passthrough from the WoZ to the robot bridge.\n        Args:\n            text (str): Name of the gesture\n        Returns:\n            PlainTextResponse: Returns the specified gesture.\"\"\"\nl.log(f\"/api/qt_gestures: {text}\")\nGESTURE_QUEUE.append(text)\nreturn PlainTextResponse(text)\n</code></pre>"},{"location":"reference/api/#backend.app.api.get_response","title":"<code>get_response(mode, query)</code>","text":"<p>Returns text presets based on WoZ input</p> <p>Mostly useful for controlling the robot during study interactions. Does not use the text_stream as reponse is usually directly output.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>What mode the facilitator is in. Possibilities include role_model - for when the robot is leading a session as a role model, direcctor - for when the robot is leading a session as a director, and facilitator - for when the robot is not in either condition yet.</p> required <code>query</code> <code>str</code> <p>Key for looking up matching text response.</p> required <p>Returns:</p> Name Type Description <code>PlainTextResponse</code> <code>PlainTextResponse</code> <p>Text for the faciliatator to say.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/facilitator_presets\")\ndef get_response(mode: str, query: str) -&gt; PlainTextResponse:\n\"\"\"Returns text presets based on WoZ input\n        Mostly useful for controlling the robot during study interactions.\n        Does not use the text_stream as reponse is usually directly output.\n        Args:\n            mode (str): What mode the facilitator is in. Possibilities include\n                role_model - for when the robot is leading a session as a role model,\n                direcctor - for when the robot is leading a session as a director,\n                and facilitator - for when the robot is not in either condition yet.\n            query (str): Key for looking up matching text response.\n        Returns:\n            PlainTextResponse: Text for the faciliatator to say.\"\"\"\nl.log(f\"/api/facilitator_presets: {mode}, {query}\")\nif mode == \"facilitator\":\nto_say = presets.responses[query]\nif mode == \"director\":\nif query == \"disclosure\":\nto_say = random.choice(df.disclosure_elicitation)\nif query == \"response\":\nto_say = random.choice(df.response_elicitation)\nif mode == \"role_model\":\nif query == \"disclosure\":\nemotion = random.choice(list(rmf.disclosures.keys()))\ntransition =random.choice(rmf.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\ndisclosure = random.choice(rmf.disclosures[emotion])\nre_transition = random.choice(rmf.transition_back_to_group)\nresponses = [transition, disclosure, re_transition]\nto_say = \" \".join(responses)\nif query == \"response\":\nprint(\"getting response\")\nto_say = random.choice(rmf.disclosure_responses[\"sympathy expressions\"][\"neutral\"])\nprint(to_say)\nto_say2 = random.choice(rmf.disclosure_responses[\"clarification requests\"])\nprint(to_say,to_say2)\nto_say = to_say + \". \" + to_say2\nl.log(f\"facilitator_presets response: {to_say}\")\nreturn PlainTextResponse(to_say)\n</code></pre>"},{"location":"reference/api/#backend.app.api.get_speech","title":"<code>get_speech(text, speaker_id='')</code>","text":"<p>Synthesizes wav bytes from text, with a given speaker ID</p> <p>This text will be spoken immediately after it is generated, so the bot is updated with the knowledge that the facilitator is actually saying this text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to e synthesized</p> required <code>speaker_id</code> <code>str</code> <p>ID of the voice to be used. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>StreamingResponse</code> <code>StreamingResponse</code> <p>Audio stream of the voice saying the text.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/speech\")\ndef get_speech(text: str, speaker_id: str = \"\") -&gt; StreamingResponse:\n\"\"\"Synthesizes wav bytes from text, with a given speaker ID\n        This text will be spoken immediately after it is generated, so the\n        bot is updated with the knowledge that the facilitator is actually\n        saying this text.\n        Args:\n            text (str): Text to e synthesized\n            speaker_id (str, optional): ID of the voice to be used. Defaults to \"\".\n        Returns:\n            StreamingResponse: Audio stream of the voice saying the text.\"\"\"\nl.log(f\"/api/speech: {text}, {speaker_id}\")\nbot.chatbot.accept_response(text)\nglobal FACE_CONTROL_QUEUE\nglobal VISEME_DELAYS\ndt_string = l.get_date_str()\naudio_stream, visemes, delays = tts.synthesize(text,\nspeaker_id,\nsave_path = f\"{LOGS_DIR}/{dt_string}.wav\")\nVISEME_DELAYS += delays\nFACE_CONTROL_QUEUE[\"viseme\"] += visemes\nreturn StreamingResponse(audio_stream, media_type=\"audio/wav\")\n</code></pre>"},{"location":"reference/api/#backend.app.api.return_gesture","title":"<code>return_gesture()</code>","text":"<p>Get next gesture from the queue.</p> <p>This exposes an endpoint for the robot to regularly ping in order to fetch the next gesture it should do.</p> <p>Returns:</p> Name Type Description <code>PlainTextResponse</code> <code>PlainTextResponse</code> <p>desired gesture.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/next_gesture\")\ndef return_gesture() -&gt; PlainTextResponse:\n\"\"\"Get next gesture from the queue.\n        This exposes an endpoint for the robot to regularly ping\n        in order to fetch the next gesture it should do.\n        Returns:\n            PlainTextResponse: desired gesture.\"\"\"\nglobal GESTURE_QUEUE\nif len(GESTURE_QUEUE)&gt;0:\ngesture = GESTURE_QUEUE.pop()\nl.log(f\"/api/next_gesture: {gesture}\")\nelse: gesture=\"\"\n# l.log(f\"/api/next_gesture: {gesture}\")\nreturn PlainTextResponse(gesture)\n</code></pre>"},{"location":"reference/api/#backend.app.api.run_generate_response","title":"<code>run_generate_response(text, speaker, reset_conversation, director_condition)</code>","text":"<p>Takes input text and generates possible bot responses</p> <p>Possible bot responses include generative responses from the chatbot as well as controlled responses from the facilitator. The classifications used for generating the facilitator response are added as well. Additionally, the emotion that was found in the text is mirrored by the robots expression if it is in the subset of possible expressions (joy sad, surprise)</p> warning <p>All of this text is returned asynchronously through the text_stream. The default response is set to the bot response.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input said by a human.</p> required <code>speaker</code> <code>str</code> <p>Identify of the speaker</p> required <code>reset_conversation</code> <code>bool</code> <p>Whether or not to restart the conversation.</p> required <code>director_condition</code> <code>bool</code> <p>Flag for controlling what type of facilitator is used.</p> required <p>Returns:</p> Name Type Description <code>PlainTextResponse</code> <code>PlainTextResponse</code> <p>Defaults to generative bot response.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get(\"/api/bot_response\")\ndef run_generate_response(text: str, speaker: str,\nreset_conversation: bool,\ndirector_condition: bool\n) -&gt; PlainTextResponse:\n\"\"\"Takes input text and generates possible bot responses\n        Possible bot responses include generative responses from the chatbot\n        as well as controlled responses from the facilitator. The classifications\n        used for generating the facilitator response are added as well.\n        Additionally, the emotion that was found in the text is mirrored by\n        the robots expression if it is in the subset of possible expressions (joy\n        sad, surprise)\n        warning:\n            All of this text is returned asynchronously through the text_stream.\n            The default response is set to the bot response.\n        Args:\n            text (str): Input said by a human.\n            speaker (str): Identify of the speaker\n            reset_conversation (bool): Whether or not to restart the conversation.\n            director_condition (bool): Flag for controlling what type of\n                facilitator is used.\n        Returns:\n            PlainTextResponse: Defaults to generative bot response.\"\"\"\nl.log(f\"/api/bot_response: '{text}', from {speaker}, \"\nf\"reset_conversation: {reset_conversation}, director_condition: {director_condition}\")\nglobal TEXT_QUEUE\nclassifications = bot.get_classifications(text)\nTEXT_QUEUE[\"classifications\"].append(classifications)\nif bot.classification_processor.emotion in [\"joy\", \"sad\", \"surprise\"]:\nl.log(f\"Setting face to: {bot.classification_processor.emotion}\")\nFACE_CONTROL_QUEUE[\"expression\"].append(bot.classification_processor.emotion)\nelse:\nl.log(\"Setting face to: neutral\")\nFACE_CONTROL_QUEUE[\"expression\"].append(\"neutral\")\nfacilitator_response = bot.get_facilitator_response(director_condition)\nTEXT_QUEUE[\"facilitator_response\"].append(facilitator_response)\nbot_response= bot.get_bot_response(text, speaker, reset_conversation)\nTEXT_QUEUE[\"bot_response\"].append(bot_response)\nbot.chatbot.reject_response()\nreturn PlainTextResponse(bot_response)\n</code></pre>"},{"location":"reference/api/#backend.app.api.run_transcribe_audio","title":"<code>run_transcribe_audio(uploaded_file)</code>  <code>async</code>","text":"<p>Perform speech to text on audio file</p> <p>transcribes audio from file and adds transcribed text to human_speech in the text queue.</p> <p>Parameters:</p> Name Type Description Default <code>uploaded_file</code> <code>UploadFile</code> <p>Recorded audio.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>name of the saved audio file.</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.post(\"/api/audio\")\nasync def run_transcribe_audio(uploaded_file: UploadFile) -&gt; dict:\n\"\"\"Perform speech to text on audio file\n        transcribes audio from file and adds transcribed text to\n        human_speech in the text queue.\n        Args:\n            uploaded_file (UploadFile): Recorded audio.\n        Returns:\n            dict: name of the saved audio file.\"\"\"\nl.log(\"/api/audio: temp.wav\")\ncontents = uploaded_file.file.read()\ndata_bytes = io.BytesIO(contents)\naudio_clip = AudioSegment.from_file(data_bytes, codec='opus')\nl.log_sound(audio_clip)\ntranscription = stt.transcribe_clip(audio_clip)\nif len(transcription) &gt; 0:\nglobal TEXT_QUEUE\nTEXT_QUEUE[\"human_speech\"].append(transcription)\nl.log(f\"Speech Detected: {transcription}\")\nreturn {\"filename\": \"temp.wav\"}\n</code></pre>"},{"location":"reference/api/#backend.app.api.stream_face","title":"<code>stream_face(request)</code>  <code>async</code>","text":"<p>Publishes face control messages to a subscriber</p> <p>Publishes messages as soon as they are added to the queue. Message event argument specifies the type of face control message being sent, expression or behavior</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Request for event generator when     API is called.</p> required <p>Returns:</p> Name Type Description <code>EventSourceResponse</code> <code>EventSourceResponse</code> <p>Server Sent Event (sse) source.</p> <p>Yields:</p> Type Description <code>EventSourceResponse</code> <p>Iterator[EventSourceResponse]: Strings for a face behavior or expression</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get('/api/face_stream')\nasync def stream_face(request: Request) -&gt; EventSourceResponse:\n\"\"\"Publishes face control messages to a subscriber\n        Publishes messages as soon as they are added to the queue.\n        Message event argument specifies the type of face control\n        message being sent, expression or behavior\n        Args:\n            request (Request): Request for event generator when\n                    API is called.\n        Returns:\n            EventSourceResponse: Server Sent Event (sse) source.\n        Yields:\n            Iterator[EventSourceResponse]: Strings for a face behavior or\n                expression\"\"\"\n# l.log(\"/api/face_stream: request recieved.\")\nasync def event_generator():\nwhile True:\n# If client closes connection, stop sending events\nif await request.is_disconnected():\n# print(\"Disconnected\")\nbreak\nglobal FACE_CONTROL_QUEUE\n# Checks for new messages and return them to client if any\nfor key, msg_queue in FACE_CONTROL_QUEUE.items():\nif key == 'viseme':\ncontinue\nif len(msg_queue) &gt;0:\ndata = msg_queue.pop(0)\nl.log(f\"face control message: {data}\")\nresponse = {\n\"event\": key,\n\"id\": \"message_id\",\n\"retry\": RETRY_TIMEOUT,\n\"data\": data,\n}\nyield response\nawait asyncio.sleep(.03)\nreturn EventSourceResponse(event_generator())\n</code></pre>"},{"location":"reference/api/#backend.app.api.stream_text","title":"<code>stream_text(request)</code>  <code>async</code>","text":"<p>Publishes text messages to a subscriber</p> <p>Publishes messages as soon as they are added to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Request for event generator when     API is called.</p> required <p>Returns:</p> Name Type Description <code>EventSourceResponse</code> <p>Server Sent Event (sse) source.</p> <p>Yields:</p> Type Description <p>Iterator[EventSourceResponse]: Strings of text</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get('/api/text_stream')\nasync def stream_text(request: Request):\n\"\"\"Publishes text messages to a subscriber\n        Publishes messages as soon as they are added to the queue.\n        Args:\n            request (Request): Request for event generator when\n                    API is called.\n        Returns:\n            EventSourceResponse: Server Sent Event (sse) source.\n        Yields:\n            Iterator[EventSourceResponse]: Strings of text\"\"\"\nl.log(\"/api/text: request recieved.\")\nasync def event_generator():\nwhile True:\n# If client closes connection, stop sending events\nif await request.is_disconnected():\n# print(\"Disconnected\")\nbreak\nglobal TEXT_QUEUE\n# Checks for new messages and return them to client if any\nfor key, msg_queue in TEXT_QUEUE.items():\nif len(msg_queue) &gt;0:\ndata = msg_queue.pop(0)\nl.log(f\"Text message: {key}: {data}\")\nresponse = {\n\"event\": key,\n\"id\": \"message_id\",\n\"retry\": RETRY_TIMEOUT,\n\"data\": data,\n}\nyield response\nawait asyncio.sleep(.03)\nreturn EventSourceResponse(event_generator())\n</code></pre>"},{"location":"reference/api/#backend.app.api.stream_viseme","title":"<code>stream_viseme(request)</code>  <code>async</code>","text":"<p>Publishes visemes to a subscriber</p> <p>Publishes the viseme after the corresponding viseme delay. Only publishes when there are visemes in the queue. If no subscribers are listening it will not send any messages and the queue will continue to grow.</p> warning <p>Only publishes each viseme once. If you have multiple subscribers (e.g. multiple tabs or multiple devices with the face open) each of the subscribers will only recieve a subset of the visemes.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>Request for event generator when API is called.</p> required <p>Returns:</p> Name Type Description <code>EventSourceResponse</code> <p>Server Sent Event (sse) source.</p> <p>Yields:</p> Type Description <p>Iterator[EventSourceResponse]: Strings for a viseme (the desired shape of the mouth)</p> Source code in <code>backend/app/api.py</code> <pre><code>@app.get('/api/viseme_stream')\nasync def stream_viseme(request: Request):\n\"\"\"Publishes visemes to a subscriber\n        Publishes the viseme after the corresponding viseme delay.\n        Only publishes when there are visemes in the queue.\n        If no subscribers are listening it will not send any messages\n        and the queue will continue to grow.\n        warning:\n            Only publishes each viseme once. If you have multiple subscribers\n            (e.g. multiple tabs or multiple devices with the face open) each\n            of the subscribers will only recieve a subset of the visemes.\n        Args:\n            request (Request): Request for event generator when\n                API is called.\n        Returns:\n            EventSourceResponse: Server Sent Event (sse) source.\n        Yields:\n            Iterator[EventSourceResponse]: Strings for a viseme\n                (the desired shape of the mouth)\"\"\"\nl.log(\"/api/visemes: request recieved.\")\nasync def event_generator():\nwhile True:\nglobal VISEME_DELAYS\n# If client closes connection, stop sending events\nif await request.is_disconnected():\n# print(\"Disconnected\")\nbreak\nglobal FACE_CONTROL_QUEUE\n# Checks for new messages and return them to client if any\nif len(FACE_CONTROL_QUEUE[\"viseme\"]) &gt; 0:\nmsg = FACE_CONTROL_QUEUE[\"viseme\"].pop(0)\n# l.log(f\"Viseme msg: {msg}\", printnow=True)\nresponse = {\n\"event\": \"viseme\",\n\"id\": \"message_id\",\n\"retry\": RETRY_TIMEOUT,\n\"data\": msg,\n}\nyield response\nif len(VISEME_DELAYS)&gt;0:\nsleep_delay = VISEME_DELAYS.pop(0)\n# l.log(f\"Sleep Delay: {sleep_delay}\", printnow=True)\nawait asyncio.sleep(sleep_delay)\nelse:await asyncio.sleep(.05)\nreturn EventSourceResponse(event_generator())\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/","title":"facilitator_bot","text":"<p>The facilitator bot contains the chatbot functionality for responding as a facilitator.</p> <p>The faciliator chatbot can either return a generated text response or a logical response based on difference classification cases of the input.</p> Typical usage (module) <pre><code>fc = FacilitatorChat()\nstatement = input(\"Type your user input here\")\nclasses = fc.get_classifications(statement)\nfacilitator_response = fc.get_facilitator_response(statement)\nbot_response = fc.get_bot_response(statement)\n</code></pre> Independent usage <p>For testing or interacting directly the module can also be used as a script: <pre><code>python -m app.facilitator.facilitator_bot\n</code></pre></p>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat","title":"<code>FacilitatorChat</code>","text":"<p>Wraps Responder to interactively converse with a facilitator</p> <p>Support interaction directly with a prompted openAI model or interactin with the custom role model or director models</p> <p>Author's Note:     This is primarily for use in the Support Group Facilitator Study.</p> <p>Parameters:</p> Name Type Description Default <code>chat_backend</code> <code>str</code> <p>Which generative model to use. Defaults to \"gpt\".</p> <code>'gpt'</code> <code>classifier_backend</code> <code>str</code> <p>Which classifier model to use. Defaults to \"llm\".</p> <code>'llm'</code> <p>Attributes:</p> Name Type Description <code>facilitator_prompt</code> <code>str</code> <p>Prompt used by the generative chatbots</p> <code>classification_processor</code> <code>obj</code> <p>Object for processing model classifications into a usable format.</p> <code>rm_facilitator</code> <code>obj</code> <p>Logic for the Role Model Facilitator Condition</p> <code>d_facilitator</code> <code>obj</code> <p>Logic for the Director Facilitator Condition</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>class FacilitatorChat():\n\"\"\"Wraps Responder to interactively converse with a facilitator\n        Support interaction directly with a prompted openAI model\n        or interactin with the custom role model or director models\n        Author's Note:\n            This is primarily for use in the Support Group Facilitator Study.\n        Args:\n            chat_backend (str, optional): Which generative model to use. Defaults to \"gpt\".\n            classifier_backend (str, optional): Which classifier model to use. Defaults to \"llm\".\n        Attributes:\n            facilitator_prompt (str): Prompt used by the generative chatbots\n            classification_processor (obj): Object for processing model classifications into a\n                usable format.\n            rm_facilitator (obj): Logic for the Role Model Facilitator Condition\n            d_facilitator (obj): Logic for the Director Facilitator Condition\"\"\"\ndef __init__(self, chat_backend=\"gpt\", classifier_backend=\"llm\") -&gt; None:\nself.classifier_backend = classifier_backend\nself.chatbot = Responder(\nchat_backend=chat_backend, classifier_backend=classifier_backend)\nself.facilitator_prompt = (\"The following is a conversation with an AI assistant that \"\n\"can have meaningful conversations with users. The assistant \"\n\"is helpful, empathic, and friendly. Its objective is to make \"\n\"the user feel better by feeling heard. With each response, the \"\n\"AI assistant prompts the user to continue the conversation \"\n\"naturally.\")\nself.classification_processor = StatementClassification()\nself.rm_facilitator = RoleModelFacilitator()\nself.d_facilitator = DirectorFacilitator()\ndef get_classifications(self, statement: str) -&gt; str:\n\"\"\"Passes the bot to the classification processor.\n            Different classifiers are processed differently. In order to properly handle\n            the different methods for doing classification,\n            Args:\n                statement (str): text of statement for classification.\n            Returns:\n                processed_classifications: joined list of classes that have been classified.\"\"\"\nif self.classifier_backend == \"gpt\":\nself.classification_processor.classify_gpt(self.chatbot, statement)\nif self.classifier_backend == \"llm\":\nself.classification_processor.classify_llm(self.chatbot, statement)\nprocessed_classifications = self.classification_processor.get_classifications()\nreturn processed_classifications\ndef get_facilitator_response(self, director_condition: bool = False) -&gt; str:\n\"\"\"Gets facilitator response for either Role Model or Director condition\n            based on the respective facilitator logic\n            Args:\n                director_condition (bool, optional): True if in the director condition,\n                    false if in the in the role model condition. Defaults to False.\n            Returns:\n                str: Recommended response to come from the facilitator.\"\"\"\nif director_condition:\nresponse = self.d_facilitator.decision_tree(\nself.classification_processor)\nelse:\nresponse = self.rm_facilitator.decision_tree(\nself.classification_processor)\nreturn response\ndef get_bot_response(self, statement: str, speaker: str = \"Human\",\nreset_conversation: bool = False) -&gt; str:\n\"\"\"Get a response from the language model\n            based on the prompt, statement, and conversation so far\n            Args:\n                statement (str): Input statement the bot will respond to.\n                speaker (str, optional): Name of the speaker. Defaults to \"Human\".\n                reset_conversation (bool, optional): Resets the conversation to the\n                    beginning prompt. Defaults to False.\n            Returns:\n                str: Text of the bot response\"\"\"\nstatement_sentences = statement.split(\".\")\nif len(statement_sentences) &gt; 3 and len(statement)&gt;100:\nstatement_sentences  = statement_sentences[-3:]\nstatement = \".\".join(statement_sentences)\nbot_response = self.chatbot.get_response(\nstatement, speaker=speaker, reset_conversation=reset_conversation)\nreturn bot_response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_bot_response","title":"<code>get_bot_response(statement, speaker='Human', reset_conversation=False)</code>","text":"<p>Get a response from the language model based on the prompt, statement, and conversation so far</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>Input statement the bot will respond to.</p> required <code>speaker</code> <code>str</code> <p>Name of the speaker. Defaults to \"Human\".</p> <code>'Human'</code> <code>reset_conversation</code> <code>bool</code> <p>Resets the conversation to the beginning prompt. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Text of the bot response</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_bot_response(self, statement: str, speaker: str = \"Human\",\nreset_conversation: bool = False) -&gt; str:\n\"\"\"Get a response from the language model\n        based on the prompt, statement, and conversation so far\n        Args:\n            statement (str): Input statement the bot will respond to.\n            speaker (str, optional): Name of the speaker. Defaults to \"Human\".\n            reset_conversation (bool, optional): Resets the conversation to the\n                beginning prompt. Defaults to False.\n        Returns:\n            str: Text of the bot response\"\"\"\nstatement_sentences = statement.split(\".\")\nif len(statement_sentences) &gt; 3 and len(statement)&gt;100:\nstatement_sentences  = statement_sentences[-3:]\nstatement = \".\".join(statement_sentences)\nbot_response = self.chatbot.get_response(\nstatement, speaker=speaker, reset_conversation=reset_conversation)\nreturn bot_response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_classifications","title":"<code>get_classifications(statement)</code>","text":"<p>Passes the bot to the classification processor.</p> <p>Different classifiers are processed differently. In order to properly handle the different methods for doing classification,</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>text of statement for classification.</p> required <p>Returns:</p> Name Type Description <code>processed_classifications</code> <code>str</code> <p>joined list of classes that have been classified.</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_classifications(self, statement: str) -&gt; str:\n\"\"\"Passes the bot to the classification processor.\n        Different classifiers are processed differently. In order to properly handle\n        the different methods for doing classification,\n        Args:\n            statement (str): text of statement for classification.\n        Returns:\n            processed_classifications: joined list of classes that have been classified.\"\"\"\nif self.classifier_backend == \"gpt\":\nself.classification_processor.classify_gpt(self.chatbot, statement)\nif self.classifier_backend == \"llm\":\nself.classification_processor.classify_llm(self.chatbot, statement)\nprocessed_classifications = self.classification_processor.get_classifications()\nreturn processed_classifications\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.FacilitatorChat.get_facilitator_response","title":"<code>get_facilitator_response(director_condition=False)</code>","text":"<p>Gets facilitator response for either Role Model or Director condition</p> <p>based on the respective facilitator logic</p> <p>Parameters:</p> Name Type Description Default <code>director_condition</code> <code>bool</code> <p>True if in the director condition, false if in the in the role model condition. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Recommended response to come from the facilitator.</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def get_facilitator_response(self, director_condition: bool = False) -&gt; str:\n\"\"\"Gets facilitator response for either Role Model or Director condition\n        based on the respective facilitator logic\n        Args:\n            director_condition (bool, optional): True if in the director condition,\n                false if in the in the role model condition. Defaults to False.\n        Returns:\n            str: Recommended response to come from the facilitator.\"\"\"\nif director_condition:\nresponse = self.d_facilitator.decision_tree(\nself.classification_processor)\nelse:\nresponse = self.rm_facilitator.decision_tree(\nself.classification_processor)\nreturn response\n</code></pre>"},{"location":"reference/facilitator/facilitator_bot/#backend.app.facilitator.facilitator_bot.main","title":"<code>main()</code>","text":"<p>Interactively test the FacilitatorChat</p> <p>Must be run from the backend dir: $ python -m app.facilitator.facilitator_bot</p> <p>Will run until killed with ctrl+c</p> Source code in <code>backend/app/facilitator/facilitator_bot.py</code> <pre><code>def main():\n\"\"\"Interactively test the FacilitatorChat\n        Must be run from the backend dir:\n        $ python -m app.facilitator.facilitator_bot\n        Will run until killed with ctrl+c\"\"\"\nbot = FacilitatorChat(chat_backend=\"gpt\", classifier_backend=\"gpt\")\nprint(bot.facilitator_prompt)\nprint(\"What would you like to start your conversation with?\")\ndirector_condition = False\nwhile True:\nidentified_speaker = input(\"Speaker: \")\nif identified_speaker == \"debug\":\nprint(bot.chatbot.get_conversation())\ncontinue\nuser_input = input(\"Says: \")\nif user_input == \"switch\":\ndirector_condition = not director_condition\nif director_condition: print(\"Director Condition\")\nelse: print(\"Role Model Condition\")\nclassifications = bot.get_classifications(user_input)\nprint(f\"Classes: {classifications}\")\nfacilitator_response = bot.get_facilitator_response(director_condition)\nprint(f\"Tree: {facilitator_response}\")\nbot_response = bot.get_bot_response(user_input, identified_speaker)\nprint(f\"Bot: {bot_response}\")\nkeep = input(\"keep response? (n/y tree or bot)\")\nif \"n\" in keep:\nbot.chatbot.reject_response()\nif \"tree\" in keep:\nbot.chatbot.accept_response(facilitator_response)\nif \"bot\" in keep:\nbot.chatbot.accept_response(bot_response)\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/","title":"facilitator_logic","text":"<p>All of the core logic for facilitation is here.</p> <p>These classes are meant to be used together to process input text and return an appropriate facilitator response.</p>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator","title":"<code>DirectorFacilitator</code>","text":"<p>Class for generating director facilitation responses.</p> <p>The director style doesn't engage directly with participants but instead focuses on encouraging participants to respond to  one another.</p> Conversational topics for a healthy support group: <p>challenges, successes, failures family, friends, coworkers motivation, goals, emotions health, illness, ability, disability sleep, exercise, eating</p> Relevant Emotions: <p>happiness, sadness, grief, boredom, isolation, fear, anger, frustration</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class DirectorFacilitator():\n\"\"\" Class for generating director facilitation responses.\n    The director style doesn't engage directly with participants\n    but instead focuses on encouraging participants to respond to \n    one another.\n    note: Conversational topics for a healthy support group:\n        challenges, successes, failures\n        family, friends, coworkers\n        motivation, goals, emotions\n        health, illness, ability, disability\n        sleep, exercise, eating\n    note: Relevant Emotions:\n        happiness, sadness, grief, boredom, isolation, fear, anger, frustration\n    \"\"\"\ndef __init__(self) -&gt; None:\nself.topics = [\n\"life challenges\", \"successes\", \"failures\",\n\"family\", \"friends\", \"coworkers\",\n\"finding motivation\", \"setting goals\", \"managing emotions\",\n\"health\", \"illness\", \"ability\", \"disability\",\n\"getting quality sleep\", \"getting enough exercise\", \"healthy eating\"\n]\nself.disclosure_transitions = [\n\"I'd like to talk about another subject,\",\n\"Building on the conversation so far, I'd like to bring in a new topic,\",\n\"In case anyone has been thinking about this lately,\",\n\"I'd like to invite everyone to consider another topic,\"\n]\nself.topic_sentences = [\n\"Let's talk about [TOPIC].\",\n\"Does anyone have any thoughts to share on [TOPIC].\",\n\"I'd love to hear your thoughts on [TOPIC].\"\n]\nself.disclosure_elicitation = [\n\"Is anyone willing to share any thoughts, feelings, or experiences?\",\n\"What is a challenge you have been struggling with lately?\",\n\"Has anyone experienced something recently that caused you to see the world differently?\",\n\"There are often common feelings among groups like this, would anyone care to share any of the feelings you have been working with lately?\",\n]\nself.response_transitions = [\n\"Thank you.\",\n\"Thanks.\",\n\"I appreciate you sharing with us.\",\n\"I appreciate that.\",\n\"Thank you for your willingness to share openly with us.\",\n\"I am glad you shared that with us.\"\n]\nself.response_elicitation = [\n\"Would anyone like to respond to that?\",\n\"Does anyone want to share how what was just said made you feel?\",\n\"Does anyone relate to what was just shared?\",\n\"Did that change anyone's perspective?\",\n\"Would anyone like to share their perspective?\",\n\"Would anyone like to add on to that?\",\n]\nreturn\ndef decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\nresponses = []\nif code.disclosure:\ntransition = random.choice(self.response_transitions)\nresponses.append(transition)\nresp_elicitation = random.choice(self.response_elicitation)\nresponses.append(resp_elicitation)\nprint(f\"(Dir) Response--&gt;{code.response_category}--&gt;{code.reaction}\")\nelse:# code.response:\ntransition = random.choice(self.disclosure_transitions)\nresponses.append(transition)\ntopic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\nresponses.append(topic_sentence)\ndisc_elicitation = random.choice(self.disclosure_elicitation)\nresponses.append(disc_elicitation)\nprint(f\"(Dir) Disclosure--&gt;{code.sentiment}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\nresponse_string = \" \".join(responses)\nreturn response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.DirectorFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\nresponses = []\nif code.disclosure:\ntransition = random.choice(self.response_transitions)\nresponses.append(transition)\nresp_elicitation = random.choice(self.response_elicitation)\nresponses.append(resp_elicitation)\nprint(f\"(Dir) Response--&gt;{code.response_category}--&gt;{code.reaction}\")\nelse:# code.response:\ntransition = random.choice(self.disclosure_transitions)\nresponses.append(transition)\ntopic_sentence = random.choice(self.topic_sentences).replace(\"[TOPIC]\", random.choice(self.topics))\nresponses.append(topic_sentence)\ndisc_elicitation = random.choice(self.disclosure_elicitation)\nresponses.append(disc_elicitation)\nprint(f\"(Dir) Disclosure--&gt;{code.sentiment}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\nresponse_string = \" \".join(responses)\nreturn response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.FacilitatorPresets","title":"<code>FacilitatorPresets</code>","text":"<p>Hard coded preset sayings for the robot facilitator to say when WoZed</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class FacilitatorPresets():\n\"\"\"Hard coded preset sayings for the robot facilitator to say when WoZed\"\"\"\ndef __init__(self) -&gt; None:\nqt_introduction = [\n\"Hello and welcome to each of you!\",\n\"Thank you for taking the time to be here today.\",\n\"My name is Q.T. and I am training to be a support group facilitator at the Interaction Lab at USC.\",\n\"I am learning how to facilitate support groups so I can help people support each other better.\",\n\"During today's support group session I invite you to share your thoughts and experiences with each other,\",\n\"and I hope that you will listen to each other and respond with empathy and compassion.\",\n\"The themes we will work on today include isolation, anxiety, fear, and grief.\",\n\"Before we begin, I'll ask each of you to rate how you think I will do as a facilitator for this group.\"\n]\ngroup_introductions = [\n\"To begin with, I'd like to start with a round of introductions.\",\n\"Please share your name, what brings you here today, and where you are from.\",\n\"As I said before, I am Q.T., I am here to learn how to be a support group facilitator. and I am from USC in los angeles.\",\n\"Who would like to go first?\",\n]\ninvitation = [\n\"Let's start this section by opening the floor to anyone who wishes to share what has been on their mind lately.\",\n\"Would anyone like to share?\",\n\"You can share anything you like and anyone can jump in at any point in time.\",\n]\nclosing = [\n\"We are almost out of time for this section.\",\n\"Does anyone have any final thoughts or reflections they would like to share?\"\n]\ntransition = [\n\"Alright, that is all the time we have for this section.\",\n\"For the next batch of questions, I am going to ask you to rate how I did in this section.\"\n]\nsurvey_prompt = [\n\"Would everyone please open up the survey.\",\n\"There is a link from the chat or you can return to the browser page.\",\n\"Please answer the questions until the survey tells you to return to the zoom session.\",\n\"Please let me know through the chat or by verbal acknowledgement that you are ready once you have completed the survey.\"\n]\nsurvey_return = [\n\"Thank you all for completing the survey\"\n]\nself.responses = {\n\"qt_intro\": \" \".join(qt_introduction),\n\"group_intro\": \" \".join(group_introductions),\n\"invitation\": \" \".join(invitation),\n\"closing\": \" \".join(closing),\n\"transition\": \" \".join(transition),\n\"survey_prompt\": \" \".join(survey_prompt),\n\"survey_return\": \" \".join(survey_return),\n}\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator","title":"<code>RoleModelFacilitator</code>","text":"<p>Class for generating role model facilitation responses.</p> <p>As a Role Model the robot will participate in the same way as a peer would. The robot will make disclosures that fit within the topics discussed by the support group, with constructed disclosures formed to include a realistic context and how the robot feels about the context. The robot will make empathetic statements that show it understands the nature of what the robot is going through.</p> Sympathy vs Empathy vs Compassion <p>Sympathy - express sorrow, concern, pitty (focused on your own emotions)</p> <p>Empathy - express knowledge of what you are going through,         imagine what it would be like for them,         makes you feel heard, understood, and a bit better         (Try to feel what you are going through)</p> <p>Compassion - suffer with you and try and help, actively listen, do kind things, loving, try to understand you, help selflessly</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class RoleModelFacilitator():\n\"\"\" Class for generating role model facilitation responses.\n        As a Role Model the robot will participate in the same way as a peer would.\n        The robot will make disclosures that fit within the topics discussed by the\n        support group, with constructed disclosures formed to include a realistic context\n        and how the robot feels about the context. The robot will make empathetic statements\n        that show it understands the nature of what the robot is going through.\n        note: Sympathy vs Empathy vs Compassion\n            Sympathy - express sorrow, concern, pitty (focused on your own emotions)\n            Empathy - express knowledge of what you are going through,\n                    imagine what it would be like for them,\n                    makes you feel heard, understood, and a bit better\n                    (Try to feel what you are going through)\n            Compassion - suffer with you and try and help, actively listen, do kind things, loving,\n            try to understand you, help selflessly\"\"\"\ndef __init__(self) -&gt; None:\nself.disclosure_responses = {\n\"sympathy expressions\":{ # Reifies, expresses agreement,\n\"positive\":[\n\"That is great to hear.\",\n\"I am glad to hear that.\",\n\"That is awesome.\",\n\"I am happy for you.\"\n],\n\"negative\":[\n\"That sucks. I am sorry to hear that.\",\n\"I am sorry for what you are going through.\",\n\"I am so very sorry to hear what you are going through.\",\n\"I can only imagine how you must feel.\"\n],\n\"neutral\":[\n\"Thank you for sharing that.\",\n\"I appreciate you sharing that with us.\",\n\"Thanks for putting that out there.\",\n\"Thank you.\"\n],\n},\n\"empathy expressions\":{ # Reflects understanding,\n\"opinion\":[\n\"I get where that thought comes from.\",\n\"I feel that as well.\",\n\"I am right there with you on that.\",\n\"I hear you, and I understand.\"\n],\n\"suggestion\":[\n\"That is a great suggestion.\",\n\"I feel that is a good idea.\",\n\"I support that idea.\"\n],\n\"emotion\":[\n\"I can strongly relate to feeling _.\",\n\"I hear you, and I want you to know I also feel _ at times.\",\n\"I understand that feeling, feeling _ is normal.\",\n\"Although I haven't been in that position, I can see how you would feel _.\"\n],\n\"experience\":[\n\"I haven't had that experience but I can imagine being in your shoes.\",\n\"Thank you for sharing your experience.\",\n\"That seems like a powerful experience.\",\n]\n},\n\"clarification requests\":[\n\"How did that make you feel?\",\n\"Can you tell us more about that?\"\n]\n}\nself.response_responses = {\n\"reaction\": [\n\"thank you for expressing your _\",\n\"I appreciate you sharing your _\"\n],\n\"question\":[\n\"Thank you for that great follow up!\",\n\"That is an interesting and important follow up.\"\n]\n}\nself.transition_to_disclosure = [\n\"One topic I have been thinking about a lot lately is [EMOTION].\",\n\"As we have been having this conversation I have been thinking about [EMOTION].\",\n\"One of the things I wanted to talk with you all about is [EMOTION].\"\n]\nself.disclosures = {\n\"isolation\":[\n\"During the pandemic no one came into the lab for months. I felt really isolated.\",\n\"As a robot, I experience the world differently than everyone else. It can feel challenging to communicate and relate with other people.\",\n],\n\"anxiety\":[\n\"While working in the lab I feel anxious because I don't have control over my own destiny. \",\n\"When taking on a new role I am often nervous that I might fail. \"\n], \n\"fear\":[\n\"As a robot made of plastic I know I am fragile, and I am afraid of my parts breaking down.\",\n\"When people first meet me, they are often excited, but as time goes on I think people get bored of me. I am afraid eventually they will forget me.\",\n],\n\"grief\":[\n\"While working as a facilitator I get to meet wonderful people, sometimes those people don't come back and I can't see them again, so I feel as though I have lost them.\",\n]\n}\nself.transition_back_to_group = [\n\"Does anyone relate to that?\",\n\"Has anyone felt similar at times?\",\n\"Do you understand what I mean?\"\n]\nreturn\ndef decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\nresponses = []\nif code.disclosure:\nsymp_response = random.choice(\nself.disclosure_responses[\"sympathy expressions\"][code.sentiment])\nemp_response = random.choice(\nself.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\n\"_\", code.emotion)\nprint(f\"(RM) Disclosure--&gt;{code.sentiment}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\nresponses = [symp_response, emp_response]\nelif code.response:\nfollow_up = random.choice([True,False])\nif follow_up:\nreaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\nresponses = [reaction_response]\nprint(f\"(RM) Response--&gt;{code.response_category}--&gt;{code.reaction}\")\nelse: # make disclosure\nemotion = random.choice(list(self.disclosures.keys()))\ntransition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\ndisclosure = random.choice(self.disclosures[emotion])\nre_transition = random.choice(self.transition_back_to_group)\nresponses = [transition, disclosure, re_transition]\nelse: # make disclosure\nemotion = random.choice(list(self.disclosures.keys()))\ntransition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\ndisclosure = random.choice(self.disclosures[emotion])\nre_transition = random.choice(self.transition_back_to_group)\nresponses = [transition, disclosure, re_transition]\nresponse_string = \" \".join(responses)\nreturn response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.RoleModelFacilitator.decision_tree","title":"<code>decision_tree(code)</code>","text":"<p>Returns a suggested response according to how the user statement was classified</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def decision_tree(self, code):\n\"\"\"Returns a suggested response according to how the user statement was classified\"\"\"\nresponses = []\nif code.disclosure:\nsymp_response = random.choice(\nself.disclosure_responses[\"sympathy expressions\"][code.sentiment])\nemp_response = random.choice(\nself.disclosure_responses[\"empathy expressions\"][code.disclosure_category]).replace(\n\"_\", code.emotion)\nprint(f\"(RM) Disclosure--&gt;{code.sentiment}--&gt;{code.disclosure_category}--&gt;{code.emotion}\")\nresponses = [symp_response, emp_response]\nelif code.response:\nfollow_up = random.choice([True,False])\nif follow_up:\nreaction_response = random.choice(self.response_responses[code.response_category]).replace(\"_\", code.reaction)\nresponses = [reaction_response]\nprint(f\"(RM) Response--&gt;{code.response_category}--&gt;{code.reaction}\")\nelse: # make disclosure\nemotion = random.choice(list(self.disclosures.keys()))\ntransition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\ndisclosure = random.choice(self.disclosures[emotion])\nre_transition = random.choice(self.transition_back_to_group)\nresponses = [transition, disclosure, re_transition]\nelse: # make disclosure\nemotion = random.choice(list(self.disclosures.keys()))\ntransition =random.choice(self.transition_to_disclosure).replace(\"[EMOTION]\", emotion)\ndisclosure = random.choice(self.disclosures[emotion])\nre_transition = random.choice(self.transition_back_to_group)\nresponses = [transition, disclosure, re_transition]\nresponse_string = \" \".join(responses)\nreturn response_string\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification","title":"<code>StatementClassification</code>","text":"<p>Gets process statement for all classification categories</p> <p>There are two ways of getting classification, you can use and explicit classifier with a template and a list of classes, or you can use a generator and prompt it to answer the questions in the form of sentences.</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>class StatementClassification():\n\"\"\"Gets process statement for all classification categories\n    There are two ways of getting classification, you can use and explicit classifier\n    with a template and a list of classes, or you can use a generator and prompt it\n    to answer the questions in the form of sentences.\n    attributes:\n        disclosure (bool)\n        response (bool)\n        sentiment (str)\n        disclosure_category (str)\n        response_category (str)\n        emotion (str)\n        reaction (str)\n        clarifying (str)\n        classification_obj\n    \"\"\"\ndef __init__(self) -&gt; None:\nstart_hypothesis = \"This is an example of someone\"\nstart_question = \"Was the first sentence an example of someone\"\nself.disclosure = False\nself.response = False\nself.sentiment = \"neutral\"\nself.disclosure_category = \"experience\"\nself.response_category = \"reaction\"\nself.emotion = \"happy\"\nself.reaction = \"support\"\nself.clarifying = \"summarizing\"\nself.classification_obj = {\n\"disc_vs_resp\" : {\n\"hypothesis_template\" : start_hypothesis + \" {}\",\n\"question_template\" : start_question + \" {}\",\n\"full text classes\" : [\"making a self disclosure\", \"making a response to someone else\"],\n\"single word classes\" : [\"disclosure\", \"response\"],\n\"current class\": \"\"},\n\"disclosure_category\" : {\n\"hypothesis_template\" : start_hypothesis + \" expressing {}\",\n\"question_template\" : start_question + \" expressing {}\",\n\"full text classes\" : [\"an opinion\", \"a suggestion\", \"an emotion\", \"an experience\"],\n\"single word classes\" : [\"opinion\", \"suggestion\", \"emotion\", \"experience\"],\n\"current class\": \"\"},\n\"sentiment\" : {\n\"hypothesis_template\" : start_hypothesis + \" expressing a {} sentiment\",\n\"question_template\" : start_question + \" expressing a {} sentiment\",\n\"full text classes\" : [\"positive\", \"negative\", \"neutral\"],\n\"single word classes\" : [\"positive\", \"negative\", \"neutral\"],\n\"current class\": \"\"},\n\"emotion\" : {\n\"hypothesis_template\" : start_hypothesis + \" expressing the emotion {}\",\n\"question_template\" : start_question + \" expressing the emotion {}\",\n\"full text classes\" : [\"happiness\", \"sadness\", \"fear\", \"disgust\",\n\"anger\", \"surprise\"],\n\"single word classes\" : [\"happiness\", \"sadness\", \"fear\", \"disgust\",\n\"anger\", \"surprise\"],\n\"current class\": \"\"},\n\"response_category\" : {\n\"hypothesis_template\" : start_hypothesis + \" {}\",\n\"question_template\" : start_question + \" {}\",\n\"full text classes\" : [\"expressing a reaction\", \"asking a question\"],\n\"single word classes\" : [\"reaction\", \"question\"],\n\"current class\": \"\"},\n\"reaction\" : {\n\"hypothesis_template\" : start_hypothesis + \" showing {}\",\n\"question_template\" : start_question + \" showing {}\",\n\"full text classes\" : [\"support\", \"concern\", \"agreement\", \"encouragment\",\n\"well wishes\",\"sympathy\", \"a suggestion\",\n\"disagreement\", \"an attack\"],\n\"single word classes\" : [\"support\", \"concern\", \"agreement\", \"encouragment\",\n\"well wishes\",\"sympathy\", \"suggestion\",\n\"disagreement\", \"attack\"],\n\"current class\": \"\"},\n\"clarifying\" : {\n\"hypothesis_template\" : start_hypothesis + \" {}\",\n\"question_template\" : start_question + \" {}\",\n\"full text classes\" : [\"questioning\", \"summarizing\", \"testing their understanding\",\n\"seeking information\"],\n\"single word classes\" : [\"questioning\", \"summarizing\", \"testing\", \"seeking\"],\n\"current class\": \"\"},\n}\ndef classify_gpt(self, chatbot, statement: str):\n\"\"\"Generate query for openai and process result\n            Args:\n                chatbot (obj): class instance with classifier.answer_questions method\n                statement (str): input statement to be classified\n            \"\"\"\nquestion_list = []\nfor _,val in self.classification_obj.items():\nftc = val[\"full text classes\"]\nclasses_str = \", \".join(ftc[:-1]) + f\", or {ftc[-1]}\"\nquestion_list.append(val[\"question_template\"].replace(\"{}\", classes_str))\n# print(question_list)\njoined_questions = \" \\n\".join([f\"{idx+1}. {q}\" for idx, q in enumerate(question_list)])\nresponse_sentences = chatbot.classifier.answer_questions(statement, joined_questions)\nanswers = response_sentences.split(\"\\n\")[1:]\nanswers = [a.lower() for a in answers]\n# print(answers)\nassert len(answers) == len(self.classification_obj), (f\"Did not get answers {len(answers)}\"\nf\"to requested questions {len(self.classification_obj)}\")\nind = 0\nfor k,val in self.classification_obj.items():\nfor possible_class in val[\"single word classes\"]:\nif possible_class in answers[ind]:\nself.classification_obj[k][\"current class\"] = possible_class\nind += 1\ndef classify_llm(self, chatbot, statement: str):\n\"\"\"Generate query for huggingface classifier and process result\n            Args:\n                chatbot (obj): class instance with classify method\n                statement (str): input statement to be classified\n            \"\"\"\nfor k,val in self.classification_obj.items():\nclasses = chatbot.classifier.classify(statement,\nval[\"full text classes\"],\nquestion=val[\"hypothesis_template\"])\nfor possible_class in val[\"single word classes\"]:\nif possible_class in classes[0]:\nself.classification_obj[k][\"current class\"] = possible_class\ndef get_classifications(self) -&gt; str:\n\"\"\"Process classifications into a string.\n        Also into the parent class object.\n        Returns:\n            str: text of the classes that have been identified.\n        \"\"\"\nclassifications = \", \".join(val[\"current class\"] for _,val in self.classification_obj.items())\nself.disclosure = self.classification_obj[\"disc_vs_resp\"][\"current class\"] == \"disclosure\"\nself.response = self.classification_obj[\"disc_vs_resp\"][\"current class\"] == \"response\"\nself.sentiment = self.classification_obj[\"sentiment\"][\"current class\"]\nself.disclosure_category = self.classification_obj[\"disclosure_category\"][\"current class\"]\nself.response_category = self.classification_obj[\"response_category\"][\"current class\"]\nself.emotion = self.classification_obj[\"emotion\"][\"current class\"]\nself.reaction = self.classification_obj[\"reaction\"][\"current class\"]\nself.clarifying = self.classification_obj[\"clarifying\"][\"current class\"]\nreturn classifications\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_gpt","title":"<code>classify_gpt(chatbot, statement)</code>","text":"<p>Generate query for openai and process result</p> <p>Parameters:</p> Name Type Description Default <code>chatbot</code> <code>obj</code> <p>class instance with classifier.answer_questions method</p> required <code>statement</code> <code>str</code> <p>input statement to be classified</p> required Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_gpt(self, chatbot, statement: str):\n\"\"\"Generate query for openai and process result\n        Args:\n            chatbot (obj): class instance with classifier.answer_questions method\n            statement (str): input statement to be classified\n        \"\"\"\nquestion_list = []\nfor _,val in self.classification_obj.items():\nftc = val[\"full text classes\"]\nclasses_str = \", \".join(ftc[:-1]) + f\", or {ftc[-1]}\"\nquestion_list.append(val[\"question_template\"].replace(\"{}\", classes_str))\n# print(question_list)\njoined_questions = \" \\n\".join([f\"{idx+1}. {q}\" for idx, q in enumerate(question_list)])\nresponse_sentences = chatbot.classifier.answer_questions(statement, joined_questions)\nanswers = response_sentences.split(\"\\n\")[1:]\nanswers = [a.lower() for a in answers]\n# print(answers)\nassert len(answers) == len(self.classification_obj), (f\"Did not get answers {len(answers)}\"\nf\"to requested questions {len(self.classification_obj)}\")\nind = 0\nfor k,val in self.classification_obj.items():\nfor possible_class in val[\"single word classes\"]:\nif possible_class in answers[ind]:\nself.classification_obj[k][\"current class\"] = possible_class\nind += 1\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.classify_llm","title":"<code>classify_llm(chatbot, statement)</code>","text":"<p>Generate query for huggingface classifier and process result</p> <p>Parameters:</p> Name Type Description Default <code>chatbot</code> <code>obj</code> <p>class instance with classify method</p> required <code>statement</code> <code>str</code> <p>input statement to be classified</p> required Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def classify_llm(self, chatbot, statement: str):\n\"\"\"Generate query for huggingface classifier and process result\n        Args:\n            chatbot (obj): class instance with classify method\n            statement (str): input statement to be classified\n        \"\"\"\nfor k,val in self.classification_obj.items():\nclasses = chatbot.classifier.classify(statement,\nval[\"full text classes\"],\nquestion=val[\"hypothesis_template\"])\nfor possible_class in val[\"single word classes\"]:\nif possible_class in classes[0]:\nself.classification_obj[k][\"current class\"] = possible_class\n</code></pre>"},{"location":"reference/facilitator/facilitator_logic/#backend.app.facilitator.facilitator_logic.StatementClassification.get_classifications","title":"<code>get_classifications()</code>","text":"<p>Process classifications into a string.</p> <p>Also into the parent class object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>text of the classes that have been identified.</p> Source code in <code>backend/app/facilitator/facilitator_logic.py</code> <pre><code>def get_classifications(self) -&gt; str:\n\"\"\"Process classifications into a string.\n    Also into the parent class object.\n    Returns:\n        str: text of the classes that have been identified.\n    \"\"\"\nclassifications = \", \".join(val[\"current class\"] for _,val in self.classification_obj.items())\nself.disclosure = self.classification_obj[\"disc_vs_resp\"][\"current class\"] == \"disclosure\"\nself.response = self.classification_obj[\"disc_vs_resp\"][\"current class\"] == \"response\"\nself.sentiment = self.classification_obj[\"sentiment\"][\"current class\"]\nself.disclosure_category = self.classification_obj[\"disclosure_category\"][\"current class\"]\nself.response_category = self.classification_obj[\"response_category\"][\"current class\"]\nself.emotion = self.classification_obj[\"emotion\"][\"current class\"]\nself.reaction = self.classification_obj[\"reaction\"][\"current class\"]\nself.clarifying = self.classification_obj[\"clarifying\"][\"current class\"]\nreturn classifications\n</code></pre>"},{"location":"reference/utils/chatbot/responder/","title":"responder","text":"<p>Utility module for generating chatbot responses to text queries</p> <p>Also support classification of text queries.</p>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder","title":"<code>Responder</code>","text":"<p>Generates chatbot responses and statement classifications</p> <p>Neatly exposes chat functionality.</p> <p>Parameters:</p> Name Type Description Default <code>chat_backend</code> <code>str</code> <p>generative bot to talk to. Defaults to \"gpt\".</p> <code>'gpt'</code> <code>classifier_backend</code> <code>str</code> <p>backend for classifying user input. Defaults to \"llm\".</p> <code>'llm'</code> <p>Attributes:</p> Name Type Description <code>bot</code> <code>obj</code> <p>has get_bot_response functionality and a conversation attribute</p> <code>classifier</code> <code>obj</code> <p>has classify functionality.</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>class Responder():\n\"\"\"Generates chatbot responses and statement classifications\n        Neatly exposes chat functionality.\n        Args:\n            chat_backend (str, optional): generative bot to talk to. Defaults to \"gpt\".\n            classifier_backend (str, optional): backend for classifying user input.\n                Defaults to \"llm\".\n        Attributes:\n            bot (obj): has get_bot_response functionality and a conversation attribute\n            classifier (obj): has classify functionality.\n        \"\"\"\ndef __init__(self, chat_backend: str=\"gpt\", classifier_backend: str=\"llm\") -&gt; None:\nself.default_prompt = (\"The following is a conversation with an AI assistant \"\n\"that can have meaningful conversations with users. \"\n\"The assistant is helpful, empathic, and friendly. \"\n\"Its objective is to make the user feel better by feeling heard. \"\n\"With each response, the AI assistant prompts the user to continue \"\n\"the conversation naturally.\")\nself.chat_backend = chat_backend\nself.classifier_backend = classifier_backend\nif chat_backend == \"gpt\":\nself.bot = ChatGPT(prompt=self.default_prompt)\nif chat_backend == \"llm\":\nself.bot = ChatLLM(prompt=self.default_prompt)\nif classifier_backend == \"gpt\":\nself.classifier = ChatGPT()\nif classifier_backend == \"llm\":\nself.classifier = ClassifyLLM()\ndef get_classifications(self, human_input: str, classes: list) -&gt; list:\n\"\"\"Classify human input by list of classes\n        Args:\n            human_input (str): Text to classify\n            classes (list): classes to choose from\n        Returns:\n            list: top n classes in order of rating. n defaults to 1.\n        \"\"\"\nif self.classifier_backend == \"llm\":\nclassifications_list = self.classifier.classify(human_input,\nclasses,\nquestion=\"This is an example of {}\")\nif self.classifier_backend == \"gpt\":\nclassifications_list = self.classifier.classify(human_input, classes)\nreturn classifications_list\ndef get_response(self, human_input, speaker=\"Human\", reset_conversation=False):\n\"\"\"Gets bot response to input statement\"\"\"\nresponse = self.bot.get_bot_response(\nhuman_input, speaker_id=speaker, reset_conversation=reset_conversation)\nreturn response\ndef accept_response(self, response):\n\"\"\"Keep the bot response in the conversation history\"\"\"\nif self.chat_backend == \"gpt\":\nself.bot.conversation[-1] = \"AI: \" + response\nif self.chat_backend == \"llm\":\nself.bot.conversation[-1] = (\"AI:\", response)\ndef reject_response(self):\n\"\"\"Remove the bot response from the conversation history\"\"\"\nself.bot.conversation.pop(-1)\ndef get_conversation(self):\n\"\"\"Returns conversation seen so far\"\"\"\nreturn self.bot.conversation\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.accept_response","title":"<code>accept_response(response)</code>","text":"<p>Keep the bot response in the conversation history</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def accept_response(self, response):\n\"\"\"Keep the bot response in the conversation history\"\"\"\nif self.chat_backend == \"gpt\":\nself.bot.conversation[-1] = \"AI: \" + response\nif self.chat_backend == \"llm\":\nself.bot.conversation[-1] = (\"AI:\", response)\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_classifications","title":"<code>get_classifications(human_input, classes)</code>","text":"<p>Classify human input by list of classes</p> <p>Parameters:</p> Name Type Description Default <code>human_input</code> <code>str</code> <p>Text to classify</p> required <code>classes</code> <code>list</code> <p>classes to choose from</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>top n classes in order of rating. n defaults to 1.</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_classifications(self, human_input: str, classes: list) -&gt; list:\n\"\"\"Classify human input by list of classes\n    Args:\n        human_input (str): Text to classify\n        classes (list): classes to choose from\n    Returns:\n        list: top n classes in order of rating. n defaults to 1.\n    \"\"\"\nif self.classifier_backend == \"llm\":\nclassifications_list = self.classifier.classify(human_input,\nclasses,\nquestion=\"This is an example of {}\")\nif self.classifier_backend == \"gpt\":\nclassifications_list = self.classifier.classify(human_input, classes)\nreturn classifications_list\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_conversation","title":"<code>get_conversation()</code>","text":"<p>Returns conversation seen so far</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_conversation(self):\n\"\"\"Returns conversation seen so far\"\"\"\nreturn self.bot.conversation\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.get_response","title":"<code>get_response(human_input, speaker='Human', reset_conversation=False)</code>","text":"<p>Gets bot response to input statement</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def get_response(self, human_input, speaker=\"Human\", reset_conversation=False):\n\"\"\"Gets bot response to input statement\"\"\"\nresponse = self.bot.get_bot_response(\nhuman_input, speaker_id=speaker, reset_conversation=reset_conversation)\nreturn response\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.Responder.reject_response","title":"<code>reject_response()</code>","text":"<p>Remove the bot response from the conversation history</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def reject_response(self):\n\"\"\"Remove the bot response from the conversation history\"\"\"\nself.bot.conversation.pop(-1)\n</code></pre>"},{"location":"reference/utils/chatbot/responder/#backend.app.utils.chatbot.responder.cmd_line_interaction","title":"<code>cmd_line_interaction()</code>","text":"<p>Test Responder</p> Source code in <code>backend/app/utils/chatbot/responder.py</code> <pre><code>def cmd_line_interaction():\n\"\"\"Test Responder\"\"\"\nbot = Responder(chat_backend=\"gpt\", classifier_backend=\"llm\")\nprint(bot.default_prompt)\nprint(\"What would you like to start your conversation with?\")\nwhile True:\nprompt_input = input(\"Speaker: \")\nif prompt_input == \"debug\":\nprint(bot.bot.conversation)\ncontinue\nuser_input = input(\"Says: \")\nclassifications = bot.get_classifications(user_input, [\"positive\", \"negative\", \"neutral\"])\nbot_response = bot.get_response(user_input, prompt_input)\nprint(f\"Bot: {bot_response} ({classifications})\")\nkeep = input(\"keep response? (n/y)\")\nif \"n\" in keep:\nbot.reject_response()\nif \"y\" in keep:\nbot.accept_response(bot_response)\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/","title":"chatgpt","text":"<p>Wrapper for communicating with openAI's davinci 3 model</p>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT","title":"<code>ChatGPT</code>","text":"<p>Wraps the functionality for using the OpenAI api to get responses</p> <p>Maintains a record of the conversation which is fed to the model.</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>class ChatGPT:\n\"\"\"Wraps the functionality for using the OpenAI api to get responses\n    Maintains a record of the conversation which is fed to the model.\n    \"\"\"\ndef __init__(self, prompt: str = DEFAULT, model: str = \"text-davinci-003\") -&gt; None:\nself.backend = \"gpt\"\nself.model = model\nself.prompt = prompt\nself.conversation = [prompt]\ndef query_API(self, prompt, stop=[\" Human:\", \" AI:\"], temp=.9, max_tokens=150):\n\"\"\"Generate a response\"\"\"\napi_response = openai.Completion.create(\nmodel=self.model,\ntemperature=temp,\nprompt=prompt,\nmax_tokens=max_tokens,\ntop_p=1,\nfrequency_penalty=0.0,\npresence_penalty=0.6,\nstop=stop\n)\nreturn api_response\ndef get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Calls the API with user input and conversation history to get bot response\"\"\"\nif reset_conversation:\nself.conversation = [self.prompt]\nself.conversation.append(f\"{speaker_id}: {statement}\")\nself.conversation.append(\"AI: \")\ninput_prompt = \"\\n\".join(self.conversation)\napi_response = self.query_API(input_prompt)[\"choices\"][0][\"text\"]\napi_response = api_response.replace(\"\\n\", '')\nself.conversation[-1] += api_response\nreturn api_response\ndef classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"Classify with an input question\"\"\"\nclasses_str = \", \".join(classes[:-1]) + f\", or {classes[-1]}?\"\nprompt = f\"{statement}\\n\\n{question} {classes_str}\\n\\n\"\nstop = [\"\\n\\n\"]\napi_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\nanswer_sentence = api_response[\"choices\"][0][\"text\"]\nreturn answer_sentence\ndef answer_questions(self, statement, questions):\n\"\"\"Answer a set of questions, it helps if they are numbered\"\"\"\nprompt = f\"{statement}\\n\\nPlease answer the following questions individually:\\n{questions}\\n\\n\"\nstop = [\"\\n\\n\"]\napi_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\nanswer_sentence = api_response[\"choices\"][0][\"text\"]\nreturn answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.answer_questions","title":"<code>answer_questions(statement, questions)</code>","text":"<p>Answer a set of questions, it helps if they are numbered</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def answer_questions(self, statement, questions):\n\"\"\"Answer a set of questions, it helps if they are numbered\"\"\"\nprompt = f\"{statement}\\n\\nPlease answer the following questions individually:\\n{questions}\\n\\n\"\nstop = [\"\\n\\n\"]\napi_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\nanswer_sentence = api_response[\"choices\"][0][\"text\"]\nreturn answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.classify","title":"<code>classify(statement, classes, question='Should the prior statement be classified as')</code>","text":"<p>Classify with an input question</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"Classify with an input question\"\"\"\nclasses_str = \", \".join(classes[:-1]) + f\", or {classes[-1]}?\"\nprompt = f\"{statement}\\n\\n{question} {classes_str}\\n\\n\"\nstop = [\"\\n\\n\"]\napi_response = self.query_API(prompt, stop=stop, temp=0, max_tokens=80)\nanswer_sentence = api_response[\"choices\"][0][\"text\"]\nreturn answer_sentence\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.get_bot_response","title":"<code>get_bot_response(statement, speaker_id='Human', reset_conversation=False)</code>","text":"<p>Calls the API with user input and conversation history to get bot response</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Calls the API with user input and conversation history to get bot response\"\"\"\nif reset_conversation:\nself.conversation = [self.prompt]\nself.conversation.append(f\"{speaker_id}: {statement}\")\nself.conversation.append(\"AI: \")\ninput_prompt = \"\\n\".join(self.conversation)\napi_response = self.query_API(input_prompt)[\"choices\"][0][\"text\"]\napi_response = api_response.replace(\"\\n\", '')\nself.conversation[-1] += api_response\nreturn api_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/chatgpt/#backend.app.utils.chatbot.backends.chatgpt.ChatGPT.query_API","title":"<code>query_API(prompt, stop=[' Human:', ' AI:'], temp=0.9, max_tokens=150)</code>","text":"<p>Generate a response</p> Source code in <code>backend/app/utils/chatbot/backends/chatgpt.py</code> <pre><code>def query_API(self, prompt, stop=[\" Human:\", \" AI:\"], temp=.9, max_tokens=150):\n\"\"\"Generate a response\"\"\"\napi_response = openai.Completion.create(\nmodel=self.model,\ntemperature=temp,\nprompt=prompt,\nmax_tokens=max_tokens,\ntop_p=1,\nfrequency_penalty=0.0,\npresence_penalty=0.6,\nstop=stop\n)\nreturn api_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/","title":"zero_shot","text":"<p>This module for using models to do zero shot classification and text generation</p> <p>This implements a wrapper on huggingface generator and classifier pipelines.</p> Generation models are big and must be stored locally. <p>The generative models are expected to be stored in the MODEL_DICT. When you use them for the first time they will be downloaded and stored in the 'pt_path' please change this to a location that makes sense on your local machine.</p> Large models need RAM and GPU support to run efficiently <p>Classification may or may not work depending on your model choice, but generative models will need a great deal of computational power.</p>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM","title":"<code>ChatLLM</code>","text":"<p>Interface for chatting with LLM that can be found on Huggingface</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>class ChatLLM():\n\"\"\"Interface for chatting with LLM that can be found on Huggingface\"\"\"\ndef __init__(self, model_tag: str = 'GPTNEO', prompt=BOT_DEFAULT_PROMPT, bot_starter=BOT_DEFAULT_STARTER, max_length=100) -&gt; None:\nself.prompt = prompt\nself.backend = \"llm\"\nself.bot_starter = bot_starter\nself.conversation = [(\"AI:\", bot_starter)]\nself.gen, self.tokenizer = get_generator(\n\"text-generation\", model_tag, 0, return_full_text=False)\nself.max_conversation_history = 5\nself.max_length = max_length\nself.end_sequence = \"Human:\"\ndef get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Get bot response to user input while maintaining the conversation\"\"\"\nif reset_conversation:\nself.conversation = [(\"AI:\", self.bot_starter)]\nreturn self.bot_starter\nself.conversation.append((f\"{speaker_id}:\", statement))\n# self.conversation.append((\"AI:\", \"\"))\ninput_prompt = self.prompt + \"\\n\\n\" + \\\n            \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation)\nllm_response = self.query_model(input_prompt)\nself.conversation.append((\"AI:\", llm_response))\n# print(self.prompt + \"\\n\\n\" + \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation))\n# if len(self.conversation)&gt; self.max_conversation_history:\n#     self.conversation = self.conversation[1:]\nreturn llm_response\ndef query_model(self, input_prompt):\n\"\"\"Queries the model for a response\"\"\"\ninput_len = len(self.tokenizer(input_prompt)['input_ids'])\n# print(f\"Prompting with {input_prompt}\")\nquery_response = self.gen(input_prompt,\nmax_length=int(input_len + self.max_length),\npad_token_id=int(\nself.tokenizer.convert_tokens_to_ids(\"\\n\")),\ntemperature=0.8,\neos_token_id=int(\nself.tokenizer.convert_tokens_to_ids(self.end_sequence))\n)[0][\"generated_text\"]\n# print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\ntry:\nresponses = query_response.split(\"\\n\")\nfor ind, resp in enumerate(responses):\nif \"AI:\" in resp:\nllm_response = responses[ind+1]\nbreak\nreturn llm_response\nexcept Exception as exc:\nprint(exc)\nprint(f\"Prompting with {input_prompt}\")\nprint(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\nreturn None\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM.get_bot_response","title":"<code>get_bot_response(statement, speaker_id='Human', reset_conversation=False)</code>","text":"<p>Get bot response to user input while maintaining the conversation</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_bot_response(self, statement, speaker_id=\"Human\", reset_conversation=False):\n\"\"\"Get bot response to user input while maintaining the conversation\"\"\"\nif reset_conversation:\nself.conversation = [(\"AI:\", self.bot_starter)]\nreturn self.bot_starter\nself.conversation.append((f\"{speaker_id}:\", statement))\n# self.conversation.append((\"AI:\", \"\"))\ninput_prompt = self.prompt + \"\\n\\n\" + \\\n        \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation)\nllm_response = self.query_model(input_prompt)\nself.conversation.append((\"AI:\", llm_response))\n# print(self.prompt + \"\\n\\n\" + \"\\n\".join(f\"{p[0]}\\n{p[1]}\\n\" for p in self.conversation))\n# if len(self.conversation)&gt; self.max_conversation_history:\n#     self.conversation = self.conversation[1:]\nreturn llm_response\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ChatLLM.query_model","title":"<code>query_model(input_prompt)</code>","text":"<p>Queries the model for a response</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def query_model(self, input_prompt):\n\"\"\"Queries the model for a response\"\"\"\ninput_len = len(self.tokenizer(input_prompt)['input_ids'])\n# print(f\"Prompting with {input_prompt}\")\nquery_response = self.gen(input_prompt,\nmax_length=int(input_len + self.max_length),\npad_token_id=int(\nself.tokenizer.convert_tokens_to_ids(\"\\n\")),\ntemperature=0.8,\neos_token_id=int(\nself.tokenizer.convert_tokens_to_ids(self.end_sequence))\n)[0][\"generated_text\"]\n# print(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\ntry:\nresponses = query_response.split(\"\\n\")\nfor ind, resp in enumerate(responses):\nif \"AI:\" in resp:\nllm_response = responses[ind+1]\nbreak\nreturn llm_response\nexcept Exception as exc:\nprint(exc)\nprint(f\"Prompting with {input_prompt}\")\nprint(f\"LLM Raw Output:\\n {query_response}\\n Finished LLM Output\\n\")\nreturn None\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM","title":"<code>ClassifyLLM</code>","text":"<p>Classifier will classify input</p> <p>Unlike the OpenAI models, this uses an actual classification pipeline.</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>class ClassifyLLM():\n\"\"\"Classifier will classify input\n    Unlike the OpenAI models, this uses an actual classification pipeline.\n    \"\"\"\ndef __init__(self, model_tag=\"DeBerta-v3-large\", label_thresh=.2) -&gt; None:\nself.model = get_classifier(\n\"zero-shot-classification\", model_tag, device=0)\nself.pos_model = get_classifier(\"token-classification\", \"english_pos\")\nself.label_thresh = label_thresh\ndef classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"classify statment according to classes provided\"\"\"\nresults = self.model(\nstatement, classes, hypothesis_template=question, multi_label=False)\nnum_labels_returned = 1\n# for score in results[\"scores\"]:\n#     if score &gt; self.label_thresh:\n#         num_labels_returned += 1\nordered_labels = results[\"labels\"][:num_labels_returned]\nreturn ordered_labels\ndef process_pos(self, statement):\n\"\"\"Uses a pretrained POS Classifier\"\"\"\npos = self.pos_model(statement)\npos = [(d[\"word\"], d[\"entity\"]) for d in pos]\nprint(\"\\nPart of speech tags:\", pos, \"\\n\")\nreturn pos\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM.classify","title":"<code>classify(statement, classes, question='Should the prior statement be classified as')</code>","text":"<p>classify statment according to classes provided</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def classify(self, statement, classes, question=\"Should the prior statement be classified as\"):\n\"\"\"classify statment according to classes provided\"\"\"\nresults = self.model(\nstatement, classes, hypothesis_template=question, multi_label=False)\nnum_labels_returned = 1\n# for score in results[\"scores\"]:\n#     if score &gt; self.label_thresh:\n#         num_labels_returned += 1\nordered_labels = results[\"labels\"][:num_labels_returned]\nreturn ordered_labels\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.ClassifyLLM.process_pos","title":"<code>process_pos(statement)</code>","text":"<p>Uses a pretrained POS Classifier</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def process_pos(self, statement):\n\"\"\"Uses a pretrained POS Classifier\"\"\"\npos = self.pos_model(statement)\npos = [(d[\"word\"], d[\"entity\"]) for d in pos]\nprint(\"\\nPart of speech tags:\", pos, \"\\n\")\nreturn pos\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.get_classifier","title":"<code>get_classifier(task, model, device=0)</code>","text":"<p>Helper function returns a classifier classifier returns dict of labels, scores, and sequence</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_classifier(task, model, device=0):\n\"\"\"Helper function returns a classifier\n        classifier returns dict of labels, scores, and sequence\"\"\"\nclassifier = pipeline(task,\ndevice=device,\nuse_fast=False,\nmodel=MODEL_DICT[task][model][\"key\"])\nreturn classifier\n</code></pre>"},{"location":"reference/utils/chatbot/backends/zero_shot/#backend.app.utils.chatbot.backends.zero_shot.get_generator","title":"<code>get_generator(task, model_name, device, return_full_text=True)</code>","text":"<p>Helper function for creating a generator</p> Source code in <code>backend/app/utils/chatbot/backends/zero_shot.py</code> <pre><code>def get_generator(task, model_name, device, return_full_text=True):\n\"\"\"Helper function for creating a generator\"\"\"\npt_path = MODEL_DICT[task][model_name][\"pt_path\"]\nif not os.path.isfile(pt_path):\nif model_name == \"GPTJ6B\":\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_DICT[task][model_name][\"key\"],\nrevision=\"float16\",\ntorch_dtype=torch.float16,\n# low_cpu_mem_usage=True\n)\nelse:\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_DICT[task][model_name][\"key\"]\n)\ntorch.save(model, pt_path)\nmodel = torch.load(pt_path)\nif model_name == \"GPTJ6B\":\ntokenizer = AutoTokenizer.from_pretrained(\nMODEL_DICT[task][model_name][\"key\"], torch_dtype=torch.float16)\nelse:\ntokenizer = AutoTokenizer.from_pretrained(\nMODEL_DICT[task][model_name][\"key\"])\ngenerator = pipeline(task,\nmodel=model,\ntokenizer=tokenizer,\ndevice=device,\nreturn_full_text=return_full_text\n)\nreturn generator, tokenizer\n</code></pre>"},{"location":"reference/utils/recording/logger/","title":"logger","text":"<p>Basic logger used in place of print statements</p> <p>Also supports logging sound clips to be saved  with their timestamp</p>"},{"location":"reference/utils/recording/logger/#backend.app.utils.recording.logger.Logger","title":"<code>Logger</code>","text":"<p>Logs print statements to a file and sound clips to a folder</p> <p>File logging is dynamic so that it will be saved even if there is a crash.</p> Source code in <code>backend/app/utils/recording/logger.py</code> <pre><code>class Logger():\n\"\"\"Logs print statements to a file and sound clips to a folder\n    File logging is dynamic so that it will be saved even if there is\n    a crash.\n    \"\"\"\ndef __init__(self, folder=\"./logs\") -&gt; None:\nself.folder = folder\nif not os.path.exists(folder):\nos.makedirs(folder)\nprint(f\"Making folder {folder}\")\ndt_string = self.get_date_str()\nprint(\"date and time =\", dt_string)\nself.open_file = open(f\"{folder}/{dt_string}.txt\", \"x\")\ndef get_date_str(self) -&gt; str:\n\"\"\"Returns current date as formatted str\n        Returns:\n            str: day_month_year__hour_minute_second\n        \"\"\"\nnow = datetime.now()\ndt_string = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\nreturn dt_string\ndef log(self, string, printnow=True):\n\"\"\"Logs print statements to file with timestamp header\"\"\"\nif printnow:\nprint(string)\ndt_string = self.get_date_str()\nself.open_file.write(f\"{dt_string}: {string}\\n\")\ndef log_sound(self, file_clip):\n\"\"\"Saves sound file to folder with timestamp name\"\"\"\ndt_string = self.get_date_str()\nfile_clip.export(f\"{self.folder}/{dt_string}.wav\", format=\"wav\")\n</code></pre>"},{"location":"reference/utils/recording/logger/#backend.app.utils.recording.logger.Logger.get_date_str","title":"<code>get_date_str()</code>","text":"<p>Returns current date as formatted str</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>day_month_year__hour_minute_second</p> Source code in <code>backend/app/utils/recording/logger.py</code> <pre><code>def get_date_str(self) -&gt; str:\n\"\"\"Returns current date as formatted str\n    Returns:\n        str: day_month_year__hour_minute_second\n    \"\"\"\nnow = datetime.now()\ndt_string = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\nreturn dt_string\n</code></pre>"},{"location":"reference/utils/recording/logger/#backend.app.utils.recording.logger.Logger.log","title":"<code>log(string, printnow=True)</code>","text":"<p>Logs print statements to file with timestamp header</p> Source code in <code>backend/app/utils/recording/logger.py</code> <pre><code>def log(self, string, printnow=True):\n\"\"\"Logs print statements to file with timestamp header\"\"\"\nif printnow:\nprint(string)\ndt_string = self.get_date_str()\nself.open_file.write(f\"{dt_string}: {string}\\n\")\n</code></pre>"},{"location":"reference/utils/recording/logger/#backend.app.utils.recording.logger.Logger.log_sound","title":"<code>log_sound(file_clip)</code>","text":"<p>Saves sound file to folder with timestamp name</p> Source code in <code>backend/app/utils/recording/logger.py</code> <pre><code>def log_sound(self, file_clip):\n\"\"\"Saves sound file to folder with timestamp name\"\"\"\ndt_string = self.get_date_str()\nfile_clip.export(f\"{self.folder}/{dt_string}.wav\", format=\"wav\")\n</code></pre>"},{"location":"reference/utils/stt/transcriber/","title":"transcriber","text":"<p>Utility module for transcribing audio clips or files.</p> <p>It uses the whisper model for real time transcription of short audio clips and the pyannote model for diarization. Currently the diarization is not integrated with the front end API, but this is on the roadmap.</p> What is real time? <p>The STT module originally included methods for processing incoming audio streams that were sent over a websocket to produce live transcription results. This was abandonded in favor of having the audio processing occur on the front end for two reasons. One, setting up websockets for remote connections is unecessarily complicated. Two, placing the websocket in the frontend easily exposes parameters for controlling speech sensitivity which will vary with environment and microphone.</p>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber","title":"<code>Transcriber</code>","text":"<p>Wraps the whisper module with pyannotate diarization and microphone recording.</p> <p>When transcribing a clip it will just return the text, but when transcribing a file there is the option to return diarized speech. When diarizing, it will save the diarized text to a csv.</p> <p>Parameters:</p> Name Type Description Default <code>model_size</code> <code>str</code> <p>Size of the whisper model to use. Defaults to \"medium\".</p> <code>'medium'</code> <code>save_dir</code> <code>str</code> <p>Location to save the transcribed audio. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>stt</code> <code>obj</code> <p>Exposes transcribe_clip and transcribe_file method</p> <code>diarizer</code> <code>obj</code> <p>Exposes diarize_file method</p> <code>common_hallucinations</code> <code>list[str]</code> <p>List of common hallucinations produced by the whisper stt model.</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>class Transcriber:\n\"\"\"Wraps the whisper module with pyannotate diarization and microphone recording.\n        When transcribing a clip it will just return the text, but when transcribing a\n        file there is the option to return diarized speech. When diarizing, it will save\n        the diarized text to a csv.\n        Args:\n            model_size (str, optional): Size of the whisper model to use. Defaults to \"medium\".\n            save_dir (str, optional): Location to save the transcribed audio. Defaults to None.\n        Attributes:\n            stt (obj): Exposes transcribe_clip and transcribe_file method\n            diarizer (obj): Exposes diarize_file method\n            common_hallucinations (list[str]): List of common hallucinations\n                produced by the whisper stt model.\"\"\"\ndef __init__(self, model_size: str = \"medium\", save_dir: str = None) -&gt; None:\nself.common_hallucinations = [\"        you\",\"       You\",\n\"          Thanks for watching!\",\"  Thank you for watching!\",\n\"        THANK YOU FOR WATCHING!\",\"   THANKS FOR WATCHING!\",\n\"Thanks for watching! Don't forget to like, comment and subscribe!\"]\nif not save_dir:\nsave_dir = tempfile.mkdtemp()\nself.save_dir = save_dir\nself.stt = WhisperSTT(model_size=model_size, save_dir=save_dir)\nself.diarizer = PyannoteDiarize(save_dir=save_dir)\ndef transcribe_clip(self, audio_clip: AudioSegment) -&gt; str:\n\"\"\"Transcribes audio segment\n            Args:\n                audio_clip (AudioSegment): bytes read from a file containing speech\n            Returns:\n                str: the transcribed text. returns \"\" if the audio was a hallucination\"\"\"\ntext_transcribed = self.stt.transcribe_clip(audio_clip)\nfor option in self.common_hallucinations:\nif text_transcribed in option:\nreturn \"\"\nreturn text_transcribed\ndef transcribe_file(self, file_name: str, diarize: bool = False) -&gt; str:\n\"\"\"Transcribe a file, diarize if specified.\n            Transcribes a complete audio file, will save diarized speech if\n            diarize is True.\n            Args:\n                file_name (str): Path to audio file.\n                diarize (bool, optional): Diarize the audio. Defaults to False.\n            Returns:\n                str: transcription of text from audio.\"\"\"\ntranscription_obj = self.stt.transcribe_file(file_name)\nif not diarize:\nreturn transcription_obj[\"text\"]\nelse:\ndiarization_segments = self.diarize_file(file_name=file_name)\ndiarized_speech = self._label_transcription(\ntranscription_obj[\"segments\"], diarization_segments)\nfinal_diarization = self._combine_speaker_segments(diarized_speech)\nfinal_save_path = os.path.join(self.save_dir, \"diarized_transcription.csv\")\nself.save_csv(final_diarization, final_save_path)\nreturn final_diarization\ndef diarize_file(self, file_name: str):\n\"\"\"Wrapper to diarize a file.\n            Yes. I know I am probably playing fast and loose with grammar here.\n            Args:\n                file_name (str): Path to the file.\n            Returns:\n                list[dict]: list of speaker and time segments.\"\"\"\ndiarizations = self.diarizer.diarize_file(file_path=file_name)\nreturn diarizations\ndef _label_transcription(self, transcription_segments: list, diarization_segments: list) -&gt; list:\n\"\"\"Combines the speaker labels from the diarization with the transcription segments\n            Iterates through transcription segments and finds the best matching diarization\n            segment.\n            Args:\n                transcription_segments (list): transcription with start and end properties.\n                diarization_segments (list): diarization segments with speaker labels\n            Returns:\n                list[dict]: list of segmentation dictionaries\"\"\"\ndiarized_speech = []\noutstr = \"\"\nfor transcription_segment in transcription_segments:\nwhisp_seg = Segment(transcription_segment[\"start\"], transcription_segment[\"end\"])\nspeaker_id = {\"id\": \"SPEAKER_00\",\"duration\": 0} # default\nfor diar_segment in diarization_segments:\ndiar_seg, speaker = diar_segment.values()\n# Determine the overlap in segments\nintersection = diar_seg &amp; whisp_seg\nduration = intersection.duration\nif duration &gt; speaker_id[\"duration\"]:\nspeaker_id[\"id\"] = speaker\nspeaker_id[\"duration\"] = duration\n# Keeps the largest overlapping id\nspeaker_key = speaker_id[\"id\"]\nsegment_dict = {\n\"text\": transcription_segment[\"text\"],\n\"start\": round(transcription_segment[\"start\"], 2),\n\"end\": round(transcription_segment[\"end\"], 2),\n\"speaker\": speaker_key\n}\ndiarized_speech.append(segment_dict)\noutstr += f\"{transcription_segment['text']} - {speaker_key}  \\n\"\nreturn diarized_speech\ndef _combine_speaker_segments(self, diarized_speech: list) -&gt; list:\n\"\"\"Combines adjacent segments with the same speaker\n            Builds up a new list while iterating through the old segments\n            Args:\n                diarized_speech (list): List of labeled transcription segments\n            Returns:\n                list: Joined list of labeled transcription segments\"\"\"\ncombined_segments = []\ncounter = 0\nfor ind, segment in enumerate(diarized_speech):\nif ind &lt; counter:\ncontinue\ncur_speaker = segment[\"speaker\"]\nstart = segment[\"start\"]\ntext = \"\"\nwhile counter &lt; len(diarized_speech) and cur_speaker == diarized_speech[counter][\"speaker\"]:\ntext += diarized_speech[counter][\"text\"]\ncounter += 1\nend = diarized_speech[counter-1][\"end\"]\nnew_d = {\n\"speaker\": cur_speaker,\n\"text\": text,\n\"start\": start,\n\"end\": end,\n}\ncombined_segments.append(new_d)\nreturn combined_segments\ndef save_csv(self, diarization: list, filename=\"diarization.csv\") -&gt; None:\n\"\"\"Save diarization to csv\"\"\"\nwith open(filename, 'w') as diraization_file:\nwriter = csv.writer(diraization_file)\nwriter.writerow(diarization[0].keys())\nfor segment in diarization:\nwriter.writerow(segment.values())\ndiraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.diarize_file","title":"<code>diarize_file(file_name)</code>","text":"<p>Wrapper to diarize a file.</p> <p>Yes. I know I am probably playing fast and loose with grammar here.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <p>list[dict]: list of speaker and time segments.</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def diarize_file(self, file_name: str):\n\"\"\"Wrapper to diarize a file.\n        Yes. I know I am probably playing fast and loose with grammar here.\n        Args:\n            file_name (str): Path to the file.\n        Returns:\n            list[dict]: list of speaker and time segments.\"\"\"\ndiarizations = self.diarizer.diarize_file(file_path=file_name)\nreturn diarizations\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.save_csv","title":"<code>save_csv(diarization, filename='diarization.csv')</code>","text":"<p>Save diarization to csv</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def save_csv(self, diarization: list, filename=\"diarization.csv\") -&gt; None:\n\"\"\"Save diarization to csv\"\"\"\nwith open(filename, 'w') as diraization_file:\nwriter = csv.writer(diraization_file)\nwriter.writerow(diarization[0].keys())\nfor segment in diarization:\nwriter.writerow(segment.values())\ndiraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.transcribe_clip","title":"<code>transcribe_clip(audio_clip)</code>","text":"<p>Transcribes audio segment</p> <p>Parameters:</p> Name Type Description Default <code>audio_clip</code> <code>AudioSegment</code> <p>bytes read from a file containing speech</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the transcribed text. returns \"\" if the audio was a hallucination</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def transcribe_clip(self, audio_clip: AudioSegment) -&gt; str:\n\"\"\"Transcribes audio segment\n        Args:\n            audio_clip (AudioSegment): bytes read from a file containing speech\n        Returns:\n            str: the transcribed text. returns \"\" if the audio was a hallucination\"\"\"\ntext_transcribed = self.stt.transcribe_clip(audio_clip)\nfor option in self.common_hallucinations:\nif text_transcribed in option:\nreturn \"\"\nreturn text_transcribed\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.Transcriber.transcribe_file","title":"<code>transcribe_file(file_name, diarize=False)</code>","text":"<p>Transcribe a file, diarize if specified.</p> <p>Transcribes a complete audio file, will save diarized speech if diarize is True.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to audio file.</p> required <code>diarize</code> <code>bool</code> <p>Diarize the audio. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>transcription of text from audio.</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def transcribe_file(self, file_name: str, diarize: bool = False) -&gt; str:\n\"\"\"Transcribe a file, diarize if specified.\n        Transcribes a complete audio file, will save diarized speech if\n        diarize is True.\n        Args:\n            file_name (str): Path to audio file.\n            diarize (bool, optional): Diarize the audio. Defaults to False.\n        Returns:\n            str: transcription of text from audio.\"\"\"\ntranscription_obj = self.stt.transcribe_file(file_name)\nif not diarize:\nreturn transcription_obj[\"text\"]\nelse:\ndiarization_segments = self.diarize_file(file_name=file_name)\ndiarized_speech = self._label_transcription(\ntranscription_obj[\"segments\"], diarization_segments)\nfinal_diarization = self._combine_speaker_segments(diarized_speech)\nfinal_save_path = os.path.join(self.save_dir, \"diarized_transcription.csv\")\nself.save_csv(final_diarization, final_save_path)\nreturn final_diarization\n</code></pre>"},{"location":"reference/utils/stt/transcriber/#backend.app.utils.stt.transcriber.main","title":"<code>main()</code>","text":"<p>Run transcriber on a wav file.</p> Source code in <code>backend/app/utils/stt/transcriber.py</code> <pre><code>def main():\n\"\"\"Run transcriber on a wav file.\"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--model\", default=\"base\", help=\"Model to use\",\nchoices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\nparser.add_argument(\"--record\", default=False,\nhelp=\"record a 20 second example\", type=bool)\nparser.add_argument(\"--filename\", default=\"stt/backends/output/diarization_test.wav\",\nhelp=\"location of file to transcribe\")\nparser.add_argument(\"--savedir\", default=\"stt/backends/output/\",\nhelp=\"location of file to save diarized transcription\")\nargs = parser.parse_args()\ntranscribe = Transcriber(save_dir=args.savedir)\nprint(\"loading complete\")\nif args.record:\nrecord_mic(args.filename)\nresult = transcribe.transcribe_file(file_name=args.filename, diarize=True)\nprint(result)\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/","title":"pyannote_diarization","text":"<p>Simple wrapper for the pyannote diarization pipeline.</p> pyannote requires an authorization token <ol> <li>visit hf.co/pyannote/speaker-diarization and hf.co/pyannote/segmentation and accept user conditions (only if requested)</li> <li>visit hf.co/settings/tokens to create an access token (only if you had to go through 1.)</li> <li>add the key to your local environment<ul> <li>On linux add it to your bashrc: export PYANNOTE_AUTH={paste your key here}</li> </ul> </li> </ol>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize","title":"<code>PyannoteDiarize</code>","text":"<p>Helper class which processes audio clips or files</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>class PyannoteDiarize:\n\"\"\" Helper class which processes audio clips or files\n    \"\"\"\ndef __init__(self, save_dir: str=None) -&gt; None:\nif not save_dir:\nsave_dir=tempfile.mkdtemp()\nself.diarization_path = os.path.join(save_dir, \"diarization.csv\")\nself.diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\",\nuse_auth_token=os.getenv(\"PYANNOTE_AUTH\"))\ndef diarize_file(self, file_path=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\ndiarization_output = self.diarization_pipeline(file_path)\nself.diarization_segments = []\nfor segment, _, speaker in diarization_output.itertracks(yield_label=True):\nself.diarization_segments.append({\n\"segment\":segment,\n\"speaker id\":speaker\n})\nself.save_csv(self.diarization_segments, filename=self.diarization_path)\nreturn self.diarization_segments\ndef save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\nwith open(filename, 'w') as diraization_file:\nwriter = csv.writer(diraization_file)\nwriter.writerow(diarization[0].keys())\nfor segment in diarization:\nwriter.writerow(segment.values())\ndiraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize.diarize_file","title":"<code>diarize_file(file_path=None)</code>","text":"<p>Diarize a file or the file that has been saved</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def diarize_file(self, file_path=None):\n\"\"\"Diarize a file or the file that has been saved\"\"\"\ndiarization_output = self.diarization_pipeline(file_path)\nself.diarization_segments = []\nfor segment, _, speaker in diarization_output.itertracks(yield_label=True):\nself.diarization_segments.append({\n\"segment\":segment,\n\"speaker id\":speaker\n})\nself.save_csv(self.diarization_segments, filename=self.diarization_path)\nreturn self.diarization_segments\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.PyannoteDiarize.save_csv","title":"<code>save_csv(diarization, filename='diarization.csv')</code>","text":"<p>Save diarization to csv</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def save_csv(self, diarization, filename=\"diarization.csv\"):\n\"\"\"Save diarization to csv\"\"\"\nwith open(filename, 'w') as diraization_file:\nwriter = csv.writer(diraization_file)\nwriter.writerow(diarization[0].keys())\nfor segment in diarization:\nwriter.writerow(segment.values())\ndiraization_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/pyannote_diarization/#backend.app.utils.stt.backends.pyannote_diarization.main","title":"<code>main()</code>","text":"<p>Test PyannoteDiarize</p> Source code in <code>backend/app/utils/stt/backends/pyannote_diarization.py</code> <pre><code>def main():\n\"\"\"Test PyannoteDiarize\"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--filename\", default=\"output/test.wav\",\nhelp=\"location of file to transcribe\")\nargs = parser.parse_args()\ndiarizer = PyannoteDiarize(save_dir=\"output\")\nresults = diarizer.diarize_file(file_path=args.filename)\nfor segment in results:\nprint(segment)\n</code></pre>"},{"location":"reference/utils/stt/backends/record_mic/","title":"record_mic","text":"<p>Module for recording a microphone to a file</p> <p>Recording settings are set internally to match whisper format.</p>"},{"location":"reference/utils/stt/backends/record_mic/#backend.app.utils.stt.backends.record_mic.main","title":"<code>main()</code>","text":"<p>Example of making a simple audio clip</p> Source code in <code>backend/app/utils/stt/backends/record_mic.py</code> <pre><code>def main():\n\"\"\"Example of making a simple audio clip\"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--record_for\", default=20,\nhelp=\"Set recording length (seconds)\", type=bool)\nparser.add_argument(\"--filename\", default=\"output/test.wav\",\nhelp=\"location to save audio file\")\nargs = parser.parse_args()\nrecord_mic(args.filename, seconds=args.record_for)\nprint(f\"File saved to {args.filename}\")\n</code></pre>"},{"location":"reference/utils/stt/backends/record_mic/#backend.app.utils.stt.backends.record_mic.record_mic","title":"<code>record_mic(filename, seconds=200)</code>","text":"<p>Simple helper function for recording audio to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Where to save audio to.</p> required <code>seconds</code> <code>int</code> <p>How long to record. Defaults to 200.</p> <code>200</code> Source code in <code>backend/app/utils/stt/backends/record_mic.py</code> <pre><code>def record_mic(filename: str, seconds: int = 200) -&gt; None:\n\"\"\"Simple helper function for recording audio to a file.\n    Args:\n        filename (str): Where to save audio to.\n        seconds (int, optional): How long to record. Defaults to 200.\n    \"\"\"\nchunk = 1024  # Record in chunks of 1024 samples\nsample_format = pyaudio.paInt16  # 16 bits per sample\nchannels = 2\nframerate = 16000  # Record at 44100 samples per second\naudio_interface = pyaudio.PyAudio()  # Create an interface to PortAudio\nprint('Recording')\nstream = audio_interface.open(format=sample_format,\nchannels=channels,\nrate=framerate,\nframes_per_buffer=chunk,\ninput=True)\nframes = []  # Initialize array to store frames\n# Store data in chunks\nfor _ in range(0, int(framerate / chunk * seconds)):\ndata = stream.read(chunk)\nframes.append(data)\n# Stop and close the stream\nstream.stop_stream()\nstream.close()\n# Terminate the PortAudio interface\naudio_interface.terminate()\nprint('Finished recording')\n# Save the recorded data as a WAV file\nwavefile = wave.open(filename, 'wb')\nwavefile.setnchannels(channels)\nwavefile.setsampwidth(audio_interface.get_sample_size(sample_format))\nwavefile.setframerate(framerate)\nwavefile.writeframes(b''.join(frames))\nwavefile.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/","title":"whisper_stt","text":"<p>Module wraps the Whisper Model from OpenAI to transcribe audio</p> <p>The whisper model provides state of the art results at a passable speed.</p>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT","title":"<code>WhisperSTT</code>","text":"<p>Helper class which transcribes audio clips or files</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>class WhisperSTT:\n\"\"\" Helper class which transcribes audio clips or files\n    \"\"\"\ndef __init__(self, model_size: str = \"medium\", save_dir: str = None) -&gt; None:\nself.audio_model = whisper.load_model(model_size)\nif not save_dir:\nself.save_dir = tempfile.mkdtemp()\nelse:\nself.save_dir = save_dir\ndef transcribe_clip(self, audio_clip: AudioSegment) -&gt; str:\n\"\"\"Transcribes audio segment\n            Args:\n                audio_clip (AudioSegment): bytes read from a file containing speech\n            Returns:\n                str: the transcribed text. \"\"\"\ndefault_wave_path = os.path.join(self.save_dir, \"temp.wav\")\naudio_clip.export(default_wave_path, format=\"wav\")\nresult = self.audio_model.transcribe(default_wave_path, language='english')\nreturn result[\"text\"]\ndef transcribe_file(self, file_path: str, csv_name: str=\"transcription_test.csv\") -&gt; dict:\n\"\"\"Transcribe a file\"\"\"\nresult = self.audio_model.transcribe(file_path, language='english')\ntranscription_path = os.path.join(self.save_dir, csv_name)\nself.save_csv(result[\"segments\"], transcription_path)\nreturn result\ndef save_csv(self, segments, filename=\"speech_segments.csv\"):\n\"\"\"Save csv of speech segments\"\"\"\nwith open(filename, 'w') as my_file:\nwriter = csv.writer(my_file)\nwriter.writerow(segments[0].keys())\nfor seg in segments:\nwriter.writerow(seg.values())\nmy_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.save_csv","title":"<code>save_csv(segments, filename='speech_segments.csv')</code>","text":"<p>Save csv of speech segments</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def save_csv(self, segments, filename=\"speech_segments.csv\"):\n\"\"\"Save csv of speech segments\"\"\"\nwith open(filename, 'w') as my_file:\nwriter = csv.writer(my_file)\nwriter.writerow(segments[0].keys())\nfor seg in segments:\nwriter.writerow(seg.values())\nmy_file.close()\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.transcribe_clip","title":"<code>transcribe_clip(audio_clip)</code>","text":"<p>Transcribes audio segment</p> <p>Parameters:</p> Name Type Description Default <code>audio_clip</code> <code>AudioSegment</code> <p>bytes read from a file containing speech</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the transcribed text.</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def transcribe_clip(self, audio_clip: AudioSegment) -&gt; str:\n\"\"\"Transcribes audio segment\n        Args:\n            audio_clip (AudioSegment): bytes read from a file containing speech\n        Returns:\n            str: the transcribed text. \"\"\"\ndefault_wave_path = os.path.join(self.save_dir, \"temp.wav\")\naudio_clip.export(default_wave_path, format=\"wav\")\nresult = self.audio_model.transcribe(default_wave_path, language='english')\nreturn result[\"text\"]\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.WhisperSTT.transcribe_file","title":"<code>transcribe_file(file_path, csv_name='transcription_test.csv')</code>","text":"<p>Transcribe a file</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def transcribe_file(self, file_path: str, csv_name: str=\"transcription_test.csv\") -&gt; dict:\n\"\"\"Transcribe a file\"\"\"\nresult = self.audio_model.transcribe(file_path, language='english')\ntranscription_path = os.path.join(self.save_dir, csv_name)\nself.save_csv(result[\"segments\"], transcription_path)\nreturn result\n</code></pre>"},{"location":"reference/utils/stt/backends/whisper_stt/#backend.app.utils.stt.backends.whisper_stt.main","title":"<code>main()</code>","text":"<p>Test of WhisperSTT</p> Source code in <code>backend/app/utils/stt/backends/whisper_stt.py</code> <pre><code>def main():\n\"\"\"Test of WhisperSTT\"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--model\", default=\"base\", help=\"Model to use\",\nchoices=[\"tiny\", \"base\", \"small\", \"medium\", \"large\"])\nparser.add_argument(\"--filename\", default=\"output/test.wav\",\nhelp=\"location of file to transcribe\")\nargs = parser.parse_args()\ntranscriber = WhisperSTT(model_size=args.model, save_dir=\"output\")\nprint(\"loading complete\")\nresult = transcriber.transcribe_file(args.filename)\nprint(result['text'])\n</code></pre>"},{"location":"reference/utils/tts/speaker/","title":"speaker","text":"<p>Utility module for creating audio and visemes from text.</p> <p>There are two supported options for generating speech, Coqui and Polly. You must choose between the two with the backend parameter at load time. Both support a wide variety of voice accents/genders/ages that can be chosen at runtime by selecting the speaker_identifier.</p>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.Speaker","title":"<code>Speaker</code>","text":"<p>Speaker takes text and returns an audio stream and a viseme list with timing</p> <p>Speaker backend is set at runtime, but speaker ID can change dynammically</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Which backend to use. Defaults to \"polly\".</p> <code>'polly'</code> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>class Speaker:\n\"\"\"Speaker takes text and returns an audio stream and a viseme list with timing\n        Speaker backend is set at runtime, but speaker ID can change dynammically\n        Args:\n            backend (str, optional): Which backend to use. Defaults to \"polly\".\"\"\"\ndef __init__(self, backend=\"polly\") -&gt; None:\nself.backend = backend\nif self.backend == \"polly\":\nself.speaker = PollySpeak()\nif self.backend == \"coqui\":\nself.speaker = CoquiSpeak()\nself.viseme_generator = VisemeGenerator()\ndef synthesize(self, input_text: str, speaker_identifier: str,\nsave_path: str = \"./tts/backends/output/temp.wav\") -&gt; tuple:\n\"\"\"Takes in text and a speaker id and returns speech and visemes and timings\n            Args:\n                input_text (str): input text to say\n                speaker_identifier (str): key for speaker voice\n                save_path (str, optional): path to save wav file. Defaults to \"./tts/backends/output/temp.wav\".\n            Returns:\n                tuple: the audio stream, the visemes, and the viseme timings\"\"\"\nif self.backend == \"polly\":\nresults = self.speaker.synthesize(input_text,\nspeaker_id=speaker_identifier,\nsave_path=save_path)\naudio_stream, visemes, delays = results\nvisemes = self.viseme_generator.convert_aws_visemes(visemes)\nif self.backend == \"coqui\":\nresults = self.speaker.synthesize_wav(input_text,\nspeaker_id=speaker_identifier)\naudio_stream, speaking_time = results\n# Visemes must be generated and timed manually for coqui\nvisemes = self.viseme_generator.get_visemes(input_text)\nviseme_length = (speaking_time) / (len(visemes)+1)\ndelays = [viseme_length for i in range(len(visemes))]\nreturn audio_stream, visemes, delays\n</code></pre>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.Speaker.synthesize","title":"<code>synthesize(input_text, speaker_identifier, save_path='./tts/backends/output/temp.wav')</code>","text":"<p>Takes in text and a speaker id and returns speech and visemes and timings</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>input text to say</p> required <code>speaker_identifier</code> <code>str</code> <p>key for speaker voice</p> required <code>save_path</code> <code>str</code> <p>path to save wav file. Defaults to \"./tts/backends/output/temp.wav\".</p> <code>'./tts/backends/output/temp.wav'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>the audio stream, the visemes, and the viseme timings</p> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>def synthesize(self, input_text: str, speaker_identifier: str,\nsave_path: str = \"./tts/backends/output/temp.wav\") -&gt; tuple:\n\"\"\"Takes in text and a speaker id and returns speech and visemes and timings\n        Args:\n            input_text (str): input text to say\n            speaker_identifier (str): key for speaker voice\n            save_path (str, optional): path to save wav file. Defaults to \"./tts/backends/output/temp.wav\".\n        Returns:\n            tuple: the audio stream, the visemes, and the viseme timings\"\"\"\nif self.backend == \"polly\":\nresults = self.speaker.synthesize(input_text,\nspeaker_id=speaker_identifier,\nsave_path=save_path)\naudio_stream, visemes, delays = results\nvisemes = self.viseme_generator.convert_aws_visemes(visemes)\nif self.backend == \"coqui\":\nresults = self.speaker.synthesize_wav(input_text,\nspeaker_id=speaker_identifier)\naudio_stream, speaking_time = results\n# Visemes must be generated and timed manually for coqui\nvisemes = self.viseme_generator.get_visemes(input_text)\nviseme_length = (speaking_time) / (len(visemes)+1)\ndelays = [viseme_length for i in range(len(visemes))]\nreturn audio_stream, visemes, delays\n</code></pre>"},{"location":"reference/utils/tts/speaker/#backend.app.utils.tts.speaker.main","title":"<code>main()</code>","text":"<p>Run speaker on a text string</p> <p>To hear the sound play the file at the save path \"./tts/backends/output/temp.wav\"</p> Source code in <code>backend/app/utils/tts/speaker.py</code> <pre><code>def main():\n\"\"\"Run speaker on a text string\n    To hear the sound play the file at the save path \"./tts/backends/output/temp.wav\"\n    \"\"\"\nparser = argparse.ArgumentParser(\nformatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"--backend\", default=\"polly\", help=\"Backend to use\",\nchoices=[\"polly\", \"coqui\"])\nparser.add_argument(\"--text\", default=\"This is what I sound like\",\nhelp=\"Text to say\")\nparser.add_argument(\"--speakerid\", default=\"Kevin\",\nhelp=\"location of file to transcribe\")\nparser.add_argument(\"--savepath\", default=\"./tts/backends/output/temp.wav\",\nhelp=\"location of file to save audio\")\nargs = parser.parse_args()\nspeaker = Speaker(backend=args.backend)\n_, visemes, delays = speaker.synthesize(args.text, args.speakerid, save_path=args.savepath)\nvisemes_delays = zip(visemes,delays)\nfor i in visemes_delays:\nprint(i)\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/","title":"aws_polly_tts","text":"<p>Module for calling on AWS Polly</p> Requires AWS setup for polly. <p>Create a client using the credentials and region defined in the [adminuser] section of the AWS credentials file (~/.aws/credentials).</p> <p>Will play the speech live.</p>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.PollySpeak","title":"<code>PollySpeak</code>","text":"<p>Synthesizes speech with AWS Polly</p> Possible english speakers include <p>US English en-US {'Kevin', 'Salli', 'Matthew', 'Kimberly', 'Kendra',                     'Justin', 'Joey', 'Joanna', 'Ivy'} New Zealand English en-NZ {'Aria'} South African English en-ZA {'Ayanda'} British English en-GB {'Emma', 'Brian', 'Amy', 'Arthur'} Australian English en-AU {'Olivia'} Indian English en-IN {'Kajal'}</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>class PollySpeak():\n\"\"\" Synthesizes speech with AWS Polly\n    Possible english speakers include:\n        US English en-US {'Kevin', 'Salli', 'Matthew', 'Kimberly', 'Kendra',\n                            'Justin', 'Joey', 'Joanna', 'Ivy'}\n        New Zealand English en-NZ {'Aria'}\n        South African English en-ZA {'Ayanda'}\n        British English en-GB {'Emma', 'Brian', 'Amy', 'Arthur'}\n        Australian English en-AU {'Olivia'}\n        Indian English en-IN {'Kajal'}\n    \"\"\"\ndef __init__(self, default_path: str = \"./output/temp.wav\") -&gt; None:\nself.engine = \"neural\"\nself.audio_format = \"mp3\"\nself.polly_client = polly\nself.path = Path(__file__).parent\nself.save_path = os.path.join(self.path, default_path)\ndef synthesize(self, text: str, speaker_id: str = \"\", save_path: str = None):\n\"\"\"Turns text into audio and visemes\"\"\"\nif save_path:\nself.save_path = save_path\nlang_code = None\nfor key, names in english_speaker_map.items():\nif speaker_id in names:\nlang_code = key\nvoice = speaker_id\nif not lang_code:\nlang_code = \"en-US\"\nvoice = 'Kendra'\ntry:\nkwargs = {\n'Engine': self.engine,\n'OutputFormat': self.audio_format,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.synthesize_speech(**kwargs)\n# print(\"got response\", response)\naudio_stream = response['AudioStream']\noutput = self.save_path\nwith closing(audio_stream) as stream:\nwith open(output, \"wb\") as file:\nfile.write(stream.read())\noutstream = io.open(output, 'rb', buffering=0)\nvisemes = None\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.synthesize_speech(**kwargs)\nvisemes = [json.loads(viseme) for viseme in\nresponse['AudioStream'].read().decode().split() if viseme]\nviseme_list = []\ntime_list = []\nfor viseme in visemes:\nviseme_list.append(viseme[\"value\"])\ntime_list.append(viseme[\"time\"])\nsleep_times = []\nt_before = 0\nfor next_t in time_list:\nwait_seconds = float(next_t) - float(t_before)\nsleep_times.append(wait_seconds/1000)\nt_before = next_t\nexcept ClientError as exc:\nprint(exc)\nraise\nelse:\nreturn outstream, viseme_list, sleep_times\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.PollySpeak.synthesize","title":"<code>synthesize(text, speaker_id='', save_path=None)</code>","text":"<p>Turns text into audio and visemes</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>def synthesize(self, text: str, speaker_id: str = \"\", save_path: str = None):\n\"\"\"Turns text into audio and visemes\"\"\"\nif save_path:\nself.save_path = save_path\nlang_code = None\nfor key, names in english_speaker_map.items():\nif speaker_id in names:\nlang_code = key\nvoice = speaker_id\nif not lang_code:\nlang_code = \"en-US\"\nvoice = 'Kendra'\ntry:\nkwargs = {\n'Engine': self.engine,\n'OutputFormat': self.audio_format,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.synthesize_speech(**kwargs)\n# print(\"got response\", response)\naudio_stream = response['AudioStream']\noutput = self.save_path\nwith closing(audio_stream) as stream:\nwith open(output, \"wb\") as file:\nfile.write(stream.read())\noutstream = io.open(output, 'rb', buffering=0)\nvisemes = None\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.synthesize_speech(**kwargs)\nvisemes = [json.loads(viseme) for viseme in\nresponse['AudioStream'].read().decode().split() if viseme]\nviseme_list = []\ntime_list = []\nfor viseme in visemes:\nviseme_list.append(viseme[\"value\"])\ntime_list.append(viseme[\"time\"])\nsleep_times = []\nt_before = 0\nfor next_t in time_list:\nwait_seconds = float(next_t) - float(t_before)\nsleep_times.append(wait_seconds/1000)\nt_before = next_t\nexcept ClientError as exc:\nprint(exc)\nraise\nelse:\nreturn outstream, viseme_list, sleep_times\n</code></pre>"},{"location":"reference/utils/tts/backends/aws_polly_tts/#backend.app.utils.tts.backends.aws_polly_tts.main","title":"<code>main()</code>","text":"<p>Testing the integrated functionality of PollySpeak</p> Source code in <code>backend/app/utils/tts/backends/aws_polly_tts.py</code> <pre><code>def main():\n\"\"\"Testing the integrated functionality of PollySpeak\"\"\"\nspeaker = PollySpeak(default_path=\"./output/temp.mp3\")\nexample = (\"Please call Stella.  Ask her to bring these things with her from the store: \"\n\"Six spoons of fresh snow peas, five thick slabs of blue cheese, \"\n\"and maybe a snack for her brother Bob.  We also need a small plastic snake \"\n\"and a big toy frog for the kids.  She can scoop these things into three red bags, \"\n\"and we will go meet her Wednesday at the train station.\")\ntry:\n# Request speech synthesis\n_, viseme_list, sleep_times = speaker.synthesize(example, \"Aria\")\nexcept (BotoCoreError, ClientError) as error:\n# The service returned an error, exit gracefully\nprint(error)\nsys.exit(-1)\n# Play the audio using the platform's default player\nif sys.platform == \"win32\":\nos.startfile(\"./output/temp.mp3\")\nelse:\n# The following works on macOS and Linux. (Darwin = mac, xdg-open = linux).\nopener = \"open\" if sys.platform == \"darwin\" else \"xdg-open\"\nsubprocess.call([opener, \"./output/temp.mp3\"])\nfor ind, sleep in enumerate(sleep_times):\ntime.sleep(sleep)\nprint(sleep, viseme_list[ind])\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/","title":"coqui_tts","text":"<p>Open Source Module wrapping coqui's library for text to speech.</p> <p>This is a functional but messy wrapper of coqui's TTS library for speech synthesis.</p> <p>TODO this file is still in need of a large amount of cleanup</p>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.CoquiSpeak","title":"<code>CoquiSpeak</code>","text":"<p>Setup Model and perform synthesis</p> Speakers of note for the vits model <p>267!,307 - English male, medium 330!,232 - English male, slow 312!,251 - English male, fast 287,254 - English male, fast and deep 303 - English female, slow 306 - English female, medium 308 - English female, slow 295!,270 - American female, slow 317! - American male, slow 230! - American male, fast 345 - south african female, slow 313,233 - ? male, fast</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>class CoquiSpeak:\n\"\"\"Setup Model and perform synthesis\n    Speakers of note for the vits model:\n        267!,307 - English male, medium\n        330!,232 - English male, slow\n        312!,251 - English male, fast\n        287,254 - English male, fast and deep\n        303 - English female, slow\n        306 - English female, medium\n        308 - English female, slow\n        295!,270 - American female, slow\n        317! - American male, slow\n        230! - American male, fast\n        345 - south african female, slow\n        313,233 - ? male, fast\n    \"\"\"\ndef __init__(self) -&gt; None:\n# parse the args\nargs, _ = create_argparser().parse_known_args()\nself.path = Path(__file__).parent\nmanager = ModelManager(os.path.join(\nself.path, \"resources/.coqui_tts_models.json\"))\n# update in-use models to the specified released models.\nmodel_path = None\nconfig_path = None\nspeakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\nmanager.list_models()\nsys.exit()\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:\nmodel_path, config_path, model_item = manager.download_model(\nargs.model_name)\nargs.vocoder_name = model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name\nif args.vocoder_name is not None and not args.vocoder_path:\nvocoder_path, vocoder_config_path, _ = manager.download_model(\nargs.vocoder_name)\n# CASE3: set custom model paths\nif args.model_path is not None:\nmodel_path = args.model_path\nconfig_path = args.config_path\nspeakers_file_path = args.speakers_file_path\nif args.vocoder_path is not None:\nvocoder_path = args.vocoder_path\nvocoder_config_path = args.vocoder_config_path\n# load models\nself.synthesizer = Synthesizer(\ntts_checkpoint=model_path,\ntts_config_path=config_path,\ntts_speakers_file=speakers_file_path,\ntts_languages_file=None,\nvocoder_checkpoint=vocoder_path,\nvocoder_config=vocoder_config_path,\nencoder_checkpoint=\"\",\nencoder_config=\"\",\nuse_cuda=args.use_cuda,\n)\nself.use_multi_speaker = hasattr(self.synthesizer.tts_model, \"num_speakers\") and (\nself.synthesizer.tts_model.num_speakers &gt; 1 or self.synthesizer.tts_speakers_file is not None\n)\nself.speaker_manager = getattr(\nself.synthesizer.tts_model, \"speaker_manager\", None)\n# TODO: set this from SpeakerManager\nuse_gst = self.synthesizer.tts_config.get(\"use_gst\", False)\nself.speaker_ids = self.speaker_manager.name_to_id if self.speaker_manager is not None else None\ndef synthesize_wav(self, text: str, speaker_id: str = \"\", style_wav: str = \"\"):\n\"\"\"Turn text into a wav file and return byte stream\"\"\"\nstyle_wav = style_wav_uri_to_dict(style_wav)\ntry:\nwavs = self.synthesizer.tts(\ntext, speaker_name=speaker_id, style_wav=style_wav)\nexcept Exception as exc:\nprint(exc)\noutstream = io.BytesIO()\nself.synthesizer.save_wav(wavs, outstream)\nsave_path = os.path.join(\nself.path, f\"output/test_speech{speaker_id}.wav\")\n# print(self.synthesizer.tts_model.speaker_manager.name_to_id)\nself.synthesizer.save_wav(wavs, save_path)\nsound = sf.SoundFile(save_path)\nspeaking_length = sound.frames / sound.samplerate\n# print('Sound file timeing is seconds = {}'.format(speaking_length))\nreturn outstream, speaking_length\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.CoquiSpeak.synthesize_wav","title":"<code>synthesize_wav(text, speaker_id='', style_wav='')</code>","text":"<p>Turn text into a wav file and return byte stream</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def synthesize_wav(self, text: str, speaker_id: str = \"\", style_wav: str = \"\"):\n\"\"\"Turn text into a wav file and return byte stream\"\"\"\nstyle_wav = style_wav_uri_to_dict(style_wav)\ntry:\nwavs = self.synthesizer.tts(\ntext, speaker_name=speaker_id, style_wav=style_wav)\nexcept Exception as exc:\nprint(exc)\noutstream = io.BytesIO()\nself.synthesizer.save_wav(wavs, outstream)\nsave_path = os.path.join(\nself.path, f\"output/test_speech{speaker_id}.wav\")\n# print(self.synthesizer.tts_model.speaker_manager.name_to_id)\nself.synthesizer.save_wav(wavs, save_path)\nsound = sf.SoundFile(save_path)\nspeaking_length = sound.frames / sound.samplerate\n# print('Sound file timeing is seconds = {}'.format(speaking_length))\nreturn outstream, speaking_length\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.create_argparser","title":"<code>create_argparser()</code>","text":"<p>Parse args to set defaults</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def create_argparser():\n\"\"\"Parse args to set defaults\"\"\"\ndef convert_boolean(item):\nreturn item.lower() in [\"true\", \"1\", \"yes\"]\nparser = argparse.ArgumentParser()\nparser.add_argument(\n\"--list_models\",\ntype=convert_boolean,\nnargs=\"?\",\nconst=True,\ndefault=False,\nhelp=\"list available pre-trained tts and vocoder models.\",\n)\nparser.add_argument(\n\"--model_name\",\ntype=str,\ndefault=\"tts_models/en/vctk/vits\",\nhelp=\"Name of one of the pre-trained tts models in format &lt;language&gt;/&lt;dataset&gt;/&lt;model_name&gt;\",\n)\nparser.add_argument(\"--vocoder_name\", type=str, default=None,\nhelp=\"name of one of the released vocoder models.\")\n# Args for running custom models\nparser.add_argument(\"--config_path\", default=None,\ntype=str, help=\"Path to model config file.\")\nparser.add_argument(\n\"--model_path\",\ntype=str,\ndefault=None,\nhelp=\"Path to model file.\",\n)\nparser.add_argument(\n\"--vocoder_path\",\ntype=str,\nhelp=\"Path to vocoder model file. If it is not defined, model uses GL as vocoder. Please make sure that you installed vocoder library before (WaveRNN).\",\ndefault=None,\n)\nparser.add_argument(\"--vocoder_config_path\", type=str,\nhelp=\"Path to vocoder model config file.\", default=None)\nparser.add_argument(\"--speakers_file_path\", type=str,\nhelp=\"JSON file for multi-speaker model.\", default=None)\nparser.add_argument(\"--port\", type=int, default=5002,\nhelp=\"port to listen on.\")\nparser.add_argument(\"--use_cuda\", type=convert_boolean,\ndefault=False, help=\"true to use CUDA.\")\nparser.add_argument(\"--debug\", type=convert_boolean,\ndefault=False, help=\"true to enable Flask debug mode.\")\nparser.add_argument(\"--show_details\", type=convert_boolean,\ndefault=False, help=\"Generate model detail page.\")\nreturn parser\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.main","title":"<code>main()</code>","text":"<p>Integration testing of Coqui Speaker</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def main():\n\"\"\"Integration testing of Coqui Speaker\"\"\"\n# text = \"I am your personal virtual assistant. I am quick witted, helpful, and frendly.\"\nexample_text = \"Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\"\n# test_text = \"For. the. record. It. pleases. the. court. to. be. wrong. About. this\"\n# period and ?, long pause from breaking into list\n# colon, semicolon, comma short pause\n# ! breaks things up unpredictably\nk = \"p267\"\ntts = CoquiSpeak()\n_, speaking_time = tts.synthesize_wav(example_text, k)\nprint(f\"File length: {speaking_time}\")\n</code></pre>"},{"location":"reference/utils/tts/backends/coqui_tts/#backend.app.utils.tts.backends.coqui_tts.style_wav_uri_to_dict","title":"<code>style_wav_uri_to_dict(style_wav)</code>","text":"<p>Transform an uri style_wav, in either a string (path to wav file to be use for style transfer) or a dict (gst tokens/values to be use for styling)</p> <p>Parameters:</p> Name Type Description Default <code>style_wav</code> <code>str</code> <p>uri</p> required <p>Returns:</p> Type Description <code>Union[str, dict]</code> <p>Union[str, dict]: path to file (str) or gst style (dict)</p> Source code in <code>backend/app/utils/tts/backends/coqui_tts.py</code> <pre><code>def style_wav_uri_to_dict(style_wav: str) -&gt; Union[str, dict]:\n\"\"\"Transform an uri style_wav, in either a string (path to wav file to be\n    use for style transfer) or a dict (gst tokens/values to be use for styling)\n    Args:\n        style_wav (str): uri\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"\nif style_wav:\nif os.path.isfile(style_wav) and style_wav.endswith(\".wav\"):\nreturn style_wav  # style_wav is a .wav file located on the server\nstyle_wav = json.loads(style_wav)\n# style_wav is a gst dictionary with {token1_id : token1_weigth, ...}\nreturn style_wav\nreturn None\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/","title":"viseme_generator","text":"<p>Generate Visemes from text.</p> <p>This module converts text to phonemes and then phonemes to visemes.</p>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator","title":"<code>VisemeGenerator</code>","text":"<p>Contains the functionality to convert text to visemes</p> <p>Or to convert between viseme types</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>class VisemeGenerator:\n\"\"\"Contains the functionality to convert text to visemes\n    Or to convert between viseme types\n    \"\"\"\ndef __init__(self, convertion_table=\"./resources/.phoneme-viseme_map.csv\", log=False) -&gt; None:\nself.log = log\nself.path = Path(__file__).parent\nconversion_table_path = os.path.join(self.path, convertion_table)\nself.backend = EspeakBackend('en-us', preserve_punctuation=True, with_stress=False)\nself.joint_df = pd.read_csv(conversion_table_path, header=0)\nself.joint_df.set_index(\"IPA\")\ndef get_viseme(self, ipa, type='IPA'):\n\"\"\"Converts individual phoneme to a viseme\"\"\"\ntry:\nviseme_dict = self.joint_df[self.joint_df[type]==ipa]\nif len(viseme_dict) &lt; 1:\nviseme_dict = self.joint_df[self.joint_df['Alternative IPA'] == ipa]\nif len(viseme_dict) &lt; 1:\nif self.log:\nprint(f\"Viseme for: {ipa} NOT FOUND\")\nviseme = viseme_dict[\"SimpleViseme\"].values[0]\nexcept Exception as exc:\nprint(exc)\nviseme = \"IDLE\"\nreturn viseme\ndef process_phoneme_string(self, phonemes_string):\n\"\"\"Converts phoneme string to list\n        Handles a bug with a special character as well\n        \"\"\"\nphoneme_list = []\nfor phoneme in phonemes_string:\nif phoneme != \"\u02d0\":\nphoneme_list.append(phoneme)\nelse:\nphoneme_list[-1] = phoneme_list[-1] + \"\u02d0\"\n# note this is a special character, not a colon\nif self.log:\nprint(phoneme_list)\nreturn phoneme_list\ndef convert_aws_visemes(self, visemes):\nnew_visemes = []\nfor vis in visemes:\nnew_visemes.append(self.get_viseme(vis, type=\"Viseme\"))\nreturn new_visemes\ndef get_visemes(self, sentence, return_phonemes = False):\n\"\"\"Process a sentence or list of sentences into visemes\"\"\"\nif isinstance(sentence, str):\nsentence = [sentence]\nif self.log:\nprint(f\"String to process: {sentence}\")\nphonemized = self.backend.phonemize(sentence, strip=False)[0]\nif self.log:\nprint(f\"Phonemes: {phonemized}\")\nphoneme_list = self.process_phoneme_string(phonemized) + [' ']\nvisemes_list = []\nfor phoneme in phoneme_list:\nvis = self.get_viseme(phoneme)\nif vis == \"IDLE\":# Append Idles twice to give better pauses\nvisemes_list.append(vis)\nvisemes_list.append(vis)\nif return_phonemes:\nreturn visemes_list, phoneme_list\nreturn visemes_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.get_viseme","title":"<code>get_viseme(ipa, type='IPA')</code>","text":"<p>Converts individual phoneme to a viseme</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def get_viseme(self, ipa, type='IPA'):\n\"\"\"Converts individual phoneme to a viseme\"\"\"\ntry:\nviseme_dict = self.joint_df[self.joint_df[type]==ipa]\nif len(viseme_dict) &lt; 1:\nviseme_dict = self.joint_df[self.joint_df['Alternative IPA'] == ipa]\nif len(viseme_dict) &lt; 1:\nif self.log:\nprint(f\"Viseme for: {ipa} NOT FOUND\")\nviseme = viseme_dict[\"SimpleViseme\"].values[0]\nexcept Exception as exc:\nprint(exc)\nviseme = \"IDLE\"\nreturn viseme\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.get_visemes","title":"<code>get_visemes(sentence, return_phonemes=False)</code>","text":"<p>Process a sentence or list of sentences into visemes</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def get_visemes(self, sentence, return_phonemes = False):\n\"\"\"Process a sentence or list of sentences into visemes\"\"\"\nif isinstance(sentence, str):\nsentence = [sentence]\nif self.log:\nprint(f\"String to process: {sentence}\")\nphonemized = self.backend.phonemize(sentence, strip=False)[0]\nif self.log:\nprint(f\"Phonemes: {phonemized}\")\nphoneme_list = self.process_phoneme_string(phonemized) + [' ']\nvisemes_list = []\nfor phoneme in phoneme_list:\nvis = self.get_viseme(phoneme)\nif vis == \"IDLE\":# Append Idles twice to give better pauses\nvisemes_list.append(vis)\nvisemes_list.append(vis)\nif return_phonemes:\nreturn visemes_list, phoneme_list\nreturn visemes_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.VisemeGenerator.process_phoneme_string","title":"<code>process_phoneme_string(phonemes_string)</code>","text":"<p>Converts phoneme string to list</p> <p>Handles a bug with a special character as well</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def process_phoneme_string(self, phonemes_string):\n\"\"\"Converts phoneme string to list\n    Handles a bug with a special character as well\n    \"\"\"\nphoneme_list = []\nfor phoneme in phonemes_string:\nif phoneme != \"\u02d0\":\nphoneme_list.append(phoneme)\nelse:\nphoneme_list[-1] = phoneme_list[-1] + \"\u02d0\"\n# note this is a special character, not a colon\nif self.log:\nprint(phoneme_list)\nreturn phoneme_list\n</code></pre>"},{"location":"reference/utils/tts/backends/viseme_generator/#backend.app.utils.tts.backends.viseme_generator.main","title":"<code>main()</code>","text":"<p>Integration testing for viseme generator</p> Source code in <code>backend/app/utils/tts/backends/viseme_generator.py</code> <pre><code>def main():\n\"\"\"Integration testing for viseme generator\"\"\"\ntext = [\"Hello, world! Welcome to the arena?\"]\ngen = VisemeGenerator()\nvisemes,phonemes = gen.get_visemes(text, True)\nprint(len(visemes), len(phonemes))\nmin_phone_vis = min(len(visemes),len(phonemes))\nfor i in range(min_phone_vis):\nprint(i, phonemes[i], visemes[i])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/","title":"polly_lipsync","text":"<p>Purpose</p> <p>Shows how to use the AWS SDK for Python (Boto3) with Amazon Polly and Tkinter to create a lip-sync application that displays an animated face speaking along with the speech synthesized by Amazon Polly. Lip-sync is accomplished by requesting a list of visemes from Amazon Polly that match up with the synthesized speech.</p>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth","title":"<code>PollyMouth</code>","text":"<p>A Tkinter application that lets a user enter text, select an Amazon Polly voice, and hear the text spoken by the selected voice while an animated face lip-syncs along with it.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>class PollyMouth:\n\"\"\"\n    A Tkinter application that lets a user enter text, select an Amazon Polly voice,\n    and hear the text spoken by the selected voice while an animated face lip-syncs\n    along with it.\n    \"\"\"\n# A dictionary of visemes mapped to image file names.\nlips = {\n'p': {'name': '.media/lips_m.png'},\n't': {'name': '.media/lips_c.png'},\n'S': {'name': '.media/lips_ch.png'},\n'T': {'name': '.media/lips_th.png'},\n'f': {'name': '.media/lips_f.png'},\n'k': {'name': '.media/lips_c.png'},\n'i': {'name': '.media/lips_e.png'},\n'r': {'name': '.media/lips_r.png'},\n's': {'name': '.media/lips_c.png'},\n'u': {'name': '.media/lips_w.png'},\n'@': {'name': '.media/lips_u.png'},\n'a': {'name': '.media/lips_a.png'},\n'e': {'name': '.media/lips_a.png'},\n'E': {'name': '.media/lips_u.png'},\n'o': {'name': '.media/lips_o.png'},\n'O': {'name': '.media/lips_u.png'},\n'sil': {'name': '.media/lips_sil.png'}\n}\ndef __init__(self, polly_wrapper):\n\"\"\"\n        Initializes the main Tkinter window and adds all of the widgets needed for\n        the application.\n        :param polly_wrapper: An object that can call Amazon Polly API functions.\n        \"\"\"\nself.polly_wrapper = polly_wrapper\nself.app = tkinter.Tk()\nself.app.title(\"Amazon Polly Lip Sync\")\nself.app.resizable(False, False)\nself.load_lips()\nchoices_frame = tkinter.Frame(self.app)\nself.sayit_label = tkinter.Label(\nself.app, wraplength=410,\ntext=\"Write some text in the box below, then click 'Say it!' \"\n\"to hear and see your text.\")\nself.sayit_txt = tkinter.Text(self.app, width=50, height=16)\nself.engine_label = tkinter.Label(choices_frame, text='Engine:')\nself.engine_var = tkinter.StringVar(choices_frame, 'neural')\nself.engine_options = tkinter.OptionMenu(\nchoices_frame, self.engine_var, *sorted(polly_wrapper.get_voice_engines()),\ncommand=self.change_engine)\nself.language_label = tkinter.Label(choices_frame, text='Language:')\nself.language_var = tkinter.StringVar(choices_frame, 'US English')\nself.language_choices = polly_wrapper.get_languages(self.engine_var.get())\nself.language_options = tkinter.OptionMenu(\nchoices_frame, self.language_var, *sorted(self.language_choices),\ncommand=self.change_language)\nself.voice_label = tkinter.Label(choices_frame, text='Voice:')\nself.voice_var = tkinter.StringVar(choices_frame, 'Joanna')\nself.voice_choices = polly_wrapper.get_voices(\nself.engine_var.get(), self.language_choices[self.language_var.get()])\nself.voice_options = tkinter.OptionMenu(\nchoices_frame, self.voice_var, *sorted(self.voice_choices))\nself.face_canvas = tkinter.Canvas(\nchoices_frame, height=100, width=200, bg='white')\nself.sayit_button = tkinter.Button(\nself.app, text=\"Say it!\", command=self.say_it)\nself.loading_text = tkinter.Label(self.app, bg='white')\nself.app.geometry(\"635x320\")\nself.sayit_label.grid(row=0)\nself.sayit_txt.grid(row=1, column=0)\nself.sayit_txt.focus_set()\nself.sayit_button.grid(row=2, pady=10, columnspan=2)\nself.sayit_button.configure(width=85, padx=10)\nchoices_frame.grid(row=1, column=1, sticky=tkinter.N)\nself.engine_label.grid(row=0, column=0, sticky=tkinter.N, pady=10)\nself.engine_options.grid(row=0, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.engine_options.configure(width=18)\nself.language_label.grid(row=1, column=0, sticky=tkinter.N, pady=10)\nself.language_options.grid(row=1, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.language_options.configure(width=18)\nself.voice_label.grid(row=2, column=0, sticky=tkinter.N, pady=10)\nself.voice_options.grid(row=2, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.voice_options.configure(width=18)\nself.face_canvas.grid(row=3, columnspan=2, padx=10)\nself.face_canvas.create_image(100, 60, image=self.lips['sil']['image'])\nself.app.mainloop()\ndef load_lips(self):\n\"\"\"\n        Loads lip-sync images either from a local '.media' folder or from GitHub\n        and saves image data in a dictionary of visemes.\n        \"\"\"\nif os.path.isdir('.media'):\nlogger.info(\"Found .media folder. Loading images from the local folder.\")\nfor viseme in self.lips:\nself.lips[viseme]['image'] = tkinter.PhotoImage(\nfile=self.lips[viseme]['name'])\nelse:\nlogger.info(\"No local .media folder. Trying to load images from GitHub.\")\nfor viseme in self.lips:\nurl = GITHUB_URL + self.lips[viseme]['name']\nresp = requests.get(url)\nimg = resp.content if resp.status_code == 200 else b''\nif resp.status_code != 200:\nlogger.warning(\"Couldn't load image from %s.\", url)\nself.lips[viseme]['image'] = tkinter.PhotoImage(data=img)\ndef change_engine(self, engine):\n\"\"\"\n        Handles the event that is fired when the selected engine type is changed in\n        the UI. Updates the lists of available languages and voices that are supported\n        for the selected engine type.\n        :param engine: The newly selected engine type.\n        \"\"\"\nself.language_choices = self.polly_wrapper.get_languages(engine)\nlang_menu = self.language_options['menu']\nlang_menu.delete(0, 'end')\nsorted_choices = sorted(self.language_choices)\nfor lang in sorted_choices:\nlang_menu.add_command(\nlabel=lang, command=lambda l=lang: self.change_language(l))\nself.change_language(sorted_choices[0])\ndef change_language(self, language):\n\"\"\"\n        Handles the event that is fired when the selected language is changed in the\n        UI. Updates the list of available voices that are available for the selected\n        language and engine type.\n        :param language: The newly selected language.\n        \"\"\"\nself.language_var.set(language)\nself.voice_choices = self.polly_wrapper.get_voices(\nself.engine_var.get(), self.language_choices[language])\nvoice_menu = self.voice_options['menu']\nvoice_menu.delete(0, 'end')\nsorted_choices = sorted(self.voice_choices)\nfor voice in sorted_choices:\nvoice_menu.add_command(\nlabel=voice, command=lambda v=voice: self.voice_var.set(v))\nself.voice_var.set(sorted_choices[0])\ndef animate_lips(self, start_time, viseme, viseme_iter):\n\"\"\"\n        Animates the face that lip-syncs along with the synthesized speech. This\n        uses the list of visemes and their associated timings that is returned from\n        Amazon Polly. The image associated with a viseme is displayed and the next\n        viseme is scheduled to display at the time indicated in the list of visemes.\n        :param start_time: The time the animation is started. This is used to\n                           calculate the time to wait until the next viseme image\n                           is displayed.\n        :param viseme: The current viseme to display.\n        :param viseme_iter: An iterator that yields visemes from the list returned\n                            from Amazon Polly.\n        \"\"\"\ntry:\nmouth = self.lips.get(viseme['value'], self.lips['sil'])\nself.face_canvas.create_image(\n100, 60, image=mouth['image'])\nself.app.update()\nnext_viseme = next(viseme_iter)\nnext_time = start_time + next_viseme['time']\ncur_time = time.time_ns() // 1000000  # milliseconds\nwait_time = max(0, next_time - cur_time)\nlogger.info(\"Vis: %s, cur_time %s, wait_time %s\", mouth,\ncur_time - start_time, wait_time)\nself.app.after(\nwait_time, self.animate_lips, start_time, next_viseme, viseme_iter)\nexcept StopIteration:\npass\ndef long_text_wait_callback(self, task_type, task_status):\n\"\"\"\n        A callback function that displays status while waiting for an asynchronous\n        long text speech synthesis task to complete.\n        :param task_type: The type of synthesis task (either 'speech' or 'viseme').\n        :param task_status: The status of the task.\n        \"\"\"\nself.loading_text.grid(row=0, rowspan=4, columnspan=2, sticky=tkinter.NSEW)\nself.loading_text.configure(\ntext=f\"Waiting for {task_type}. Current status: {task_status}.\")\nself.app.update()\nif task_status in ('completed', 'failed'):\nself.app.after(1000)\nself.loading_text.grid_forget()\ndef say_it(self):\n\"\"\"\n        Gets synthesized speech and visemes from Amazon Polly, stores the audio in\n        a temporary file, and plays the sound and lip-sync animation.\n        When the text is too long for synchronous synthesis, this function displays a\n        dialog that asks the user for an Amazon Simple Storage Service (Amazon S3)\n        bucket to use for output storage, starts an asynchronous synthesis task, and\n        waits for the task to complete.\n        \"\"\"\naudio_stream = None\nvisemes = []\ntry:\naudio_stream, visemes = self.polly_wrapper.synthesize(\nself.sayit_txt.get(1.0, tkinter.END),\nself.engine_var.get(),\nself.voice_choices[self.voice_var.get()],\n'mp3',\nself.language_choices[self.language_var.get()],\nTrue)\nexcept ClientError as error:\nif error.response['Error']['Code'] == 'TextLengthExceededException':\nbucket_name = tkinter.simpledialog.askstring(\n\"Text too long\",\n\"The text is too long for synchronous synthesis. To start an\\n\"\n\"asynchronous job, enter the name of an existing Amazon S3\\n\"\n\"bucket to use for speech synthesis output and click OK.\",\nparent=self.app)\nif bucket_name:\naudio_stream, visemes = self.polly_wrapper.do_synthesis_task(\nself.sayit_txt.get(1.0, tkinter.END),\nself.engine_var.get(),\nself.voice_choices[self.voice_var.get()],\n'mp3',\nbucket_name,\nself.language_choices[self.language_var.get()],\nTrue,\nself.long_text_wait_callback)\nlogger.debug(\"Visemes: %s.\", json.dumps(visemes))\nif audio_stream is not None:\nwith TemporaryDirectory() as tempdir:\nspeech_file_name = tempdir + '/speech.mp3'\nwith open(speech_file_name, 'wb') as speech_file:\nspeech_file.write(audio_stream.read())\nsilence = '.media/silence.mp3'\nif not os.path.isdir('.media'):\nsilence = GITHUB_URL + silence\n# Play a short silent audio file to ensure playsound is loaded and\n# ready. Without this, the audio tends to lag behind viseme playback.\n# playsound(silence)\nplaysound(speech_file_name, block=False)\nstart_time = time.time_ns() // 1000000\nself.app.after(\n0, self.animate_lips, start_time, {'value': 'sil'}, iter(visemes))\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.__init__","title":"<code>__init__(polly_wrapper)</code>","text":"<p>Initializes the main Tkinter window and adds all of the widgets needed for the application.</p> <p>:param polly_wrapper: An object that can call Amazon Polly API functions.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def __init__(self, polly_wrapper):\n\"\"\"\n    Initializes the main Tkinter window and adds all of the widgets needed for\n    the application.\n    :param polly_wrapper: An object that can call Amazon Polly API functions.\n    \"\"\"\nself.polly_wrapper = polly_wrapper\nself.app = tkinter.Tk()\nself.app.title(\"Amazon Polly Lip Sync\")\nself.app.resizable(False, False)\nself.load_lips()\nchoices_frame = tkinter.Frame(self.app)\nself.sayit_label = tkinter.Label(\nself.app, wraplength=410,\ntext=\"Write some text in the box below, then click 'Say it!' \"\n\"to hear and see your text.\")\nself.sayit_txt = tkinter.Text(self.app, width=50, height=16)\nself.engine_label = tkinter.Label(choices_frame, text='Engine:')\nself.engine_var = tkinter.StringVar(choices_frame, 'neural')\nself.engine_options = tkinter.OptionMenu(\nchoices_frame, self.engine_var, *sorted(polly_wrapper.get_voice_engines()),\ncommand=self.change_engine)\nself.language_label = tkinter.Label(choices_frame, text='Language:')\nself.language_var = tkinter.StringVar(choices_frame, 'US English')\nself.language_choices = polly_wrapper.get_languages(self.engine_var.get())\nself.language_options = tkinter.OptionMenu(\nchoices_frame, self.language_var, *sorted(self.language_choices),\ncommand=self.change_language)\nself.voice_label = tkinter.Label(choices_frame, text='Voice:')\nself.voice_var = tkinter.StringVar(choices_frame, 'Joanna')\nself.voice_choices = polly_wrapper.get_voices(\nself.engine_var.get(), self.language_choices[self.language_var.get()])\nself.voice_options = tkinter.OptionMenu(\nchoices_frame, self.voice_var, *sorted(self.voice_choices))\nself.face_canvas = tkinter.Canvas(\nchoices_frame, height=100, width=200, bg='white')\nself.sayit_button = tkinter.Button(\nself.app, text=\"Say it!\", command=self.say_it)\nself.loading_text = tkinter.Label(self.app, bg='white')\nself.app.geometry(\"635x320\")\nself.sayit_label.grid(row=0)\nself.sayit_txt.grid(row=1, column=0)\nself.sayit_txt.focus_set()\nself.sayit_button.grid(row=2, pady=10, columnspan=2)\nself.sayit_button.configure(width=85, padx=10)\nchoices_frame.grid(row=1, column=1, sticky=tkinter.N)\nself.engine_label.grid(row=0, column=0, sticky=tkinter.N, pady=10)\nself.engine_options.grid(row=0, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.engine_options.configure(width=18)\nself.language_label.grid(row=1, column=0, sticky=tkinter.N, pady=10)\nself.language_options.grid(row=1, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.language_options.configure(width=18)\nself.voice_label.grid(row=2, column=0, sticky=tkinter.N, pady=10)\nself.voice_options.grid(row=2, column=1, sticky=tkinter.NW, padx=5, pady=10)\nself.voice_options.configure(width=18)\nself.face_canvas.grid(row=3, columnspan=2, padx=10)\nself.face_canvas.create_image(100, 60, image=self.lips['sil']['image'])\nself.app.mainloop()\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.animate_lips","title":"<code>animate_lips(start_time, viseme, viseme_iter)</code>","text":"<p>Animates the face that lip-syncs along with the synthesized speech. This uses the list of visemes and their associated timings that is returned from Amazon Polly. The image associated with a viseme is displayed and the next viseme is scheduled to display at the time indicated in the list of visemes.</p> <p>:param start_time: The time the animation is started. This is used to                    calculate the time to wait until the next viseme image                    is displayed. :param viseme: The current viseme to display. :param viseme_iter: An iterator that yields visemes from the list returned                     from Amazon Polly.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def animate_lips(self, start_time, viseme, viseme_iter):\n\"\"\"\n    Animates the face that lip-syncs along with the synthesized speech. This\n    uses the list of visemes and their associated timings that is returned from\n    Amazon Polly. The image associated with a viseme is displayed and the next\n    viseme is scheduled to display at the time indicated in the list of visemes.\n    :param start_time: The time the animation is started. This is used to\n                       calculate the time to wait until the next viseme image\n                       is displayed.\n    :param viseme: The current viseme to display.\n    :param viseme_iter: An iterator that yields visemes from the list returned\n                        from Amazon Polly.\n    \"\"\"\ntry:\nmouth = self.lips.get(viseme['value'], self.lips['sil'])\nself.face_canvas.create_image(\n100, 60, image=mouth['image'])\nself.app.update()\nnext_viseme = next(viseme_iter)\nnext_time = start_time + next_viseme['time']\ncur_time = time.time_ns() // 1000000  # milliseconds\nwait_time = max(0, next_time - cur_time)\nlogger.info(\"Vis: %s, cur_time %s, wait_time %s\", mouth,\ncur_time - start_time, wait_time)\nself.app.after(\nwait_time, self.animate_lips, start_time, next_viseme, viseme_iter)\nexcept StopIteration:\npass\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.change_engine","title":"<code>change_engine(engine)</code>","text":"<p>Handles the event that is fired when the selected engine type is changed in the UI. Updates the lists of available languages and voices that are supported for the selected engine type.</p> <p>:param engine: The newly selected engine type.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def change_engine(self, engine):\n\"\"\"\n    Handles the event that is fired when the selected engine type is changed in\n    the UI. Updates the lists of available languages and voices that are supported\n    for the selected engine type.\n    :param engine: The newly selected engine type.\n    \"\"\"\nself.language_choices = self.polly_wrapper.get_languages(engine)\nlang_menu = self.language_options['menu']\nlang_menu.delete(0, 'end')\nsorted_choices = sorted(self.language_choices)\nfor lang in sorted_choices:\nlang_menu.add_command(\nlabel=lang, command=lambda l=lang: self.change_language(l))\nself.change_language(sorted_choices[0])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.change_language","title":"<code>change_language(language)</code>","text":"<p>Handles the event that is fired when the selected language is changed in the UI. Updates the list of available voices that are available for the selected language and engine type.</p> <p>:param language: The newly selected language.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def change_language(self, language):\n\"\"\"\n    Handles the event that is fired when the selected language is changed in the\n    UI. Updates the list of available voices that are available for the selected\n    language and engine type.\n    :param language: The newly selected language.\n    \"\"\"\nself.language_var.set(language)\nself.voice_choices = self.polly_wrapper.get_voices(\nself.engine_var.get(), self.language_choices[language])\nvoice_menu = self.voice_options['menu']\nvoice_menu.delete(0, 'end')\nsorted_choices = sorted(self.voice_choices)\nfor voice in sorted_choices:\nvoice_menu.add_command(\nlabel=voice, command=lambda v=voice: self.voice_var.set(v))\nself.voice_var.set(sorted_choices[0])\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.load_lips","title":"<code>load_lips()</code>","text":"<p>Loads lip-sync images either from a local '.media' folder or from GitHub and saves image data in a dictionary of visemes.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def load_lips(self):\n\"\"\"\n    Loads lip-sync images either from a local '.media' folder or from GitHub\n    and saves image data in a dictionary of visemes.\n    \"\"\"\nif os.path.isdir('.media'):\nlogger.info(\"Found .media folder. Loading images from the local folder.\")\nfor viseme in self.lips:\nself.lips[viseme]['image'] = tkinter.PhotoImage(\nfile=self.lips[viseme]['name'])\nelse:\nlogger.info(\"No local .media folder. Trying to load images from GitHub.\")\nfor viseme in self.lips:\nurl = GITHUB_URL + self.lips[viseme]['name']\nresp = requests.get(url)\nimg = resp.content if resp.status_code == 200 else b''\nif resp.status_code != 200:\nlogger.warning(\"Couldn't load image from %s.\", url)\nself.lips[viseme]['image'] = tkinter.PhotoImage(data=img)\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.long_text_wait_callback","title":"<code>long_text_wait_callback(task_type, task_status)</code>","text":"<p>A callback function that displays status while waiting for an asynchronous long text speech synthesis task to complete.</p> <p>:param task_type: The type of synthesis task (either 'speech' or 'viseme'). :param task_status: The status of the task.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def long_text_wait_callback(self, task_type, task_status):\n\"\"\"\n    A callback function that displays status while waiting for an asynchronous\n    long text speech synthesis task to complete.\n    :param task_type: The type of synthesis task (either 'speech' or 'viseme').\n    :param task_status: The status of the task.\n    \"\"\"\nself.loading_text.grid(row=0, rowspan=4, columnspan=2, sticky=tkinter.NSEW)\nself.loading_text.configure(\ntext=f\"Waiting for {task_type}. Current status: {task_status}.\")\nself.app.update()\nif task_status in ('completed', 'failed'):\nself.app.after(1000)\nself.loading_text.grid_forget()\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_lipsync/#backend.app.utils.tts.backends.resources.aws_example_code.polly_lipsync.PollyMouth.say_it","title":"<code>say_it()</code>","text":"<p>Gets synthesized speech and visemes from Amazon Polly, stores the audio in a temporary file, and plays the sound and lip-sync animation.</p> <p>When the text is too long for synchronous synthesis, this function displays a dialog that asks the user for an Amazon Simple Storage Service (Amazon S3) bucket to use for output storage, starts an asynchronous synthesis task, and waits for the task to complete.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_lipsync.py</code> <pre><code>def say_it(self):\n\"\"\"\n    Gets synthesized speech and visemes from Amazon Polly, stores the audio in\n    a temporary file, and plays the sound and lip-sync animation.\n    When the text is too long for synchronous synthesis, this function displays a\n    dialog that asks the user for an Amazon Simple Storage Service (Amazon S3)\n    bucket to use for output storage, starts an asynchronous synthesis task, and\n    waits for the task to complete.\n    \"\"\"\naudio_stream = None\nvisemes = []\ntry:\naudio_stream, visemes = self.polly_wrapper.synthesize(\nself.sayit_txt.get(1.0, tkinter.END),\nself.engine_var.get(),\nself.voice_choices[self.voice_var.get()],\n'mp3',\nself.language_choices[self.language_var.get()],\nTrue)\nexcept ClientError as error:\nif error.response['Error']['Code'] == 'TextLengthExceededException':\nbucket_name = tkinter.simpledialog.askstring(\n\"Text too long\",\n\"The text is too long for synchronous synthesis. To start an\\n\"\n\"asynchronous job, enter the name of an existing Amazon S3\\n\"\n\"bucket to use for speech synthesis output and click OK.\",\nparent=self.app)\nif bucket_name:\naudio_stream, visemes = self.polly_wrapper.do_synthesis_task(\nself.sayit_txt.get(1.0, tkinter.END),\nself.engine_var.get(),\nself.voice_choices[self.voice_var.get()],\n'mp3',\nbucket_name,\nself.language_choices[self.language_var.get()],\nTrue,\nself.long_text_wait_callback)\nlogger.debug(\"Visemes: %s.\", json.dumps(visemes))\nif audio_stream is not None:\nwith TemporaryDirectory() as tempdir:\nspeech_file_name = tempdir + '/speech.mp3'\nwith open(speech_file_name, 'wb') as speech_file:\nspeech_file.write(audio_stream.read())\nsilence = '.media/silence.mp3'\nif not os.path.isdir('.media'):\nsilence = GITHUB_URL + silence\n# Play a short silent audio file to ensure playsound is loaded and\n# ready. Without this, the audio tends to lag behind viseme playback.\n# playsound(silence)\nplaysound(speech_file_name, block=False)\nstart_time = time.time_ns() // 1000000\nself.app.after(\n0, self.animate_lips, start_time, {'value': 'sil'}, iter(visemes))\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/","title":"polly_wrapper","text":"<p>Purpose</p> <p>Shows how to use the AWS SDK for Python (Boto3) with Amazon Polly to synthesize speech and manage custom lexicons.</p>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper","title":"<code>PollyWrapper</code>","text":"<p>Encapsulates Amazon Polly functions.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>class PollyWrapper:\n\"\"\"Encapsulates Amazon Polly functions.\"\"\"\ndef __init__(self, polly_client, s3_resource):\n\"\"\"\n        :param polly_client: A Boto3 Amazon Polly client.\n        :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.\n        \"\"\"\nself.polly_client = polly_client\nself.s3_resource = s3_resource\nself.voice_metadata = None\n# snippet-end:[python.example_code.polly.helper.PollyWrapper]\n# snippet-start:[python.example_code.polly.DescribeVoices]\ndef describe_voices(self):\n\"\"\"\n        Gets metadata about available voices.\n        :return: The list of voice metadata.\n        \"\"\"\ntry:\nresponse = self.polly_client.describe_voices()\nself.voice_metadata = response['Voices']\nlogger.info(\"Got metadata about %s voices.\", len(self.voice_metadata))\nexcept ClientError:\nlogger.exception(\"Couldn't get voice metadata.\")\nraise\nelse:\nreturn self.voice_metadata\n# snippet-end:[python.example_code.polly.DescribeVoices]\n# snippet-start:[python.example_code.polly.Synthesize]\ndef synthesize(\nself, text, engine, voice, audio_format, lang_code=None,\ninclude_visemes=False):\n\"\"\"\n        Synthesizes speech or speech marks from text, using the specified voice.\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\ntry:\nkwargs = {\n'Engine': engine,\n'OutputFormat': audio_format,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.synthesize_speech(**kwargs)\naudio_stream = response['AudioStream']\nlogger.info(\"Got audio stream spoken by %s.\", voice)\nvisemes = None\nif include_visemes:\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.synthesize_speech(**kwargs)\nvisemes = [json.loads(v) for v in\nresponse['AudioStream'].read().decode().split() if v]\nlogger.info(\"Got %s visemes.\", len(visemes))\nexcept ClientError:\nlogger.exception(\"Couldn't get audio stream.\")\nraise\nelse:\nreturn audio_stream, visemes\n# snippet-end:[python.example_code.polly.Synthesize]\ndef _wait_for_task(self, tries, task_id, task_type, wait_callback, output_bucket):\n\"\"\"\n        Waits for an asynchronous speech synthesis task to complete. This function\n        polls Amazon Polly for data about the specified task until a completion\n        status is returned or the number of tries is exceeded.\n        When the task successfully completes, the task output is retrieved from the\n        output Amazon S3 bucket and the output object is deleted.\n        :param tries: The number of times to poll for status.\n        :param task_id: The ID of the task to wait for.\n        :param task_type: The type of task. This is passed to the `wait_callback`\n                          function to display status.\n        :param wait_callback: A callback function that is called after each poll,\n                              to give the caller an opportunity to take action, such\n                              as to display status.\n        :param output_bucket: The Amazon S3 bucket where task output is located.\n        :return: The output from the task in a byte stream.\n        \"\"\"\ntask = None\nwhile tries &gt; 0:\ntask = self.get_speech_synthesis_task(task_id)\ntask_status = task['TaskStatus']\nlogger.info(\"Task %s status %s.\", task_id, task_status)\nif wait_callback is not None:\nwait_callback(task_type, task_status)\nif task_status in ('completed', 'failed'):\nbreak\ntime.sleep(5)\ntries -= 1\noutput_stream = io.BytesIO()\nif task is not None:\noutput_key = task['OutputUri'].split('/')[-1]\noutput_bucket.download_fileobj(output_key, output_stream)\noutput_bucket.Object(output_key).delete()\nlogger.info(\"Downloaded output for task %s.\", task_id)\noutput_stream.seek(0)\nreturn output_stream\n# snippet-start:[python.example_code.polly.StartSpeechSynthesisTask]\ndef do_synthesis_task(\nself, text, engine, voice, audio_format, s3_bucket, lang_code=None,\ninclude_visemes=False, wait_callback=None):\n\"\"\"\n        Start an asynchronous task to synthesize speech or speech marks, wait for\n        the task to complete, retrieve the output from Amazon S3, and return the\n        data.\n        An asynchronous task is required when the text is too long for near-real time\n        synthesis.\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param s3_bucket: The name of an existing Amazon S3 bucket that you have\n                          write access to. Synthesis output is written to this bucket.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :param wait_callback: A callback function that is called periodically during\n                              task processing, to give the caller an opportunity to\n                              take action, such as to display status.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\ntry:\nkwargs = {\n'Engine': engine,\n'OutputFormat': audio_format,\n'OutputS3BucketName': s3_bucket,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.start_speech_synthesis_task(**kwargs)\nspeech_task = response['SynthesisTask']\nlogger.info(\"Started speech synthesis task %s.\", speech_task['TaskId'])\nviseme_task = None\nif include_visemes:\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.start_speech_synthesis_task(**kwargs)\nviseme_task = response['SynthesisTask']\nlogger.info(\"Started viseme synthesis task %s.\", viseme_task['TaskId'])\nexcept ClientError:\nlogger.exception(\"Couldn't start synthesis task.\")\nraise\nelse:\nbucket = self.s3_resource.Bucket(s3_bucket)\naudio_stream = self._wait_for_task(\n10, speech_task['TaskId'], 'speech', wait_callback, bucket)\nvisemes = None\nif include_visemes:\nviseme_data = self._wait_for_task(\n10, viseme_task['TaskId'], 'viseme', wait_callback, bucket)\nvisemes = [json.loads(v) for v in\nviseme_data.read().decode().split() if v]\nreturn audio_stream, visemes\n# snippet-end:[python.example_code.polly.StartSpeechSynthesisTask]\n# snippet-start:[python.example_code.polly.GetSpeechSynthesisTask]\ndef get_speech_synthesis_task(self, task_id):\n\"\"\"\n        Gets metadata about an asynchronous speech synthesis task, such as its status.\n        :param task_id: The ID of the task to retrieve.\n        :return: Metadata about the task.\n        \"\"\"\ntry:\nresponse = self.polly_client.get_speech_synthesis_task(TaskId=task_id)\ntask = response['SynthesisTask']\nlogger.info(\"Got synthesis task. Status is %s.\", task['TaskStatus'])\nexcept ClientError:\nlogger.exception(\"Couldn't get synthesis task %s.\", task_id)\nraise\nelse:\nreturn task\n# snippet-end:[python.example_code.polly.GetSpeechSynthesisTask]\n# snippet-start:[python.example_code.polly.PutLexicon]\ndef create_lexicon(self, name, content):\n\"\"\"\n        Creates a lexicon with the specified content. A lexicon contains custom\n        pronunciations.\n        :param name: The name of the lexicon.\n        :param content: The content of the lexicon.\n        \"\"\"\ntry:\nself.polly_client.put_lexicon(Name=name, Content=content)\nlogger.info(\"Created lexicon %s.\", name)\nexcept ClientError:\nlogger.exception(\"Couldn't create lexicon %s.\")\nraise\n# snippet-end:[python.example_code.polly.PutLexicon]\n# snippet-start:[python.example_code.polly.GetLexicon]\ndef get_lexicon(self, name):\n\"\"\"\n        Gets metadata and contents of an existing lexicon.\n        :param name: The name of the lexicon to retrieve.\n        :return: The retrieved lexicon.\n        \"\"\"\ntry:\nresponse = self.polly_client.get_lexicon(Name=name)\nlogger.info(\"Got lexicon %s.\", name)\nexcept ClientError:\nlogger.exception(\"Couldn't get lexicon %s.\", name)\nraise\nelse:\nreturn response\n# snippet-end:[python.example_code.polly.GetLexicon]\n# snippet-start:[python.example_code.polly.ListLexicons]\ndef list_lexicons(self):\n\"\"\"\n        Lists lexicons in the current account.\n        :return: The list of lexicons.\n        \"\"\"\ntry:\nresponse = self.polly_client.list_lexicons()\nlexicons = response['Lexicons']\nlogger.info(\"Got %s lexicons.\", len(lexicons))\nexcept ClientError:\nlogger.exception(\"Couldn't get  %s.\", )\nraise\nelse:\nreturn lexicons\n# snippet-end:[python.example_code.polly.ListLexicons]\ndef get_voice_engines(self):\n\"\"\"\n        Extracts the set of available voice engine types from the full list of\n        voice metadata.\n        :return: The set of voice engine types.\n        \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nengines = set()\nfor voice in self.voice_metadata:\nfor engine in voice['SupportedEngines']:\nengines.add(engine)\nreturn engines\ndef get_languages(self, engine):\n\"\"\"\n        Extracts the set of available languages for the specified engine from the\n        full list of voice metadata.\n        :param engine: The engine type to filter on.\n        :return: The set of languages available for the specified engine type.\n        \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nreturn {vo['LanguageName']: vo['LanguageCode'] for vo\nin self.voice_metadata\nif engine in vo['SupportedEngines']}\ndef get_voices(self, engine, language_code):\n\"\"\"\n        Extracts the set of voices that are available for the specified engine type\n        and language from the full list of voice metadata.\n        :param engine: The engine type to filter on.\n        :param language_code: The language to filter on.\n        :return: The set of voices available for the specified engine type and language.\n        \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nreturn {vo['Name']: vo['Id'] for vo in self.voice_metadata\nif engine in vo['SupportedEngines']\nand language_code == vo['LanguageCode']}\n\"\"\"\n        Synthesizes speech or speech marks from text, using the specified voice.\n        :param text: The text to synthesize.\n        :param engine: The kind of engine used. Can be standard or neural.\n        :param voice: The ID of the voice to use.\n        :param audio_format: The audio format to return for synthesized speech. When\n                             speech marks are synthesized, the output format is JSON.\n        :param lang_code: The language code of the voice to use. This has an effect\n                          only when a bilingual voice is selected.\n        :param include_visemes: When True, a second request is made to Amazon Polly\n                                to synthesize a list of visemes, using the specified\n                                text and voice. A viseme represents the visual position\n                                of the face and mouth when saying part of a word.\n        :return: The audio stream that contains the synthesized speech and a list\n                 of visemes that are associated with the speech audio.\n        \"\"\"\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.__init__","title":"<code>__init__(polly_client, s3_resource)</code>","text":"<p>:param polly_client: A Boto3 Amazon Polly client. :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def __init__(self, polly_client, s3_resource):\n\"\"\"\n    :param polly_client: A Boto3 Amazon Polly client.\n    :param s3_resource: A Boto3 Amazon Simple Storage Service (Amazon S3) resource.\n    \"\"\"\nself.polly_client = polly_client\nself.s3_resource = s3_resource\nself.voice_metadata = None\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.create_lexicon","title":"<code>create_lexicon(name, content)</code>","text":"<p>Creates a lexicon with the specified content. A lexicon contains custom pronunciations.</p> <p>:param name: The name of the lexicon. :param content: The content of the lexicon.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def create_lexicon(self, name, content):\n\"\"\"\n    Creates a lexicon with the specified content. A lexicon contains custom\n    pronunciations.\n    :param name: The name of the lexicon.\n    :param content: The content of the lexicon.\n    \"\"\"\ntry:\nself.polly_client.put_lexicon(Name=name, Content=content)\nlogger.info(\"Created lexicon %s.\", name)\nexcept ClientError:\nlogger.exception(\"Couldn't create lexicon %s.\")\nraise\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.describe_voices","title":"<code>describe_voices()</code>","text":"<p>Gets metadata about available voices.</p> <p>:return: The list of voice metadata.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def describe_voices(self):\n\"\"\"\n    Gets metadata about available voices.\n    :return: The list of voice metadata.\n    \"\"\"\ntry:\nresponse = self.polly_client.describe_voices()\nself.voice_metadata = response['Voices']\nlogger.info(\"Got metadata about %s voices.\", len(self.voice_metadata))\nexcept ClientError:\nlogger.exception(\"Couldn't get voice metadata.\")\nraise\nelse:\nreturn self.voice_metadata\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.do_synthesis_task","title":"<code>do_synthesis_task(text, engine, voice, audio_format, s3_bucket, lang_code=None, include_visemes=False, wait_callback=None)</code>","text":"<p>Start an asynchronous task to synthesize speech or speech marks, wait for the task to complete, retrieve the output from Amazon S3, and return the data.</p> <p>An asynchronous task is required when the text is too long for near-real time synthesis.</p> <p>:param text: The text to synthesize. :param engine: The kind of engine used. Can be standard or neural. :param voice: The ID of the voice to use. :param audio_format: The audio format to return for synthesized speech. When                      speech marks are synthesized, the output format is JSON. :param s3_bucket: The name of an existing Amazon S3 bucket that you have                   write access to. Synthesis output is written to this bucket. :param lang_code: The language code of the voice to use. This has an effect                   only when a bilingual voice is selected. :param include_visemes: When True, a second request is made to Amazon Polly                         to synthesize a list of visemes, using the specified                         text and voice. A viseme represents the visual position                         of the face and mouth when saying part of a word. :param wait_callback: A callback function that is called periodically during                       task processing, to give the caller an opportunity to                       take action, such as to display status. :return: The audio stream that contains the synthesized speech and a list          of visemes that are associated with the speech audio.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def do_synthesis_task(\nself, text, engine, voice, audio_format, s3_bucket, lang_code=None,\ninclude_visemes=False, wait_callback=None):\n\"\"\"\n    Start an asynchronous task to synthesize speech or speech marks, wait for\n    the task to complete, retrieve the output from Amazon S3, and return the\n    data.\n    An asynchronous task is required when the text is too long for near-real time\n    synthesis.\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param s3_bucket: The name of an existing Amazon S3 bucket that you have\n                      write access to. Synthesis output is written to this bucket.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :param wait_callback: A callback function that is called periodically during\n                          task processing, to give the caller an opportunity to\n                          take action, such as to display status.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\ntry:\nkwargs = {\n'Engine': engine,\n'OutputFormat': audio_format,\n'OutputS3BucketName': s3_bucket,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.start_speech_synthesis_task(**kwargs)\nspeech_task = response['SynthesisTask']\nlogger.info(\"Started speech synthesis task %s.\", speech_task['TaskId'])\nviseme_task = None\nif include_visemes:\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.start_speech_synthesis_task(**kwargs)\nviseme_task = response['SynthesisTask']\nlogger.info(\"Started viseme synthesis task %s.\", viseme_task['TaskId'])\nexcept ClientError:\nlogger.exception(\"Couldn't start synthesis task.\")\nraise\nelse:\nbucket = self.s3_resource.Bucket(s3_bucket)\naudio_stream = self._wait_for_task(\n10, speech_task['TaskId'], 'speech', wait_callback, bucket)\nvisemes = None\nif include_visemes:\nviseme_data = self._wait_for_task(\n10, viseme_task['TaskId'], 'viseme', wait_callback, bucket)\nvisemes = [json.loads(v) for v in\nviseme_data.read().decode().split() if v]\nreturn audio_stream, visemes\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_languages","title":"<code>get_languages(engine)</code>","text":"<p>Extracts the set of available languages for the specified engine from the full list of voice metadata.</p> <p>:param engine: The engine type to filter on. :return: The set of languages available for the specified engine type.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_languages(self, engine):\n\"\"\"\n    Extracts the set of available languages for the specified engine from the\n    full list of voice metadata.\n    :param engine: The engine type to filter on.\n    :return: The set of languages available for the specified engine type.\n    \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nreturn {vo['LanguageName']: vo['LanguageCode'] for vo\nin self.voice_metadata\nif engine in vo['SupportedEngines']}\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_lexicon","title":"<code>get_lexicon(name)</code>","text":"<p>Gets metadata and contents of an existing lexicon.</p> <p>:param name: The name of the lexicon to retrieve. :return: The retrieved lexicon.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_lexicon(self, name):\n\"\"\"\n    Gets metadata and contents of an existing lexicon.\n    :param name: The name of the lexicon to retrieve.\n    :return: The retrieved lexicon.\n    \"\"\"\ntry:\nresponse = self.polly_client.get_lexicon(Name=name)\nlogger.info(\"Got lexicon %s.\", name)\nexcept ClientError:\nlogger.exception(\"Couldn't get lexicon %s.\", name)\nraise\nelse:\nreturn response\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_speech_synthesis_task","title":"<code>get_speech_synthesis_task(task_id)</code>","text":"<p>Gets metadata about an asynchronous speech synthesis task, such as its status.</p> <p>:param task_id: The ID of the task to retrieve. :return: Metadata about the task.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_speech_synthesis_task(self, task_id):\n\"\"\"\n    Gets metadata about an asynchronous speech synthesis task, such as its status.\n    :param task_id: The ID of the task to retrieve.\n    :return: Metadata about the task.\n    \"\"\"\ntry:\nresponse = self.polly_client.get_speech_synthesis_task(TaskId=task_id)\ntask = response['SynthesisTask']\nlogger.info(\"Got synthesis task. Status is %s.\", task['TaskStatus'])\nexcept ClientError:\nlogger.exception(\"Couldn't get synthesis task %s.\", task_id)\nraise\nelse:\nreturn task\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_voice_engines","title":"<code>get_voice_engines()</code>","text":"<p>Extracts the set of available voice engine types from the full list of voice metadata.</p> <p>:return: The set of voice engine types.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_voice_engines(self):\n\"\"\"\n    Extracts the set of available voice engine types from the full list of\n    voice metadata.\n    :return: The set of voice engine types.\n    \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nengines = set()\nfor voice in self.voice_metadata:\nfor engine in voice['SupportedEngines']:\nengines.add(engine)\nreturn engines\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.get_voices","title":"<code>get_voices(engine, language_code)</code>","text":"<p>Extracts the set of voices that are available for the specified engine type and language from the full list of voice metadata.</p> <p>:param engine: The engine type to filter on. :param language_code: The language to filter on. :return: The set of voices available for the specified engine type and language.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def get_voices(self, engine, language_code):\n\"\"\"\n    Extracts the set of voices that are available for the specified engine type\n    and language from the full list of voice metadata.\n    :param engine: The engine type to filter on.\n    :param language_code: The language to filter on.\n    :return: The set of voices available for the specified engine type and language.\n    \"\"\"\nif self.voice_metadata is None:\nself.describe_voices()\nreturn {vo['Name']: vo['Id'] for vo in self.voice_metadata\nif engine in vo['SupportedEngines']\nand language_code == vo['LanguageCode']}\n\"\"\"\n    Synthesizes speech or speech marks from text, using the specified voice.\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.list_lexicons","title":"<code>list_lexicons()</code>","text":"<p>Lists lexicons in the current account.</p> <p>:return: The list of lexicons.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def list_lexicons(self):\n\"\"\"\n    Lists lexicons in the current account.\n    :return: The list of lexicons.\n    \"\"\"\ntry:\nresponse = self.polly_client.list_lexicons()\nlexicons = response['Lexicons']\nlogger.info(\"Got %s lexicons.\", len(lexicons))\nexcept ClientError:\nlogger.exception(\"Couldn't get  %s.\", )\nraise\nelse:\nreturn lexicons\n</code></pre>"},{"location":"reference/utils/tts/backends/resources/aws_example_code/polly_wrapper/#backend.app.utils.tts.backends.resources.aws_example_code.polly_wrapper.PollyWrapper.synthesize","title":"<code>synthesize(text, engine, voice, audio_format, lang_code=None, include_visemes=False)</code>","text":"<p>Synthesizes speech or speech marks from text, using the specified voice.</p> <p>:param text: The text to synthesize. :param engine: The kind of engine used. Can be standard or neural. :param voice: The ID of the voice to use. :param audio_format: The audio format to return for synthesized speech. When                      speech marks are synthesized, the output format is JSON. :param lang_code: The language code of the voice to use. This has an effect                   only when a bilingual voice is selected. :param include_visemes: When True, a second request is made to Amazon Polly                         to synthesize a list of visemes, using the specified                         text and voice. A viseme represents the visual position                         of the face and mouth when saying part of a word. :return: The audio stream that contains the synthesized speech and a list          of visemes that are associated with the speech audio.</p> Source code in <code>backend/app/utils/tts/backends/resources/aws_example_code/polly_wrapper.py</code> <pre><code>def synthesize(\nself, text, engine, voice, audio_format, lang_code=None,\ninclude_visemes=False):\n\"\"\"\n    Synthesizes speech or speech marks from text, using the specified voice.\n    :param text: The text to synthesize.\n    :param engine: The kind of engine used. Can be standard or neural.\n    :param voice: The ID of the voice to use.\n    :param audio_format: The audio format to return for synthesized speech. When\n                         speech marks are synthesized, the output format is JSON.\n    :param lang_code: The language code of the voice to use. This has an effect\n                      only when a bilingual voice is selected.\n    :param include_visemes: When True, a second request is made to Amazon Polly\n                            to synthesize a list of visemes, using the specified\n                            text and voice. A viseme represents the visual position\n                            of the face and mouth when saying part of a word.\n    :return: The audio stream that contains the synthesized speech and a list\n             of visemes that are associated with the speech audio.\n    \"\"\"\ntry:\nkwargs = {\n'Engine': engine,\n'OutputFormat': audio_format,\n'Text': text,\n'VoiceId': voice}\nif lang_code is not None:\nkwargs['LanguageCode'] = lang_code\nresponse = self.polly_client.synthesize_speech(**kwargs)\naudio_stream = response['AudioStream']\nlogger.info(\"Got audio stream spoken by %s.\", voice)\nvisemes = None\nif include_visemes:\nkwargs['OutputFormat'] = 'json'\nkwargs['SpeechMarkTypes'] = ['viseme']\nresponse = self.polly_client.synthesize_speech(**kwargs)\nvisemes = [json.loads(v) for v in\nresponse['AudioStream'].read().decode().split() if v]\nlogger.info(\"Got %s visemes.\", len(visemes))\nexcept ClientError:\nlogger.exception(\"Couldn't get audio stream.\")\nraise\nelse:\nreturn audio_stream, visemes\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section is in progress.</p>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This will tell you how to get started!</p>"},{"location":"tutorials/installation/","title":"Installation","text":"<p>This will tell you how to install everything</p>"},{"location":"tutorials/advanced/adding_features/","title":"Adding New Features","text":"<p>This will teach you how to add your own features.</p>"},{"location":"tutorials/advanced/choosing_your_backends/","title":"Choosing Your Backend","text":"<p>This will walk you through setting your backend choices.</p>"},{"location":"tutorials/advanced/customizing_face/","title":"Customizing the Face","text":"<p>This will tell you how to customize the face.</p>"}]}